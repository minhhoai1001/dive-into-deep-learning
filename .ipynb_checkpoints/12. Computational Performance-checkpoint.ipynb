{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Hiệu năng Tính toán\n",
    "Trong học sâu, các tập dữ liệu thường rất lớn và mô hình tính toán rất phức tạp. Vì vậy, ta luôn cần quan tâm tới vấn đề hiệu năng tính toán. Trong chương này, ta sẽ tập trung vào các yếu tố then chốt ảnh hưởng đến hiệu năng tính toán: lập trình mệnh lệnh, lập trình ký hiệu, lập trình bất đồng bộ, tính toán song song tự động và tính toán đa GPU. Qua đó, độc giả có thể cải thiện nhiều hơn về hiệu năng tính toán của mô hình đã được triển khai trong các chương trước, như là giảm thời gian huấn luyện mà không ảnh hưởng tới độ chính xác của mô hình."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.1. Trình biên dịch và Trình thông dịch\n",
    "Cho đến nay, ta mới chỉ tập trung vào lập trình mệnh lệnh, kiểu lập trình sử dụng các câu lệnh như print, + hay if để thay đổi trạng thái của chương trình. Hãy cùng xét ví dụ đơn giản sau về lập trình mệnh lệnh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "def fancy_func(a, b, c, d):\n",
    "    e = add(a, b)\n",
    "    f = add(c, d)\n",
    "    g = add(e, f)\n",
    "    return g\n",
    "\n",
    "print(fancy_func(1, 2, 3, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python là một ngôn ngữ thông dịch. Khi thực hiện hàm `fancy_func`, nó thực thi các lệnh trong thân hàm một cách tuần tự. Như vậy, nó sẽ chạy lệnh `e = add(a, b)` rồi sau đó lưu kết quả vào biến `e`, làm cho trạng thái chương trình thay đổi. Hai câu lệnh tiếp theo `f = add(c, d)` và `g = add(e, f)` sẽ được thực thi tương tự, thực hiện phép cộng và lưu kết quả vào các biến. Fig. 12.1.1 sẽ minh họa luồng dữ liệu.\n",
    "\n",
    "![](images/computegraph.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mặc dù lập trình mệnh lệnh rất thuận tiện, nhưng nó lại không quá hiệu quả. Ở đây nếu hàm `add` được gọi nhiều lần trong `fancy_func`, Python cũng sẽ thực thi ba lần gọi hàm độc lập. Nếu điều này xảy ra, giả sử trên một GPU (hay thậm chí nhiều GPU), chi phí phát sinh từ trình thông dịch Python có thể sẽ rất lớn. Hơn nữa, nó sẽ cần phải lưu giá trị các biến `e` và `f` cho tới khi tất cả các lệnh trong `fancy_func` thực thi xong. Điều này là do ta không biết liệu biến `e` và `f` có được sử dụng bởi các phần chương trình khác sau hai lệnh `e = add(a, b)` và `f = add(c, d)` nữa hay không."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.1.1. Lập trình Ký hiệu\n",
    "Lập trình ký hiệu là kiểu lập trình mà ở đó các tính toán thường chỉ được thực hiện một khi chương trình đã được định nghĩa đầy đủ. Cơ chế này được sử dụng trong nhiều framework, bao gồm: Theano, Keras và TensorFlow (hai framework sau đã hỗ trợ lập trình mệnh lệnh). Lập trình ký hiệu thường gồm những bước sau:\n",
    "\n",
    "1. Khai báo các thao tác sẽ được thực thi.\n",
    "2. Biên dịch các thao tác thành chương trình có thể chạy được.\n",
    "3. Thực thi bằng cách cung cấp đầu vào và gọi chương trình đã được biên dịch.\n",
    "\n",
    "Quy trình trên cho phép chúng ta tối ưu hóa chương trình một cách đáng kể. Đầu tiên, ta có thể bỏ qua trình thông dịch Python trong nhiều trường hợp, từ đó loại bỏ được vấn đề nghẽn cổ chai có thể ảnh hưởng nghiêm trọng tới tốc độ tính toán khi sử dụng nhiều GPU tốc độ cao với một luồng Python duy nhất trên CPU. Thứ hai, trình biên dịch có thể tối ưu và viết lại mã nguồn thành `print((1 + 2) + (3 + 4))` hoặc thậm chí `print(10)`. Điều này hoàn toàn khả thi bởi trình biên dịch có thể thấy toàn bộ mã nguồn rồi mới dịch sang mã máy. Ví dụ, nó có thể giải phóng bộ nhớ (hoặc không cấp phát) bất cứ khi nào một biến không còn được dùng đến. Hoặc nó có thể chuyển toàn bộ mã nguồn thành một đoạn tương đương. Để hiểu rõ hơn vấn đề, dưới đây ta sẽ thử mô phỏng quá trình lập trình mệnh lệnh (dựa trên Python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "def add(a, b):\n",
      "    return a + b\n",
      "\n",
      "def fancy_func(a, b, c, d):\n",
      "    e = add(a, b)\n",
      "    f = add(c, d)\n",
      "    g = add(e, f)\n",
      "    return g\n",
      "print(fancy_func(1, 2, 3, 4))\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "def add_():\n",
    "    return '''\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "'''\n",
    "\n",
    "def fancy_func_():\n",
    "    return '''\n",
    "def fancy_func(a, b, c, d):\n",
    "    e = add(a, b)\n",
    "    f = add(c, d)\n",
    "    g = add(e, f)\n",
    "    return g\n",
    "'''\n",
    "\n",
    "def evoke_():\n",
    "    return add_() + fancy_func_() + 'print(fancy_func(1, 2, 3, 4))'\n",
    "\n",
    "prog = evoke_()\n",
    "print(prog)\n",
    "y = compile(prog, '', 'exec')\n",
    "exec(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sự khác biệt giữa lập trình mệnh lệnh (thông dịch) và lập trình ký hiệu như sau:\n",
    "\n",
    "* Lập trình mệnh lệnh dễ hơn. Khi lập trình mệnh lệnh được sử dụng trong Python, mã nguồn trông rất trực quan và dễ viết. Mã nguồn của lập trình mệnh lệnh cũng dễ gỡ lỗi hơn. Điều này là do ta có thể dễ dàng lấy và in ra giá trị của các biến trung gian liên quan, hoặc sử dụng công cụ gỡ lỗi có sẵn của Python.\n",
    "* Lập trình ký hiệu lại hiệu quả hơn và dễ sử dụng trên nền tảng khác. Nó giúp việc tối ưu mã nguồn trong quá trình biên dịch trở nên dễ dàng hơn, đồng thời cho phép ta chuyển đổi chương trình sang một định dạng khác không phụ thuộc vào Python. Do đó chương trình có thể chạy trong các môi trường khác ngoài Python, từ đó tránh được mọi vấn đề tiềm ẩn về hiệu năng liên quan tới trình thông dịch Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.1.2. Lập trình Hybrid\n",
    "Trong quá khứ, hầu hết các framework đều chọn một trong hai phương án tiếp cận: lập trình mệnh lệnh hoặc lập trình ký hiệu. Ví dụ như Theano, TensorFlow, Keras và CNTK đều xây dựng mô hình dạng ký hiệu. Ngược lại, Chainer và PyTorch tiếp cận theo hướng lập trình mệnh lệnh. Mô hình kiểu mệnh lệnh đã được bổ sung vào TensorFlow 2.0 (thông qua chế độ Eager) và Keras trong những bản cập nhật sau này. \n",
    "\n",
    "Như đã đề cập ở trên, PyTorch dựa trên lập trình mệnh lệnh và sử dụng đồ thị tính toán động. Trong nỗ lực tận dụng tính di động và hiệu quả của lập trình ký hiệu, các nhà phát triển đã xem xét liệu có thể kết hợp các lợi ích của cả hai mô hình lập trình hay không. Điều này dẫn đến một torchscript cho phép người dùng phát triển và gỡ lỗi bằng cách sử dụng lập trình mệnh lệnh thuần túy, đồng thời có khả năng chuyển đổi hầu hết các chương trình thành các chương trình ký hiệu để chạy khi hiệu suất và triển khai tính toán cấp sản phẩm được yêu cầu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.1.3. HybridSequential\n",
    "Cách đơn giản nhất để hiểu cách hoạt động của phép hybrid hóa là xem xét các mạng sâu đa tầng. Thông thường, trình thông dịch Python sẽ thực thi mã nguồn cho tất cả các tầng để sinh một lệnh mà sau đó có thể được truyền tới CPU hoặc GPU. Đối với thiết bị tính toán đơn (và nhanh), quá trình trên không gây ra vấn đề lớn nào cả. Mặt khác, nếu ta sử dụng một máy chủ tiên tiến có 8 GPU, ví dụ như P3dn.24xlarge trên AWS, Python sẽ gặp khó khăn trong việc tận dụng tất cả các GPU cùng lúc. Lúc này trình thông dịch Python đơn luồng sẽ trở thành nút nghẽn cổ chai. Hãy xem làm thế nào để giải quyết vấn đề trên cho phần lớn đoạn mã nguồn bằng cách thay Sequential bằng HybridSequential. Chúng ta bắt đầu với việc định nghĩa một mạng MLP đơn giản."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0791, -0.0796]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from d2l import torch as d2l\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Factory for networks\n",
    "def get_net():\n",
    "    net = nn.Sequential(nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 2))\n",
    "    return net\n",
    "\n",
    "x = torch.randn(size=(1, 512))\n",
    "net = get_net()\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bằng cách gọi hàm `torch.jit.script`, ta có thể biên dịch và tối ưu hóa các tính toán trong MLP. Kết quả tính toán của mô hình vẫn không thay đổi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0791, -0.0796]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = torch.jit.script(net)\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bằng cách chuyển đổi mô hình bằng cách sử dụng `torch.jit.script`. Điều này có vẻ tốt đến mức khó tin: viết mã giống như trước đây và chỉ cần chuyển đổi mô hình bằng cách sử dụng `torch.jit.script`. Khi điều này xảy ra, mạng sẽ được tối ưu hóa (chúng tôi sẽ đánh giá hiệu suất bên dưới)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.1.3.1. Tăng tốc bằng Hybrid hóa\n",
    "Để minh hoạ những cải thiện đạt được từ quá trình biên dịch, ta hãy so sánh thời gian cần thiết để đánh giá net(x) trước và sau phép hybrid hóa. Đầu tiên hãy định nghĩa một hàm để đo thời gian trên. Hàm này sẽ hữu ích trong suốt chương này khi chúng ta đo (và cải thiện) hiệu năng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "class Benchmark:\n",
    "    def __init__(self, description='Done'):\n",
    "        self.description = description\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.timer = d2l.Timer()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        print(f'{self.description}: {self.timer.stop():.4f} sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bây giờ ta có thể gọi mạng hai lần với có hybrid hóa và không hybrid hóa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without torchscript: 0.1610 sec\n",
      "With torchscript: 0.1934 sec\n"
     ]
    }
   ],
   "source": [
    "net = get_net()\n",
    "with Benchmark('Without torchscript'):\n",
    "    for i in range(1000): net(x)\n",
    "\n",
    "net = torch.jit.script(net)\n",
    "with Benchmark('With torchscript'):\n",
    "    for i in range(1000): net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Như được quan sát trong kết quả ở trên, sau khi một phiên bản nn.Sequential được tập lệnh bằng cách sử dụng hàm `torch.jit.script`, hiệu suất tính toán được cải thiện thông qua việc sử dụng lập trình ký hiệu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.1.3.2. Chuỗi hóa\n",
    "Một trong những lợi ích của việc biên dịch các mô hình là ta có thể chuỗi hóa (**serialize**) mô hình và các tham số mô hình để lưu trữ. Điều này cho phép ta lưu trữ mô hình mà không phụ thuộc vào ngôn ngữ **front-end**. Điều này cũng cho phép ta sử dụng các mô hình đã huấn luyện trên các thiết bị khác và dễ dàng sử dụng các ngôn ngữ lập trình **front-end** khác. Đồng thời, mã nguồn này thường thực thi nhanh hơn so với khi lập trình mệnh lệnh. Hãy xem xét phương thức `save` sau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "net.save('my_mlp')\n",
    "!ls -lh my_mlp*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Điều này khá khác biệt so vớinhững gì ta đã thấy trước đó. Tất cả các lệnh in được định nghĩa trong hybrid_forward đều bị bỏ qua. Thật vậy, sau khi hybrid hóa, việc thực thi lệnh `net(x)` không còn liên quan gì tới trình thông dịch của Python nữa. Nghĩa là bất cứ đoạn mã Python nào không cần thiết cho tính toán sẽ bị bỏ qua (chẳng hạn như các lệnh in) để việc thực thi trôi chảy hơn và hiệu năng tốt hơn. Và thay vì gọi Python, MXNet gọi trực tiếp back-end C++.\n",
    "Cũng nên lưu ý rằng một số hàm không được hỗ trợ trong mô-đun symbol (như asnumpy) và các toán tử thực thi tại chỗ (in-place) như `a += b` và `a[:] = a + b` phải được viết lại là `a = a + b`. Tuy nhiên, việc biên dịch mô hình vẫn đáng để thực hiện bất cứ khi nào ta quan tâm đến tốc độ. Lợi ích về tốc độ này có thể tăng từ vài phần trăm tới hơn hai lần, tùy thuộc vào sự phức tạp của mô hình, tốc độ của CPU, tốc độ và số lượng GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.1.4. Tóm tắt\n",
    "* Lập trình mệnh lệnh khiến việc thiết kế mô hình mới dễ dàng hơn vì ta có thể viết mã với luồng điều khiển và được sử dụng hệ sinh thái phần mềm của Python.\n",
    "* Lập trình ký hiệu đòi hỏi chúng ta định nghĩa và biên dịch chương trình trước khi thực thi nó. Lợi ích là hiệu năng được cải thiện.\n",
    "* MXNet có thể kết hợp những ưu điểm của cả hai phương pháp khi cần thiết.\n",
    "* Mô hình được xây dựng bởi các lớp `HybridSequential` và `HybridBlock` có thể chuyển đổi các chương trình mệnh lệnh thành các chương trình ký hiệu bằng cách gọi phương thức hybridize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.1.5. Bài tập\n",
    "1. Hãy thiết kế một mạng bằng cách sử dụng lớp `HybridConcurrent`, có thể thử với `GoogleNet` trong :ref: sec_googlenet.\n",
    "2. Hãy thêm `x.asnumpy()` vào dòng đầu tiên của hàm `hybrid_forward` trong lớp `HybridNet`, rồi thực thi mã nguồn và quan sát các lỗi bạn gặp phải. Tại sao các lỗi này xảy ra?\n",
    "3. Điều gì sẽ xảy ra nếu ta thêm luồng điều khiển, cụ thể là các lệnh Python `if` và `for` trong hàm `hybrid_forward`?\n",
    "4. Hãy lập trình các mô hình bạn thích trong các chương trước bằng cách sử dụng lớp `HybridBlock`hoặc `HybridSequential`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.2. Tính toán Bất đồng bộ\n",
    "Máy tính ngày nay là các hệ thống có tính song song cao, được cấu thành từ nhiều lõi CPU (mỗi lõi thường có nhiều luồng), nhiều phần tử xử lý trong mỗi GPU và thường có nhiều GPU trong mỗi máy. Nói ngắn gọn, ta có thể xử lý nhiều tác vụ cùng một lúc, thường là trên nhiều thiết bị khác nhau. Tiếc thay, Python không phải là một ngôn ngữ phù hợp để viết mã tính toán song song và bất đồng bộ, nhất là khi không có sự trợ giúp từ bên ngoài. Xét cho cùng, Python là ngôn ngữ đơn luồng, và có lẽ trong tương lai sẽ không có gì thay đổi. Các framework học sâu như MXNet và TensorFlow tận dụng mô hình lập trình bất đồng bộ để cải thiện hiệu năng (PyTorch sử dụng bộ định thời của chính Python nên có tiêu chí đánh đổi hiệu năng khác). Do đó, việc hiểu rõ cách lập trình bất đồng bộ giúp ta phát triển các chương trình hiệu quả hơn bằng cách chủ động giảm thiểu yêu cầu tính toán và các quan hệ phụ thuộc tương hỗ. Việc này cho phép ta giảm chi phí tính toán phụ trợ và tăng khả năng tận dụng vi xử lý. Ta bắt đầu bằng việc nhập các thư viện cần thiết."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import torch as d2l\n",
    "import numpy, os, subprocess\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2.1. Bất đồng bộ qua Back-end\n",
    "Để khởi động, hãy cùng xét một bài toán nhỏ - ta muốn sinh ra một ma trận ngẫu nhiên và nhân nó lên nhiều lần. Hãy thực hiện việc này bằng cả NumPy và NumPy của Pytorch để xem xét sự khác nhau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'd2l.torch' has no attribute 'Benchmark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-1f7039a9d07b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0md2l\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBenchmark\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'numpy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'd2l.torch' has no attribute 'Benchmark'"
     ]
    }
   ],
   "source": [
    "# warmup for gpu computation\n",
    "device = d2l.try_gpu()\n",
    "a = torch.randn(size=(1000, 1000), device=device)\n",
    "b = torch.mm(a, a)\n",
    "\n",
    "with d2l.Benchmark('numpy'):\n",
    "    for _ in range(10):\n",
    "        a = numpy.random.normal(size=(1000, 1000))\n",
    "        b = numpy.dot(a, a)\n",
    "\n",
    "with d2l.Benchmark('torch'):\n",
    "    for _ in range(10):\n",
    "        a = torch.randn(size=(1000, 1000), device=device)\n",
    "        b = torch.mm(a, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy của Pytorch nhanh hơn tới cả hàng trăm hàng ngàn lần. Ít nhất là có vẻ là như vậy. Do cả hai thư viện đều được thực hiện trên cùng một bộ xử lý, chắc hẳn phải có gì đó ảnh hướng đến kết quả. Nếu ta ép Pytorch phải hoàn thành tất cả phép tính trước khi trả về kết quả, ta có thể thấy rõ điều gì đã xảy ra ở trên: phần tính toán được thực hiện bởi back-end trong khi front-end đã trả lại quyền điều khiển cho Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'd2l.torch' has no attribute 'Benchmark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-8ebab7821b3a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0md2l\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBenchmark\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msynchronize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'd2l.torch' has no attribute 'Benchmark'"
     ]
    }
   ],
   "source": [
    "with d2l.Benchmark():\n",
    "    for _ in range(10):\n",
    "        a = torch.randn(size=(1000, 1000), device=device)\n",
    "        b = torch.mm(a, a)\n",
    "    torch.cuda.synchronize(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nhìn chung, Pytorch có front-end cho phép tương tác trực tiếp với người dùng thông qua Python, cũng như back-end được sử dụng bởi hệ thống nhằm thực hiện nhiệm vụ tính toán. Như ở Fig. 12.2.1, người dùng có thể viết chương trình MXNet bằng nhiều ngôn ngữ front-end như Python, R, Scala và C++. Dù sử dụng ngôn ngữ front-end nào, chương trình Pytorch chủ yếu thực thi trên back-end lập trình bằng C++. Các thao tác đưa ra bởi ngôn ngữ front-end được truyền vào back-end để thực thi. Back-end tự quản lý các luồng xử lý bằng việc liên tục tập hợp và thực thi các tác vụ trong hàng đợi. Chú ý rằng, back-end cần phải có khả năng theo dõi quan hệ phụ thuộc giữa các bước trong đồ thị tính toán để có thể hoạt động. Nghĩa là ta không thể song song hóa các thao tác phụ thuộc lẫn nhau.\n",
    "\n",
    "![](images/frontends.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hãy xét một ví dụ đơn giản để có thể hiểu rõ hơn đồ thị quan hệ phụ thuộc (**dependency graph**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3.]], device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones((1, 2), device=device)\n",
    "y = torch.ones((1, 2), device=device)\n",
    "z = x * y + 2\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/asyncgraph.svg)\n",
    "<center>Fig. 12.2.2 Quan hệ phụ thuộc.</center>\n",
    "Đoạn mã trên cũng được mô tả trong Fig. 12.2.2. Mỗi khi luồng front-end của Python thực thi một trong ba câu lệnh đầu tiên, nó sẽ chỉ đưa tác vụ đó vào hàng chờ của back-end. Khi kết quả của câu lệnh cuối cùng cần được in ra, luồng front-end của Python sẽ chờ luồng xử lý back-end C++ tính toán xong kết quả của biến z. Lợi ích của thiết kế này nằm ở việc luồng front-end Python không cần phải đích thân thực hiện việc tính toán. Do đó, hiệu năng tổng thể của chương trình cũng ít bị ảnh hưởng bởi hiệu năng của Python. Fig. 12.2.3 mô tả cách front-end và back-end tương tác với nhau.\n",
    "\n",
    "![](images/threading.svg)\n",
    "<center>Fig. 12.2.3 Front-end và Back-end</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2.2. Lớp cản và Bộ chặn\n",
    "Có khá nhiều thao tác buộc Python phải chờ cho đến khi nó hoàn thành:\n",
    "\n",
    "* Hiển nhiên nhất là lệnh npx.waitall() chờ đến khi toàn bộ phép toán đã hoàn thành, bất chấp thời điểm câu lệnh tính toán được đưa ra. Trong thực tế, trừ khi thực sự cần thiết, việc sử dụng thao tác này là một ý tưởng tồi do nó có thể làm giảm hiệu năng.\n",
    "* Nếu ta chỉ muốn chờ đến khi một biến cụ thể nào đó sẵn sàng, ta có thể gọi z.wait_to_read(). Trong trường hợp này MXNet chặn việc trả luồng điều khiển về Python cho đến khi biến z đã được tính xong. Các thao tác khác sau đó mới có thể tiếp tục.\n",
    "\n",
    "Hãy xem cách các lệnh chờ trên hoạt động trong thực tế:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cả hai thao tác hoàn thành với thời gian xấp xỉ nhau. Ngoài các thao tác chặn (blocking operation) tường minh, bạn đọc cũng nên biết về việc chặn ngầm. Rõ ràng việc in một biến ra yêu cầu biến đó phải sẵn sàng và do đó nó là một bộ chặn. Cuối cùng, ép kiểu sang NumPy bằng `z.asnumpy()` và ép kiểu sang số vô hướng bằng `z.item()` cũng là bộ chặn, do trong NumPy không có khái niệm bất đồng bộ. Có thể thấy việc ép kiểu cũng cần truy cập giá trị, giống như hàm `print`. Việc thường xuyên sao chép một lượng nhỏ dữ liệu từ phạm vi của MXNet sang NumPy và ngược lại có thể làm giảm đáng kể hiệu năng của một đoạn mã đáng lẽ sẽ có hiệu năng tốt, do mỗi thao tác như vậy buộc đồ thị tính toán phải tính toàn bộ các giá trị trung gian để suy ra các số hạng cần thiết trước khi thực hiện bất cứ thao tác nào khác."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2.3. Cải thiện Năng lực Tính toán\n",
    "Trong một hệ thống đa luồng lớn (ngay cả laptop phổ thông cũng có 4 luồng hoặc hơn, và trên các máy trạm đa socket, số luồng có thể vượt quá 256), chi phí phụ trợ từ việc định thời các thao tác có thể trở nên khá lớn. Đó là lý do tại sao hai quá trình tính toán và định thời nên xảy ra song song và bất đồng bộ. Để minh hoạ cho lợi ích của việc này, hãy so sánh khi liên tục cộng 1 vào một biến theo cách đồng bộ và bất đồng bộ. Ta mô phỏng quá trình thực thi đồng bộ bằng cách chèn một lớp cản wait_to_read() giữa mỗi phép cộng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta có thể tóm tắt đơn giản sự tương tác giữa luồng front-end Python và luồng back-end C++ như sau:\n",
    "\n",
    "* Front-end ra lệnh cho back-end đưa tác vụ tính `y = x + 1` vào hàng đợi.\n",
    "* Back-end sau đó nhận các tác vụ tính toán từ hàng đợi và thực hiện các phép tính.\n",
    "* Back-end trả kết quả tính toán về cho front-end.\n",
    "\n",
    "Giả sử thời gian thực hiện mỗi giai đoạn trên lần lượt là  $t_1,t_2$  và  $t_3$ . Nếu ta không áp dụng lập trình bất đồng bộ, tổng thời gian để thực hiện 1000 phép tính xấp xỉ bằng  $1000 (t_1+ t_2 + t_3)$ . Còn nếu ta áp dụng lập trình bất đồng bộ, tổng thời gian để thực hiện 1000 phép tính có thể giảm xuống còn $t_1 + 1000 t_2 + t_3$  (giả sử  $1000 t_2 > 999t_1$ ), do front-end không cần phải chờ back-end trả về kết quả tính toán sau mỗi vòng lặp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.2.4. Cải thiện Mức chiếm dụng Bộ nhớ\n",
    "Cùng hình dung với trường hợp ta liên tục thêm các tính toán vào back-end bằng cách thực thi mã Python trên front-end. Ví dụ, trong một khoảng thời gian rất ngắn, front-end liên tục thêm vào một lượng lớn các tác vụ trên minibatch. Xét cho cùng, công việc trên có thể hoàn thành nhanh chóng nếu không có phép tính nào thật sự diễn ra trên Python. Nếu tất cả tác vụ trên cùng được khởi động một cách nhanh chóng thì có thể dẫn đến dung lượng bộ nhớ sử dụng tăng đột ngột. Do dung lượng bộ nhớ có sẵn trên GPU (và ngay cả CPU) là có hạn, điều này có thể gây ra sự tranh chấp tài nguyên hoặc thậm chí làm sập chương trình. Độc giả có lẽ đã nhận ra rằng ở các quy trình huấn luyện trước, ta áp dụng các thao tác đồng bộ như item hay ngay cả asnumpy.\n",
    "\n",
    "Chúng tôi khuyến nghị nên sử dụng các thao tác này một cách cẩn thận, ví dụ như với từng minibatch, ta cần đảm bảo sao cho hiệu năng tính toán và mức chiếm dụng bộ nhớ (**memory footprint**) được cân bằng. Để minh họa, hãy cùng lập trình một vòng lặp huấn luyện đơn giản, đo lượng bộ nhớ tiêu hao và thời gian thực thi, sử dụng hàm sinh dữ liệu và mạng học sâu dưới đây."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mặc dù thời gian để đưa ra chỉ dẫn cho back-end nhỏ hơn đến hàng chục lần, ta vẫn cần thực hiện các bước tính toán. Hậu quả là một lượng lớn các kết quả trung gian không được đưa ra sử dụng và có thể chất đống trong bộ nhớ. Dù rằng việc này không gây ra bất cứ vấn đề nào trong ví dụ nhỏ trên, nó có thể dẫn đến tình trạng cạn kiệt bộ nhớ nếu không được kiểm tra trong viễn cảnh thực tế."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2.5. Tóm tắt\n",
    "* MXNet tách riêng khối front-end Python khỏi khối back-end thực thi. Điều này cho phép nhanh chóng chèn các câu lệnh một cách bất đồng bộ vào khối back-end và kết hợp tính toán song song.\n",
    "* Sự bất đồng bộ giúp front-end phản ứng nhanh hơn. Tuy nhiên, cần phải áp dụng cẩn thận để không làm tràn các tác vụ ở trạng thái đợi, gây chiếm dụng bộ nhớ.\n",
    "* Nên đồng bộ theo từng minibatch một để giữ cho front-end và back-end được đồng bộ tương đối.\n",
    "* Nên nhớ rằng việc chuyển quản lý bộ nhớ từ MXNet sang Python sẽ buộc back-end phải chờ cho đến khi biến đó sẵn sàng. `print`, `asnumpy` và `item` đều gây ra hiệu ứng trên. Điều này có thể có ích đôi lúc, tuy nhiên lạm dụng chúng có thể làm sụt giảm hiệu năng.\n",
    "* Nhà sản xuất vi xử lý cung cấp các công cụ phân tích hiệu năng tinh vi, cho phép đánh giá hiệu năng của học sâu một cách chi tiết hơn rất nhiều."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2.6. Bài tập\n",
    "1. Như đã đề cập ở trên, sử dụng tính toán bất đồng bộ có thể giảm tổng thời gian cần thiết để thực hiện  1000  phép tính xuống  $t_1 + 1000 t_2 + t_3$ . Tại sao ở đó ta lại phải giả sử  $1000 t_2 > 999 t_1$ ?\n",
    "2. Bạn có thể chỉnh sửa vòng lặp huấn luyện như thế nào nếu muốn xử lý 2 batch cùng lúc (đảm bảo batch  $b_t$  hoàn thành trước khi batch $b_{t+2}$ bắt đầu)?\n",
    "3. Chuyện gì sẽ xảy ra nếu thực thi mã nguồn đồng thời trên cả CPU và GPU? Liệu có nên tiếp tục đồng bộ sau khi xử lý mỗi minibatch?\n",
    "4. So sánh sự khác nhau giữa `waitall` và `wait_to_read`. Gợi ý: thực hiện một số lệnh và đồng bộ theo kết quả trung gian."
   ]
<<<<<<< HEAD
=======
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Song song hóa Tự động\n",
    "\n",
    "PyTorch tự động xây dựng các đồ thị tính toán (computational graph) ở back-end. Sử dụng đồ thị tính toán, hệ thống nhận biết được tất cả thành phần phụ thuộc, từ đó thực hiện song song có chọn lọc các tác vụ không liên quan đến nhau để cải thiện tốc độ. Chẳng hạn, Fig. 12.2.2 trong Section 12.2 khởi tạo hai biến độc lập. Do đó hệ thống có thể chọn để thực hiện chúng song song với nhau.\n",
    "\n",
    "Thông thường, một toán tử đơn sẽ sử dụng toàn bộ tài nguyên tính toán trên tất cả các CPU hoặc trên một GPU đơn. Chẳng hạn như toán tử dot sẽ sử dụng tất cả các lõi (và các luồng) của toàn bộ CPU trên một máy tính đơn. Điều tương tự cũng xảy ra trên một GPU đơn. Do đó việc song song hóa không thật sự hữu dụng mấy với các máy tính đơn lõi/đơn luồng. Với các thiết bị đa xử lý thì nó lại có giá trị hơn rất nhiều. Trong khi xử lý song song thường liên quan đến các GPU, sử dụng thêm các vi xử lý CPU cục bộ trên máy sẽ tăng hiệu năng tính toán lên chút đỉnh. Tham khảo [Hadjis.Zhang.Motliagkas.ea.2016], một bài báo tập trung về việc huấn luyện mô hình thị giác máy tính kết hợp một GPU và một CPU. Với sự thuận tiện từ một framework cho phép song song hóa một cách tự động, ta có thể thực hiện việc đó chỉ với vài dòng mã lệnh Python. Mở rộng hơn, thảo luận của chúng ta về tính toán song song tự động tập trung vào tính toán song song sử dụng cả CPU và GPU, cũng như tính toán và giao tiếp song song. Chúng ta bắt đầu bằng việc nhập các gói thư viện và mô-đun cần thiết. Lưu ý rằng chúng ta cần ít nhất một GPU để chạy các thử nghiệm trong phần này."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import torch as d2l\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.3.1. Tính toán Song song trên CPU và GPU\n",
    "\n",
    "Ta hãy bắt đầu bằng việc định nghĩa một khối lượng công việc tham khảo để kiểm thử. Hàm run dưới đây thực hiện 10 phép nhân ma trận trên thiết bị mà chúng ta lựa chọn bằng cách sử dụng dữ liệu được lưu ở hai biến x_cpu và x_gpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices = d2l.try_all_gpus()\n",
    "def run(x):\n",
    "    return [x.mm(x) for _ in range(50)]\n",
    "\n",
    "x_gpu1 = torch.rand(size=(4000, 4000), device=devices[0])\n",
    "x_gpu2 = torch.rand(size=(4000, 4000), device=devices[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bây giờ ta sẽ gọi hàm với dữ liệu. Để chắc chắn rằng bộ nhớ đệm không ảnh hưởng đến kết quả, ta khởi động các thiết bị bằng việc thực hiện một lượt tính cho mỗi biến trước khi bắt đầu đo lường."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(x_gpu1)\n",
    "run(x_gpu2)  # Warm-up all devices\n",
    "torch.cuda.synchronize(devices[0])\n",
    "torch.cuda.synchronize(devices[1])\n",
    "\n",
    "with d2l.Benchmark('GPU 1 time'):\n",
    "    run(x_gpu1)\n",
    "    torch.cuda.synchronize(devices[0])\n",
    "\n",
    "with d2l.Benchmark('GPU 2 time'):\n",
    "    run(x_gpu2)\n",
    "    torch.cuda.synchronize(devices[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nếu ta bỏ qua `torch.cuda.synchronize()` giữa hai tác vụ thì hệ thống sẽ tự động song song hóa việc tính toán trên cả hai thiết bị."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with d2l.Benchmark('GPU1 & GPU2'):\n",
    "    run(x_gpu1)\n",
    "    run(x_gpu2)\n",
    "    torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong trường hợp phía trên, thời gian thi hành toàn bộ các tác vụ ít hơn tổng thời gian thi hành từng tác vụ riêng lẻ, bởi vì Pytorch tự động định thời việc tính toán trên cả CPU và GPU mà không đòi hỏi người dùng phải cung cấp các đoạn mã phức tạp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3.2. Tính toán và Giao tiếp Song song\n",
    "\n",
    "Trong nhiều trường hợp ta cần di chuyển dữ liệu giữa các thiết bị như CPU và GPU, hoặc giữa các GPU với nhau. Điều này xảy ra, chẳng hạn như khi ta cần tổng hợp gradient trên các thẻ tăng tốc (accelerator card) khi cần thực hiện tối ưu hóa phân tán. Hãy cùng mô phỏng điều này bằng việc tính toán trên GPU và sau đó sao chép kết quả trở lại CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_to_cpu(x, non_blocking=False):\n",
    "    return [y.to('cpu', non_blocking=non_blocking) for y in x]\n",
    "\n",
    "with d2l.Benchmark('Run on GPU1'):\n",
    "    y = run(x_gpu1)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "with d2l.Benchmark('Copy to CPU'):\n",
    "    y_cpu = copy_to_cpu(y)\n",
    "    torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Điều này có phần không hiệu quả. Lưu ý rằng ta có thể bắt đầu sao chép một vài phần đã tính xong của y đến CPU trong khi các phần còn lại của y vẫn đang được tính toán. Tình huống này có thể xảy ra khi ta tính gradient (lan truyền ngược) trên một minibatch. Gradient của một vài tham số sẽ được tính xong sớm hơn so với các tham số khác. Do đó sẽ có lợi nếu ta bắt đầu truyền dữ liệu về bằng bus băng thông PCI-Express trong khi GPU vẫn còn đang chạy. Trong PyTorch, một số hàm như `to()` và `copy_()` thừa nhận một đối số `non_blocking` rõ ràng, cho phép người gọi bỏ qua đồng bộ hóa khi không cần thiết. Đặt `non_blocking = True` cho phép chúng tôi mô phỏng tình huống này."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with d2l.Benchmark('Run on GPU1 and copy to CPU'):\n",
    "    y = run(x_gpu1)\n",
    "    y_cpu = copy_to_cpu(y, True)\n",
    "    torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thời gian cần cho cả hai thao tác ít hơn hẳn (như mong đợi) so với tổng thời gian thực hiện từng thao tác đơn lẻ. Lưu ý rằng tác vụ này khác với việc tính toán song song bởi nó sử dụng một tài nguyên khác: bus giữa CPU và các GPU. Thực tế, ta có thể vừa tính toán và giao tiếp trên cả hai thiết bị cùng một lúc. Như đã lưu ý phía trên, có một sự phụ thuộc giữa việc tính toán và giao tiếp: `y[i]` phải được tính xong trước khi ta có thể sao chép nó qua CPU. May mắn thay, hệ thống có thể sao chép `y[i-1]` trong khi tính toán `y[i]`để giảm thiểu tổng thời gian chạy.\n",
    "\n",
    "Để tổng kết phần này, ta xét một ví dụ minh hoạ đồ thị tính toán và các quan hệ phụ thuộc của nó trong một mạng Perceptron hai tầng đơn giản khi huấn luyện trên một CPU và hai GPU, như miêu tả trong Fig. 12.3.1. Có thể thấy tự mình định thời chương trình tính toán song song từ mô tả trên sẽ khá phức tạp. Do đó, việc sử dụng back-end tính toán dựa trên đồ thị là một lợi thế để tối ưu hóa hiệu năng.\n",
    "\n",
    "![](images/twogpu.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  12.3.3. Tóm tắt\n",
    "\n",
    "* Các hệ thống hiện đại thường bao gồm nhiều thiết bị, ví dụ như nhiều GPU và CPU. Các thiết bị này có thể được sử dụng song song, một cách bất đồng bộ.\n",
    "* Các hệ thống hiện đại cũng có nhiều nguồn tài nguyên phục vụ cho giao tiếp, ví dụ như kết nối PCI Express, bộ nhớ (thường là SSD hoặc thông qua mạng), và băng thông mạng. Chúng có thể được sử dụng song song để đạt hiệu năng tối đa.\n",
    "* Back-end có thể cải thiện hiệu năng thông qua việc tự động tính toán và giao tiếp song song.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  12.3.4. Bài tập\n",
    "1. Có 10 thao tác được thực hiện trong hàm run đã được định nghĩa trong phần này. Giữa chúng không có bất cứ quan hệ phụ thuộc nào. Thiết kế một thí nghiệm để xem liệu MXNet có tự động thực thi các thao tác này một cách song song.\n",
    "2. Khi khối lượng công việc của một thao tác đủ nhỏ, song song hóa có thể hữu ích ngay cả khi chạy trên CPU hay GPU đơn. Thiết kế một thí nghiệm để kiểm chứng.\n",
    "3. Thiết kế một thí nghiệm sử dụng tính toán song song trên CPU, GPU và giao tiếp giữa cả hai thiết bị.\n",
    "4. Sử dụng một trình gỡ lỗi (debugger) như Nsight của NVIDIA để kiểm chứng rằng đoạn mã của bạn hoạt động hiệu quả.\n",
    "5. Thiết kế các tác vụ tính toán chứa nhiều dữ liệu có quan hệ phụ thuộc phức tạp hơn, và thực hiện các thí nghiệm để xem liệu bạn có thể thu lại kết quả chính xác trong khi vẫn cải thiện hiệu năng.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.4. Phần cứng\n",
    "\n",
    "Để xây dựng các hệ thống có hiệu năng cao, ta cần nắm chắc kiến thức về các thuật toán và mô hình để có thể biểu diễn được những khía cạnh thống kê của bài toán. Đồng thời, ta cũng cần có một chút kiến thức cơ bản về phần cứng thực thi ở bên dưới. Nội dung trong phần này không thể thay thế một khóa học đầy đủ về phần cứng và thiết kế hệ thống, mà sẽ chỉ đóng vai trò như điểm bắt đầu để giúp người đọc hiểu tại sao một số thuật toán lại hiệu quả hơn các thuật toán khác và làm thế nào để đạt được thông lượng cao. Thiết kế tốt có thể dễ dàng tạo ra sự khác biệt rất lớn, giữa việc có thể huấn luyện một mô hình (ví dụ trong khoảng một tuần) và không thể huấn luyện (ví dụ mất 3 tháng để huấn luyện xong, từ đó không kịp tiến độ). Ta sẽ bắt đầu bằng việc quan sát tổng thể một hệ thống máy tính. Tiếp theo, ta sẽ đi sâu hơn và xem xét chi tiết về CPU và GPU. Cuối cùng, ta sẽ tìm hiểu cách các máy tính được kết nối với nhau trong trạm máy chủ hay trên đám mây. Cần lưu ý, phần này sẽ không hướng dẫn cách lựa chọn card GPU. Nếu bạn cần gợi ý, hãy xem Section 19.5. Phần giới thiệu về điện toán đám mây trên AWS có thể tìm thấy tại Section 19.3.\n",
    "\n",
    "Bạn đọc có thể tham khảo nhanh thông tin tóm tắt trong Fig. 12.4.1. Nội dung này được trích dẫn từ bài viết của [Colin Scott](https://colin-scott.github.io/personal_website/research/interactive_latency.html) trình bày tổng quan về những tiến bộ trong thập kỉ qua. Số liệu gốc được trích dẫn từ buổi thảo luận của Jeff Dean tại trường [Stanford năm 2010](https://static.googleusercontent.com/media/research.google.com/en//people/jeff/Stanford-DL-Nov-2010.pdf). Phần thảo luận dưới đây sẽ giải thích cơ sở cho những con số trên và cách mà chúng dẫn dắt ta trong quá trình thiết kế thuật toán. Nội dung khái quát và ngắn gọn nên nó không thể thay thế một khóa học đầy đủ, nhưng sẽ cung cấp đủ thông tin cho những người làm mô hình thống kê để có thể đưa ra lựa chọn thiết kế phù hợp. Để có cái nhìn tổng quan chuyên sâu về kiến trúc máy tính, bạn đọc có thể tham khảo [Hennessy & Patterson, 2011](https://inst.eecs.berkeley.edu//~cs152/sp19/) hay một khóa học gần đây của Arste Asanovic.\n",
    "\n",
    "![](images/latencynumbers.png)\n",
    "<center>Fig. 12.4.1 Số liệu về độ trễ mà mọi lập trình viên nên biết.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.4.1. Máy tính\n",
    "\n",
    "Hầu hết những nhà nghiên cứu học sâu đều được trang bị hệ thống máy tính có bộ nhớ và khả năng tính toán khá lớn với một hay nhiều GPU. Những máy tính này thường có những thành phần chính sau:\n",
    "\n",
    "* Bộ xử lý, thường được gọi là CPU, có khả năng thực thi các chương trình được nhập bởi người dùng (bên cạnh chức năng chạy hệ điều hành và các tác vụ khác), thường có 8 lõi (core) hoặc nhiều hơn.\n",
    "* Bộ nhớ (RAM) được sử dụng để lưu trữ và truy xuất các kết quả tính toán như vector trọng số, giá trị kích hoạt và dữ liệu huấn luyện.\n",
    "* Một hay nhiều kết nối Ethernet với tốc độ đường truyền từ 1 Gbit/s tới 100 Gbit/s (các máy chủ tân tiến còn có các phương thức kết nối cao cấp hơn nữa).\n",
    "* Cổng giao tiếp bus mở rộng tốc độ cao (PCIe) kết nối hệ thống với một hay nhiều GPU. Các hệ thống máy chủ thường có tới 8 GPU được kết nối với nhau theo cấu trúc liên kết phức tạp. Còn các hệ thống máy tính thông thường thì có 1-2 GPU, phụ thuộc vào túi tiền của người dùng và công suất nguồn điện.\n",
    "* Bộ lưu trữ tốt, thường là ổ cứng từ (HDD) hay ổ cứng thể rắn (SSD), được kết nối bằng bus PCIe giúp truyền dữ liệu huấn luyện tới hệ thống và sao lưu các checkpoint trung gian một cách hiệu quả.\n",
    "\n",
    "![](images/mobo-symbol.svg)\n",
    "<center>Fig. 12.4.2 Kết nối các thành phần máy tính</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hình Fig. 12.4.2 cho thấy, hầu hết các thành phần (mạng, GPU, ổ lưu trữ) được kết nối tới GPU thông qua đường bus PCI mở rộng. Đường truyền này gồm nhiều làn kết nối trực tiếp tới CPU. Ví dụ, Threadripper 3 của AMD có 64 làn PCIe 4.0, mỗi làn có khả năng truyền dẫn 16 Gbit/s dữ liệu theo cả hai chiều. Bộ nhớ được kết nối trực tiếp tới CPU với tổng băng thông lên đến 100 GB/s.\n",
    "\n",
    "Khi ta chạy chương trình trên máy tính, ta cần trộn dữ liệu ở các bộ xử lý (CPU hay GPU), thực hiện tính toán và sau đó truyền kết quả tới RAM hay ổ lưu trữ. Do đó, để có hiệu năng tốt, ta cần đảm bảo rằng chương trình chạy mượt mà và hệ thống không có nút nghẽn cổ chai. Ví dụ, nếu ta không thể tải ảnh đủ nhanh, bộ xử lý sẽ không có có dữ liệu để chạy. Tương tự, nếu ta không thể truyền các ma trận tới CPU (hay GPU) đủ nhanh, bộ xử lý sẽ thiếu dữ liệu để hoạt động. Cuối cùng, nếu ta muốn đồng bộ nhiều máy tính trong một mạng, kết nối mạng không nên làm chậm việc tính toán. Xen kẽ việc giao tiếp và tính toán giữa các máy tính là một phương án cho vấn đề này. Giờ hãy xem xét các thành phần trên một cách chi tiết hơn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.4.2. Bộ nhớ\n",
    "\n",
    "Về cơ bản, bộ nhớ được sử dụng để lưu trữ dữ liệu khi cần sẵn sàng truy cập. Hiện tại bộ nhớ RAM của CPU thường thuộc loại DDR4, trong đó mỗi mô-đun có băng thông 20-25GB/s và độ rộng bus 64 bit. Thông thường, các cặp mô-đun bộ nhớ cho phép sử dụng đa kênh. CPU có từ 2 đến 4 kênh bộ nhớ, nghĩa là chúng có băng thông bộ nhớ tối đa từ 40 GB/s đến 100 GB/s. Thường thì mỗi kênh có hai dải (bank). Ví dụ, Zen 3 Threadripper của AMD có 8 khe cắm.\n",
    "\n",
    "Dù những con số trên trông khá ấn tượng, trên thực tế chúng chỉ nói lên một phần nào đó. Khi muốn đọc một phần nào đó từ bộ nhớ, trước tiên ta cần chỉ cho mô-đun bộ nhớ vị trí chứa thông tin, tức cần gửi địa chỉ đến RAM. Khi thực hiện xong việc này, ta có thể chọn chỉ đọc một bản ghi 64 bit hoặc một chuỗi dài các bản ghi. Lựa chọn thứ hai được gọi là đọc nhanh (burst read). Nói ngắn gọn, việc gửi một địa chỉ vào bộ nhớ và thiết lập chuyển tiếp sẽ mất khoảng 100ns (thời gian cụ thể phụ thuộc vào hệ số thời gian của từng chip bộ nhớ được sử dụng), mỗi lần chuyển tiếp sau đó chỉ mất 0.2ns. Có thể thấy lần đọc đầu tiên tốn thời gian gấp 500 lần những lần sau! Ta có thể đọc ngẫu nhiên tối đa 10,000,000\n",
    "\n",
    "lần mỗi giây. Điều này cho thấy rằng ta nên hạn chế tối đa việc truy cập bộ nhớ ngẫu nhiên và thay vào đó nên sử dụng cách đọc (và ghi) nhanh (burst read, và burst write).\n",
    "\n",
    "Mọi thứ trở nên phức tạp hơn một chút khi ta tính đến việc có nhiều dải bộ nhớ. Mỗi dải có thể đọc bộ nhớ gần như là độc lập với nhau. Điều này có hai ý sau. Thứ nhất, số lần đọc ngẫu nhiên thực sự cao hơn tới 4 lần, miễn là chúng được trải đều trên bộ nhớ. Điều đó cũng có nghĩa là việc thực hiện các lệnh đọc ngẫu nhiên vẫn không phải là một ý hay vì các lệnh đọc nhanh (burst read) cũng nhanh hơn gấp 4 lần. Thứ hai, do việc căn chỉnh bộ nhớ theo biên 64 bit, ta nên căn chỉnh mọi cấu trúc dữ liệu theo cùng biên đó. Trình biên dịch thực hiện việc này một cách tự động khi các cờ thích hợp được đặt. Độc giả có thể tham khảo thêm bài giảng về DRAM ví dụ như Zeshan Chishti.\n",
    "\n",
    "Bộ nhớ GPU còn yêu cầu băng thông cao hơn nữa vì chúng có nhiều phần tử xử lý hơn CPU. Nhìn chung có hai phương án tiếp cận đối với vấn đề này. Một cách là mở rộng bus bộ nhớ. Chẳng hạn NVIDIA’s RTX 2080 Ti dùng bus có kích thước 352 bit. Điều này cho phép truyền đi lượng thông tin lớn hơn cùng lúc. Một cách khác là sử dụng loại bộ nhớ chuyên biệt có hiệu năng cao cho GPU. Các thiết bị hạng phổ thông, điển hình như dòng RTX và Titan của NVIDIA, dùng các chip GDDR6 với băng thông tổng hợp hơn 500 GB/s. Một loại bộ nhớ chuyên biệt khác là mô-đun HBM (bộ nhớ băng thông rộng). Chúng dùng phương thức giao tiếp rất khác và kết nối trực tiếp với GPU trên một tấm bán dẫn silic chuyên biệt. Điều này dẫn đến giá thành rất cao và chúng chỉ được sử dụng chủ yếu cho các chip máy chủ cao cấp, ví dụ như dòng GPU NVIDIA Volta V100. Không quá ngạc nhiên, kích thước bộ nhớ GPU nhỏ hơn nhiều so với bộ nhớ CPU do giá thành cao của nó. Nhìn chung các đặc tính hiệu năng của bộ nhớ GPU khá giống bộ nhớ CPU, nhưng nhanh hơn nhiều. Ta có thể bỏ qua các chi tiết sâu hơn trong cuốn sách này, do chúng chỉ quan trọng khi cần điều chỉnh các hạt nhân GPU để đạt thông lượng xử lý cao hơn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.4.3. Lưu trữ\n",
    "\n",
    "Chúng ta đã thấy đặc tính then chốt của RAM chính là băng thông và độ trễ. Điều này cũng đúng đối với các thiết bị lưu trữ, sự khác biệt chỉ có thể là các đặc tính trên lớn hơn nhiều lần.\n",
    "\n",
    "Các ổ cứng đã được sử dụng hơn nửa thế kỷ. Một cách ngắn gọn, chúng chứa một số đĩa quay với những đầu kim có thể di chuyển để đọc/ghi ở bất cứ rãnh nào. Các ổ đĩa cao cấp có thể lưu trữ lên tới 16 TB trên 9 đĩa. Một trong những lợi ích chính của ổ đĩa cứng là chúng tương đối rẻ. Nhược điểm của chúng là độ trễ tương đối cao khi đọc dữ liệu và hay bị hư hỏng nặng dẫn đến không thể đọc dữ liệu, thậm chí là mất dữ liệu.\n",
    "\n",
    "Để hiểu về nhược điểm thứ hai, hãy xem xét thực tế rằng ổ cứng quay với tốc độ khoảng 7,200 vòng/phút. Nếu tốc độ này cao hơn, các đĩa sẽ vỡ tan do tác dụng của lực ly tâm. Điều này dẫn đến một nhược điểm lớn khi truy cập vào một khu vực cụ thể trên đĩa: chúng ta cần đợi cho đến khi đĩa quay đúng vị trí (chúng ta có thể di chuyển đầu kim nhưng không được tăng tốc các đĩa). Do đó, có thể mất hơn 8ms cho đến khi truy cập được dữ liệu yêu cầu. Vì thế mà ta hay nói ổ cứng có thể hoạt động ở mức xấp xỉ 100 IOP. Con số này về cơ bản vẫn không thay đổi trong hai thập kỷ qua. Tệ hơn nữa, việc tăng băng thông cũng khó khăn không kém (ở mức độ 100-200 MB/s). Rốt cuộc, mỗi đầu đọc một rãnh bit, do đó tốc độ bit chỉ tăng theo tỷ lệ căn bậc hai của mật độ thông tin. Kết quả là các ổ cứng đang nhanh chóng biến thành nơi lưu trữ cấp thấp cho các bộ dữ liệu rất lớn.\n",
    "\n",
    "**Ổ cứng thể rắn (SSD)** sử dụng bộ nhớ Flash để liên tục lưu trữ thông tin. Điều này cho phép truy cập nhanh hơn nhiều vào các bản ghi đã được lưu trữ. SSD hiện đại có thể hoạt động ở mức 100,000 đến 500,000 IOP, tức là nhanh hơn gấp 1000 lần so với ổ cứng HDD. Hơn nữa, băng thông của chúng có thể đạt tới 1-3GB/s nghĩa là nhanh hơn 10 lần so với ổ cứng. Những cải tiến này nghe có vẻ tốt đến mức khó tin. Thật vậy, và SSD cũng đi kèm với một số hạn chế do cách mà chúng được thiết kế.\n",
    "\n",
    "* Các ổ SSD lưu trữ thông tin theo khối (256 KB trở lên). Ta sẽ phải ghi cả khối cùng một lúc, mất thêm thời gian đáng kể. Do đó việc ghi ngẫu nhiên theo bit trên SSD có hiệu suất rất tệ. Tương tự như vậy, việc ghi dữ liệu nói chung mất thời gian đáng kể vì khối phải được đọc, xóa và sau đó viết lại với thông tin mới. Cho đến nay, bộ điều khiển và firmware của SSD đã phát triển các thuật toán để giảm thiểu vấn đề này. Tuy nhiên tốc độ ghi vẫn có thể chậm hơn nhiều, đặc biệt là đối với SSD QLC (ô bốn cấp). Chìa khóa để cải thiện hiệu suất là đưa các thao tác vào một hàng đợi để ưu tiên việc đọc trước và chỉ ghi theo các khối lớn nếu có thể.\n",
    "* Các ô nhớ trong SSD bị hao mòn tương đối nhanh (thường sau vài nghìn lần ghi). Các thuật toán bảo vệ mức hao mòn có thể phân bổ đều sự xuống cấp trên nhiều ô. Dù vậy, vẫn không nên sử dụng SSD cho các tệp hoán đổi (swap file) hoặc cho tập hợp lớn các tệp nhật ký (log file).\n",
    "* Cuối cùng, sự gia tăng lớn về băng thông đã buộc các nhà thiết kế máy tính phải gắn SSD trực tiếp vào bus PCIe. Các ổ đĩa có khả năng xử lý việc này, được gọi là NVMe (Bộ nhớ không biến động tăng cường - Non Volatile Memory enhanced), có thể sử dụng lên tới 4 làn PCIe. Băng thông có thể lên tới 8GB/s trên PCIe 4.0.\n",
    "\n",
    "Lưu trữ đám mây cung cấp nhiều lựa chọn hiệu suất có thể tùy chỉnh. Nghĩa là, việc chỉ định bộ lưu trữ cho các máy ảo là tùy chỉnh, cả về số lượng và tốc độ, do người dùng quyết định. Chúng tôi khuyên người dùng nên tăng số lượng IOP được cung cấp bất cứ khi nào độ trễ quá cao, ví dụ như trong quá trình huấn luyện với dữ liệu gồm nhiều bản ghi nhỏ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.4.4. CPU\n",
    "Bộ xử lý trung tâm (Central Processing Units - CPU) là trung tâm của mọi máy tính (như ở phần trước, chúng tôi đã mô tả tổng quan về những phần cứng quan trọng cho các mô hình học sâu hiệu quả). CPU gồm một số thành tố quan trọng: lõi xử lý (core) với khả năng thực thi mã máy,\n",
    "bus kết nối các lõi (cấu trúc kết nối cụ thể có sự khác biệt lớn giữa các mô hình xử lý, đời chip và nhà sản xuất) và bộ nhớ đệm (cache) cho phép truy cập với băng thông cao hơn và độ trễ thấp hơn so với việc đọc từ bộ nhớ chính. Cuối cùng, hầu hết CPU hiện đại chứa những đơn vị xử lý vector để hỗ trợ tính toán đại số tuyến tính và tích chập với tốc độ cao vì chúng khá phổ biến trong xử lý phương tiện và học máy.\n",
    "\n",
    "![](images/skylake.svg)\n",
    "<center>Fig. 12.4.3 CPU lõi tứ của bộ xử lý Intel Skylake</center>\n",
    "\n",
    "Fig. 12.4.3 minh hoạ bộ xử lý Intel Skylake với CPU lõi tứ. Nó có một GPU tích hợp, bộ nhớ cache và phương tiện kết nối bốn lõi. Thiết bị ngoại vi (Ethernet, WiFi, Bluetooth, bộ điều khiển SSD, USB, v.v.) là một phần của chipset hoặc được đính kèm trực tiếp (PCIe) với CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.4.5. Vi kiến trúc (Micro-architecture)\n",
    "\n",
    "Mỗi nhân xử lý bao gồm các thành phần rất tinh vi. Mặc dù chi tiết khác nhau giữa đời chip và nhà sản xuất, chức năng cơ bản của chúng đã được chuẩn hóa tương đối. Front-end tải các lệnh và dự đoán nhánh nào sẽ được thực hiện (ví dụ: cho luồng điều khiển). Sau đó các lệnh được giải mã từ mã nguồn hợp ngữ (assembly code) thành vi lệnh. Mã nguồn hợp ngữ thường chưa phải là mã nguồn cấp thấp nhất mà bộ xử lý thực thi. Thay vào đó, các lệnh phức tạp có thể được giải mã thành một tập hợp các phép tính cấp thấp hơn. Tiếp đó chúng được xử lý bằng một lõi thực thi. Các bộ xử lý đời mới thường có khả năng thực hiện đồng thời nhiều câu lệnh. Ví dụ, lõi ARM Cortex A77 trong Fig. 12.4.4 có thể thực hiện lên đến 8 phép tính cùng một lúc.\n",
    "\n",
    "![](images/a77.svg)\n",
    "\n",
    "Fig. 12.4.4 Tổng quan về vi kiến trúc ARM Cortex A77\n",
    "\n",
    "Điều này có nghĩa là các chương trình hiệu quả có thể thực hiện nhiều hơn một lệnh trên một chu kỳ xung nhịp, giả sử rằng chúng có thể được thực hiện một cách độc lập. Không phải tất cả các bộ xử lý đều được tạo ra như nhau. Một số được thiết kế chuyên biệt cho các lệnh về số nguyên, trong khi một số khác được tối ưu hóa cho việc tính toán số thực dấu phẩy động. Để tăng thông lượng, bộ xử lý cũng có thể theo đồng thời nhiều nhánh trong một lệnh rẽ nhánh và sau đó loại bỏ các kết quả của nhánh không được thực hiện. Đây là lý do vì sao đơn vị dự đoán nhánh có vai trò quan trọng (trên front-end), bởi chúng chỉ chọn những nhánh có khả năng cao được rẽ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.4.6. Vector hóa (Vectorization)\n",
    "\n",
    "Học sâu đòi hỏi sức mạnh tính toán cực kỳ lớn. Vì vậy, CPU phù hợp với học máy cần phải thực hiện được nhiều thao tác trong một chu kỳ xung nhịp. Ta có thể đạt được điều này thông qua các đơn vị vector. Trên chip ARM chúng được gọi là NEON, trên x86 thế hệ đơn vị vector mới nhất được gọi là AVX2. Một khía cạnh chung là chúng có thể thực hiện SIMD (**đơn lệnh đa dữ liệu - single instruction multiple data**). Fig. 12.4.5 cho thấy cách cộng 8 số nguyên ngắn trong một chu kỳ xung nhịp trên ARM.\n",
    "\n",
    "![](images/neon128.svg)\n",
    "\n",
    "Phụ thuộc vào các lựa chọn kiến trúc, các thanh ghi như vậy có thể dài tới 512 bit, cho phép tổ hợp tối đa 64 cặp số. Chẳng hạn, ta có thể nhân hai số và cộng chúng với số thứ ba, cách này còn được biết đến như phép nhân-cộng hợp nhất (fused multiply-add). OpenVino của Intel sử dụng thao tác này để đạt được thông lượng đáng nể cho học sâu trên CPU máy chủ. Tuy nhiên, xin lưu ý rằng tốc độ này hoàn toàn không đáng kể so với khả năng của GPU. Ví dụ, RTX 2080 Ti của NVIDIA có 4,352 nhân CUDA, mỗi nhân có khả năng xử lý một phép tính như vậy tại bất cứ thời điểm nào."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.4.7. Bộ nhớ đệm\n",
    "\n",
    "Xét tình huống sau: ta có một CPU bình thường với 4 nhân như trong Fig. 12.4.3 trên, hoạt động ở tần số 2 GHz. Thêm nữa, hãy giả sử IPC (instruction per clock - số lệnh mỗi xung nhịp) là 1 và mỗi nhân đều đã kích hoạt AVX2 rộng 256 bit. Ngoài ra, giả sử bộ nhớ cần truy cập ít nhất một thanh ghi được sử dụng trong các lệnh AVX2. Điều này có nghĩa CPU xử lý 4 x 256 bit = 1 kbit dữ liệu mỗi chu kỳ xung nhịp. Trừ khi ta có thể truyền 2⋅109⋅128=256⋅109\n",
    "byte đến vi xử lý mỗi giây, các nhân sẽ thiếu dữ liệu để xử lý. Tiếc thay giao diện bộ nhớ của bộ vi xử lý như trên chỉ hỗ trợ tốc độ truyền dữ liệu khoảng 20-40 GB/s, nghĩa là thấp hơn 10 lần. Để khắc phục vấn đề này, ta cần tránh nạp dữ liệu mới từ bộ nhớ ngoài, và tốt hơn hết là lưu trong bộ nhớ cục bộ trên CPU. Đây chính là lúc bộ nhớ đệm trở nên hữu ích (xem bài viết trên [Wikipedia](https://en.wikipedia.org/wiki/Cache_hierarchy) này để bắt đầu). Một số tên gọi/khái niệm thường gặp:\n",
    "\n",
    "   * **Thanh ghi** không phải là một bộ phận của bộ nhớ đệm. Chúng hỗ trợ sắp xếp các câu lệnh cho CPU. Nhưng dù sao thanh ghi cũng là một vùng nhớ mà CPU có thể truy cập với tốc độ xung nhịp mà không có độ trễ. Các CPU thường có hàng chục thanh ghi. Việc sử dụng các thanh ghi sao cho hiệu quả hoàn toàn phụ thuộc vào trình biên dịch (hoặc lập trình viên). Ví dụ như trong ngôn ngữ C, ta có thể sử dụng từ khóa register để lưu các biến vào thanh ghi thay vì bộ nhớ.\n",
    "   * **Bộ nhớ đệm L1** là lớp bảo vệ đầu tiên khi nhu cầu băng thông bộ nhớ quá cao. Bộ nhớ đệm L1 rất nhỏ (kích thước điển hình khoảng 32-64 kB) và thường được chia thành bộ nhớ đệm dữ liệu và câu lệnh. Nếu dữ liệu được tìm thấy trong bộ nhớ đệm L1, việc truy cập diễn ra rất nhanh chóng. Nếu không, việc tìm kiếm sẽ tiếp tục theo hệ thống phân cấp bộ nhớ đệm (cache hierarchy).\n",
    "   * **Bộ nhớ đệm L2** là điểm dừng tiếp theo. Vùng nhớ này có thể chuyên biệt tuỳ theo kiến trúc thiết kế và kích thước vi xử lý. Nó có thể chỉ được truy cập từ một lõi nhất định hoặc được chia sẻ với nhiều lõi khác nhau. Bộ nhớ đệm L2 có kích thước lớn hơn (thường là 256-512 kB mỗi lõi) và chậm hơn L1. Hơn nữa, để truy cập vào dữ liệu trong L2, đầu tiên ta cần kiểm tra để chắc rằng dữ liệu đó không nằm trong L1, việc này làm tăng độ trễ lên một chút.\n",
    "   * **Bộ nhớ đệm L3** được sử dụng chung cho nhiều lõi khác nhau và có thể khá lớn. CPU máy chủ Epyc 3 của AMD có bộ nhớ đệm 256MB cực lớn được phân bổ trên nhiều vi xử lý con (chiplet). Thường thì kích thước của L3 nằm trong khoảng 4-8MB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Việc dự đoán phần tử bộ nhớ nào sẽ cần tiếp theo là một trong những tham số tối ưu chính trong thiết kế vi xử lý. Ví dụ, việc duyệt xuôi bộ nhớ được coi là thích hợp do đa số các thuật toán ghi đệm (**caching algorithms**) sẽ cố gắng đọc về trước hơn là về sau. Tương tự, việc giữ hành vi truy cập bộ nhớ ở mức cục bộ là một cách tốt để cải thiện hiệu năng. Tăng số lượng bộ nhớ đệm là một con dao hai lưỡi. Một mặt việc này đảm bảo các nhân vi xử lý không bị thiếu dữ liệu. Mặt khác nó tăng kích thước vi xử lý, lấn chiếm phần diện tích mà đáng ra có thể được sử dụng vào việc tăng khả năng xử lý. Xét trường hợp tệ nhất như mô tả trong Fig. 12.4.6. Một địa chỉ bộ nhớ được lưu trữ tại vi xử lý 0 trong khi một luồng của vi xử lý 1 yêu cầu dữ liệu đó. Để có thể lấy dữ liệu, vi xử lý 0 phải dừng công việc đang thực hiện, ghi lại thông tin vào bộ nhớ chính để vi xử lý 1 đọc dữ liệu từ đó. Trong suốt quá trình này, cả hai vi xử lý đều ở trong trạng thái chờ. Một đoạn mã như vậy khả năng cao là sẽ chạy chậm hơn trên một hệ đa vi xử lý so với một vi xử lý đơn được lập trình hiệu quả. Đây là một lý do nữa cho việc tại sao thực tế phải giới hạn kích thước bộ nhớ đệm (**ngoài việc chiếm diện tích vật lý**).\n",
    "\n",
    "<img src=\"images/falsesharing.svg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.4.8. GPU và các Thiết bị Tăng tốc khác\n",
    "\n",
    "Không hề phóng đại khi nói rằng học sâu có lẽ sẽ không thành công nếu không có GPU. Và cũng nhờ có học sâu mà tài sản của các công ty sản suất GPU tăng trưởng đáng kể. Sự đồng tiến hóa giữa phần cứng và các thuật toán dẫn tới tình huống mà học sâu trở thành mẫu mô hình thống kê được ưa thích bất kể có hiệu quả hay không. Do đó, ta cần phải hiểu rõ ràng lợi ích mà GPU và các thiết bị tăng tốc khác như TPU [Jouppi et al., 2017](https://d2l.aivivn.com/chapter_references/zreferences.html#jouppi-young-patil-ea-2017) mang lại.\n",
    "\n",
    "Ta cần chú ý đến đặc thù thường được sử dụng trong thực tế: thiết bị tăng tốc được tối ưu hoặc cho bước huấn luyện hoặc cho bước suy luận. Đối với bước suy luận, ta chỉ cần tính toán lượt truyền xuôi qua mạng, không cần sử dụng bộ nhớ để lưu dữ liệu trung gian ở bước lan truyền ngược. Hơn nữa, ta có thể không cần đến phép tính quá chính xác (thường thì FP16 hoặc INT8 là đủ) Mặt khác trong quá trình huấn luyện, tất cả kết quả trung gian đều cần phải lưu lại để tính gradient. Hơn nữa, việc tích luỹ gradient yêu cầu độ chính xác cao hơn nhằm tránh lỗi tràn số trên hoặc dưới, do đó bước huấn luyện yêu cầu tối thiểu độ chính xác FP16 (hoặc độ chính xác hỗn hợp khi kết hợp với FP32). Tất cả các yếu tố trên đòi hỏi bộ nhớ nhanh hơn và lớn hơn (HBM2 hoặc GDDR6) và nhiều khả năng xử lý hơn. Ví dụ, GPU [Turing](https://developer.nvidia.com/blog/nvidia-turing-architecture-in-depth/) T4 của NVIDIA được tối ưu cho bước suy luận trong khi GPU V100 phù hợp cho quá trình huấn luyện.\n",
    "\n",
    "Xem lại Fig. 12.4.5. Việc thêm các đơn vị vector vào lõi vi xử lý cho phép ta tăng đáng kể thông lượng xử lý (ở ví dụ trong hình ta có thể thực hiện 16 thao tác cùng lúc). Chuyện gì sẽ xảy ra nếu ta không chỉ tối ưu cho phép tính giữa các vector mà còn tối ưu cho các ma trận? Chiến lược này dẫn tới sự ra đời của Lõi Tensor (chi tiết sẽ được thảo luận sau đây). Thứ hai, nếu tăng số lượng lõi thì sao? Nói tóm lại, hai chiến lược trên tóm tắt việc quyết định thiết kế của GPU. Fig. 12.4.7 mô tả tổng quan một khối xử lý đơn giản, bao gồm 16 đơn vị số nguyên và 16 đơn vị dấu phẩy động. Thêm vào đó, hai Lõi Tensor xử lý một tập nhỏ các thao thác liên quan đến học sâu được thêm vào. Mỗi Hệ vi xử lý Luồng (**Streaming Multiprocessor - SM**) bao gồm bốn khối như vậy.\n",
    "\n",
    "<img src=\"images/turing_processing_block.png\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12 hệ vi xử lý luồng sau đó được nhóm vào một cụm xử lý đồ hoạ tạo nên vi xử lý cao cấp TU102. Số lượng kênh bộ nhớ phong phú và bộ nhớ đệm L2 được bổ sung vào cấu trúc. Thông tin chi tiết được mô tả trong Fig. 12.4.8. Một trong những lý do để thiết kế một thiết bị như vậy là từng khối riêng biệt có thể được thêm vào hoặc bỏ đi tuỳ theo nhu cầu để có thể tạo thành một vi xử lý nhỏ gọn và giải quyết một số vấn đề phát sinh (các mô-đun lỗi có thể không được kích hoạt). May mắn thay, các nhà nghiên cứu học sâu bình thường không cần lập trình cho các thiết bị này do đã có các lớp mã nguồn framework CUDA ở tầng thấp. Cụ thể, có thể có nhiều hơn một chương trình được thực thi đồng thời trên GPU, với điều kiện là còn đủ tài nguyên. Tuy nhiên ta cũng cần để ý đến giới hạn của các thiết bị nhằm tránh việc lựa chọn mô hình quá lớn so với bộ nhớ của thiết bị.\n",
    "\n",
    "<img src=\"images/turing.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Khía cạnh cuối cùng đáng để bàn luận chi tiết là Lõi Tensor (**TensorCore**). Đây là một ví dụ của xu hướng gần đây là sử dụng thêm nhiều mạch đã được tối ưu để tăng hiệu năng cho học sâu. Ví dụ, TPU có thêm một mảng tâm thu (**systolic array**) [Kung, 1988] để tăng tốc độ nhân ma trận. Thiết kế của TPU chỉ hỗ trợ một số lượng rất ít các phép tính kích thước lớn (thế hệ TPU đầu tiên hỗ trợ một phép tính). Lõi Tensor thì ngược lại, được tối ưu cho các phép tính kích thước nhỏ cho các ma trận kích thước 4x4 đến 16x16, tuỳ vào độ chính xác số học. Fig. 12.4.9 mô tả tổng quan quá trình tối ưu.\n",
    "\n",
    "Đương nhiên khi tối ưu cho quá trình tính toán, ta buộc phải có một số đánh đổi nhất định. Một trong số đó là GPU không xử lý tốt dữ liệu ngắt quãng hoặc thưa. Trừ một số ngoại lệ đáng chú ý, ví dụ như [Gunrock](https://github.com/gunrock/gunrock) [Wang et al., 2016](https://d2l.aivivn.com/chapter_references/zreferences.html#wang-davidson-pan-ea-2016), việc truy cập vector và ma trận thưa không phù hợp với các thao tác đọc theo cụm (burst read) với băng thông cao của GPU. Đạt được cả hai mục tiêu là một lĩnh vực đang được đẩy mạnh nghiên cứu. Ví dụ, tham khảo [DGL](https://www.dgl.ai/), một thư viện được điều chỉnh cho phù hợp với học sâu trên đồ thị."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.4.9. Mạng máy tính và Bus\n",
    "\n",
    "Mỗi khi một thiết bị đơn không đủ cho quá trình tối ưu, ta cần chuyển dữ liệu đến và đi khỏi nó để đồng bộ hóa quá trình xử lý. Đây chính là lúc mà mạng máy tính và bus trở nên hữu dụng. Ta có một vài tham số thiết kế gồm: băng thông, chi phí, khoảng cách và tính linh hoạt. Tuy ta cũng có Wifi với phạm vi hoạt động tốt, dễ dàng để sử dụng (dù sao cũng là không dây), rẻ nhưng lại có băng thông không quá tốt và độ trễ lớn. Sẽ không có bất cứ nhà nghiên cứu học máy tỉnh táo nào lại nghĩ đến việc sử dụng Wifi để xây dựng một cụm máy chủ. Sau đây, ta sẽ chỉ tập trung vào các cách kết nối phù hợp cho học sâu.\n",
    "\n",
    "   * PCIe là một bus riêng chỉ phục vụ cho kết nối điểm – điểm với băng thông trên mỗi làn rất lớn (lên đến 16 GB/s trên PCIe 4.0). Độ trễ thường có giá trị cỡ vài micro giây (5 μs). Kết nối PCIe khá quan trọng. Vi xử lý chỉ có một số lượng làn PCIe nhất định: EPYC 3 của AMD có 128 làn, Xeon của Intel lên đến 48 làn cho mỗi chip; trên CPU dùng cho máy tính để bàn, số lượng này lần lượt là 20 (với Ryzen 9) và 16 (với Core i9). Do GPU thường có 16 luồng nên số lượng GPU có thể kết nối với CPU bị giới hạn tại băng thông tối đa. Xét cho cùng, chúng cần chia sẻ liên kết với các thiết bị ngoại vi khác như bộ nhớ và cổng Ethernet. Giống như việc truy cập RAM, việc truyền lượng lớn dữ liệu thường được ưa chuộng hơn nhằm giảm tổng chi phí theo gói tin.\n",
    "   * Ethernet là cách phổ biến nhất để kết nối máy tính với nhau. Dù nó chậm hơn đáng kể so với PCIe, nó rất rẻ và dễ cài đặt, bao phủ khoảng cách lớn hơn nhiều. Băng thông đặc trưng đối với máy chủ cấp thấp là 1 GBit/s. Các thiết bị cao cấp hơn (ví dụ như máy chủ loại [C5](https://aws.amazon.com/ec2/instance-types/c5/) trên AWS) cung cấp băng thông từ 10 đến 100 GBit/s. Cũng như các trường hợp trên, việc truyền dữ liệu có tổng chi phí đáng kể. Chú ý rằng ta hầu như không bao giờ sử dụng trực tiếp Ethernet thuần mà sử dụng một giao thức được thực thi ở tầng trên của kết nối vật lý (ví dụ như UDP hay TCP/IP). Việc này làm tăng tổng chi phí. Giống như PCIe, Ethernet được thiết kế để kết nối hai thiết bị, ví dụ như máy tính với một bộ chuyển mạch (switch).\n",
    "   * Bộ chuyển mạch cho phép ta kết nối nhiều thiết bị theo cách mà bất cứ cặp thiết bị nào cũng có thể (thường là với băng thông tối đa) thực hiện kết nối điểm – điểm cùng lúc. Ví dụ, bộ chuyển mạch Ethernet có thể kết nối 40 máy chủ với băng thông xuyên vùng (cross-sectional bandwidth) cao. Chú ý rằng bộ chuyển mạch không phải chỉ có trong mạng máy tính truyền thống. Ngay cả làn PCIe cũng có thể [chuyển mạch](https://www.broadcom.com/products/pcie-switches-bridges/pcie-switches). Điều này xảy ra khi kết nối một lượng lớn GPU tới vi xử lý chính, như với trường hợp [máy chủ loại P2](https://aws.amazon.com/ec2/instance-types/p2/).\n",
    "   * NVLink là một phương pháp thay thế PCIe khi ta cần kết nối với băng thông rất lớn. NVLink cung cấp tốc độ truyền dữ liệu lên đến 300 Gbit/s mỗi đường dẫn (link). GPU máy chủ (Volta V100) có 6 đường dẫn, trong khi GPU thông dụng (RTX 2080 Ti) chỉ có một đường dẫn, hoạt động ở tốc độ thấp 100 Gbit/s. Vì vậy, chúng tôi gợi ý sử dụng [NCCL](https://github.com/NVIDIA/nccl) để có thể đạt được tốc độ truyền dữ liệu cao giữa các GPU.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.4.10. Tóm tắt\n",
    "\n",
    "   * Các thiết bị đều có chi phí phụ trợ trên mỗi hành động. Do đó ta nên nhắm tới việc di chuyển ít lần các lượng dữ liệu lớn thay vì di chuyển nhiều lần các lượng dữ liệu nhỏ. Điều này đúng với RAM, SSD, các thiết bị mạng và GPU.\n",
    "   * Vector hóa rất quan trọng để tăng hiệu năng. Hãy đảm bảo bạn hiểu các điểm mạnh đặc thù của thiết bị tăng tốc mình đang có. Ví dụ, một vài CPU Intel Xeon thực hiện cực kỳ hiệu quả phép toán với dữ liệu kiểu INT8, GPU NVIDIA Volta rất phù hợp với các phép toán với ma trận dữ liệu kiểu FP16; còn NVIDIA Turing chạy tốt cho cả các phép toán với dữ liệu kiểu FP16, INT8, INT4.\n",
    "   * Hiện tượng tràn số trên do kiểu dữ liệu không đủ số bit để biểu diễn giá trị có thể là một vấn đề khi huấn luyện (và cả khi suy luận, dù ít nghiêm trọng hơn).\n",
    "   * Việc cùng dữ liệu nhưng có nhiều địa chỉ (aliasing) có thể làm giảm đáng kể hiệu năng. Ví dụ, việc sắp xếp dữ liệu trong bộ nhớ (memory alignment) trên CPU 64 bit nên được thực hiện theo từng khối 64 bit. Trên GPU, tốt hơn là nên giữ kích thước tích chập đồng bộ, với TensorCores chẳng hạn.\n",
    "   * Sử dụng thuật toán phù hợp với phần cứng (về mức chiếm dụng bộ nhớ, băng thông, v.v). Thời gian thực thi có thể giảm hàng trăm ngàn lần khi tất cả tham số đều được chứa trong bộ đệm.\n",
    "   * Chúng tôi khuyến khích bạn đọc tính toán trước hiệu năng của một thuật toán mới trước khi kiểm tra bằng thực nghiệm. Sự khác biệt lên tới hàng chục lần hoặc hơn là dấu hiệu cần quan tâm.\n",
    "   * Sử dụng các công cụ phân tích hiệu năng (profiler) để tìm điểm nghẽn cổ chai của hệ thống.\n",
    "   * Phần cứng sử dụng cho huấn luyện và suy luận có các cấu hình hiệu quả khác nhau để cân đối giá tiền và hiệu năng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.4.11. Độ trễ\n",
    "\n",
    "Các thông tin trong **table_latency_numbers** và **table_latency_numbers_tesla** được [Eliot Eshelman](https://gist.github.com/eshelman) duy trì cập nhật trên [GitHub Gist](https://gist.github.com/eshelman/343a1c46cb3fba142c1afdcdeec17646).\n",
    "\n",
    ":Các độ trễ thường gặp.\n",
    "\n",
    "|                         Hoạt động                         | Th ời gi an |                 Chú thích                |\n",
    "|:---------------------------------------------------------:|:-----------:|:----------------------------------------:|\n",
    "| Truy xuất bộ đệm L1                                       | 1. 5 ns     | 4 chu kỳ                                 |\n",
    "| Cộng, nhân, cộng kết hợp nhân (FMA) số thực dấu phẩy động | 1. 5 ns     | 4 chu kỳ                                 |\n",
    "| Truy xuất bộ đệm L2                                       | 5 ns        | 12 ~ 17 chu kỳ                           |\n",
    "| Rẽ nhánh sai                                              | 6 ns        | 15 ~ 20 chu kỳ                           |\n",
    "| Truy xuất bộ đệm L3 (không chia sẻ)                       | 16 ns       | 42 chu kỳ                                |\n",
    "| Truy xuất bộ đệm L3 (chia sẻ với nhân khác)               | 25 ns       | 65 chu kỳ                                |\n",
    "| Khóa/mở đèn báo lập trình (mutex)                         | 25 ns       |                                          |\n",
    "| Truy xuất bộ đệm L3 (được nhân khác thay đổi)             | 29 ns       | 75 chu kỳ                                |\n",
    "| Truy xuất bộ đệm L3 (tại CPU socket từ xa)                | 40 ns       | 100 ~ 300 chu kỳ (40 ~ 116 ns)           |\n",
    "| QPI hop đến CPU khác (cho mỗi hop)                        | 40 ns       |                                          |\n",
    "| Truy xuất 64MB (CPU cục bộ)                               | 46 ns       | TinyMemBench trên Broadwell E5-2690v4    |\n",
    "| Truy xuất 64MB (CPU từ xa)                                | 70 ns       | TinyMemBench trên Broadwell E5-2690v4    |\n",
    "| Truy xuất 256MB (CPU cục bộ)                              | 75 ns       | TinyMemBench trên Broadwell E5-2690v4    |\n",
    "| Ghi ngẫu nhiên vào Intel Optane                           | 94 ns       | UCSD Non-Volatile Systems Lab            |\n",
    "| Truy xuất 256MB (CPU từ xa)                               | 12 0 ns     | TinyMemBench trên Broadwell E5-2690v4    |\n",
    "| Đọc ngẫu nhiên từ Intel Optane                            | 30 5 ns     | UCSD Non-Volatile Systems Lab            |\n",
    "| Truyền 4KB trên sợi HPC 100 Gbps                          | 1 μs        | MVAPICH2 trên Intel Omni-Path            |\n",
    "| Nén 1KB với Google Snappy                                 | 3 μs        |                                          |\n",
    "| Truyền 4KB trên cáp mạng 10 Gbps                          | 10 μs       |                                          |\n",
    "| Ghi ngẫu nhiên 4KB vào SSD NVMe                           | 30 μs       | DC P3608 SSD NVMe (QOS 99% khoảng 500μs) |\n",
    "| Truyền 1MB từ/đến NVLink GPU                              | 30 μs       | ~33GB/s trên NVIDIA 40GB NVLink          |\n",
    "| Truyền 1MB từ/đến PCI-E GPU                               | 80 μs       | ~12GB/s trên PCIe 3.0 x16 link           |\n",
    "| Đọc ngẫu nhiên 4KB từ SSD NVMe                            | 12 0 μs     | DC P3608 SSD NVMe (QOS 99%)              |\n",
    "| Đọc tuần tự 1MB từ SSD NVMe                               | 20 8 μs     | ~4.8GB/s DC P3608 SSD NVMe               |\n",
    "| Ghi ngẫu nhiên 4KB vào SSD SATA                           | 50 0 μs     | DC S3510 SSD SATA (QOS 99.9%)            |\n",
    "| Đọc ngẫu nhiên 4KB từ SSD SATA                            | 50 0 μs     | DC S3510 SSD SATA (QOS 99.9%)            |\n",
    "| Truyền 2 chiều trong cùng trung tâm dữ liệu               | 50 0 μs     | Ping một chiều ~250μs                    |\n",
    "| Đọc tuần tự 1MB từ SSD SATA                               | 2 ms        | ~550MB/s DC S3510 SSD SATA               |\n",
    "| Đọc tuần tự 1MB từ ổ đĩa                                  | 5 ms        | ~200MB/s server HDD                      |\n",
    "| Truy cập ngẫu nhiên ổ đĩa (tìm + xoay)                    | 10 ms       |                                          |\n",
    "| Gửi gói dữ liệu từ California -> Hà Lan -> California     | 15 0 ms     |                                          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":Độ trễ của GPU NVIDIA Tesla.\n",
    "\n",
    "|             Hoạt động            | Thời gian |                   Chú thích                  |\n",
    "|:--------------------------------:|:---------:|:--------------------------------------------:|\n",
    "| Truy cập bộ nhớ chung của GPU    | 30 ns     | 30~90 chu kỳ (tính cả xung đột của các bank) |\n",
    "| Truy cập bộ nhớ toàn cục của GPU | 200 ns    | 200~800 chu kỳ                               |\n",
    "| Khởi chạy nhân CUDA trên GPU     | 10 μs     | CPU host ra lệnh cho GPU khởi chạy nhân      |\n",
    "| Truyền 1MB từ/đến GPU NVLink     | 30 μs     | ~33GB/s trên NVIDIA NVLink 40GB              |\n",
    "| Truyền 1MB từ/đến GPU PCI-E      | 80 μs     | ~12GB/s trên PCI-Express link x16            |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.4.12. Bài tập\n",
    "\n",
    "   * Viết đoạn mã C để so sánh tốc độ khi truy cập bộ nhớ được sắp xếp theo khối (aligned memory) với khi truy cập bộ nhớ không được sắp xếp như vậy (một cách tương đối so với bộ nhớ ngoài). Gợi ý: hãy loại bỏ hiệu ứng của bộ nhớ đệm.\n",
    "   * So sánh tốc độ khi truy cập bộ nhớ tuần tự với khi truy cập theo sải bước cho trước.\n",
    "   * Làm thế nào để đo kích thước bộ nhớ đệm trên CPU?\n",
    "   * Bạn sẽ sắp xếp dữ liệu trên nhiều bộ nhớ như thế nào để có băng thông tối đa? Sắp xếp như thế nào nếu bạn có nhiều luồng nhỏ?\n",
    "   * Tốc độ quay của một ổ cứng HDD dùng cho công nghiệp là 10,000 rpm. Thời gian tối thiểu mà HDD đó cần (trong trường hợp tệ nhất) trước khi có thể đọc dữ liệu là bao nhiêu (có thể giả sử các đầu đọc ổ đĩa di chuyển tức thời)?\n",
    "   * Giả sử nhà sản xuất HDD tăng sức chứa bộ nhớ từ 1 Tbit mỗi inch vuông lên 5 Tbit mỗi inch vuông. Có thể lưu bao nhiêu dữ liệu trên một đĩa từ của một HDD 2.5”? Có sự khác biệt nào giữa track trong và track ngoài không?\n",
    "   * Một máy chủ loại P2 trên AWS có 16 GPU K80 Kepler. Sử dụng lệnh lspci trên một máy p2.16xlarge và một máy p2.8xlarge để hiểu cách các GPU được kết nối với các CPU. Gợi ý: để ý đến chip cầu nối PLX cho chuẩn kết nối PCI.\n",
    "   * Chuyển từ kiểu dữ liệu 8 bit sang 16 bit cần lượng silicon gấp 4 lần. Tại sao? Tại sao NVIDIA thêm các phép toán cho kiểu dữ liệu INT4 vào GPU Turing?\n",
    "   * Có 6 đường truyền tốc độ cao giữa các GPU (như GPU Volta V100 chẳng hạn), bạn sẽ kết nối 8 GPU đó như thế nào? Tham khảo cách kết nối cho máy chủ p3.16xlarge trên AWS.\n",
    "   * Đọc xuôi bộ nhớ nhanh gấp bao nhiêu lần đọc ngược? Sự chênh lệch này có khác nhau giữa các nhà sản xuất máy tính và CPU không? Tại sao? Thí nghiệm với mã nguồn C.\n",
    "   * Bạn có thể đo kích thước bộ nhớ đệm trên ổ đĩa của mình không? Bộ nhớ đệm trên HDD là gì? SSD có cần bộ nhớ đệm không?\n",
    "   * Chi phí bộ nhớ phụ trợ khi gửi một gói dữ liệu qua cáp mạng (Ethernet) là bao nhiêu. So sánh các giao thức UDP và TCP/IP.\n",
    "   * Truy cập Bộ nhớ Trực tiếp (Direct Memory Access) cho phép các thiết bị khác ngoài CPU ghi (và đọc) trực tiếp vào (từ) bộ nhớ. Tại sao đây là một ý tưởng hay?\n",
    "   * Nhìn vào thông số hiệu năng của GPU Turing T4. Tại sao hiệu năng chỉ tăng gấp đôi khi chuyển từ phép toán với kiểu dữ liệu FP16 sang INT8 và INT4?\n",
    "   * Thời gian truyền một gói dữ liệu hai chiều giữa San Francisco và Amsterdam là bao nhiêu? Gợi ý: giả sử khoảng cách giữa 2 thành phố là 10,000km.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.5. Huấn luyện đa GPU\n",
    "\n",
    "Đến nay ta đã thảo luận về cách huấn luyện mô hình trên CPU và GPU một cách hiệu quả. Trong Section 12.3, ta biết được cách mà các framework học sâu như MXNet (và TensorFlow) thực hiện song song hóa việc tính toán và giao tiếp giữa các thiết bị một cách tự động. Cuối cùng, Section 5.6 đã trình bày cách liệt kê toàn bộ các GPU có trong máy bằng lệnh nvidia-smi. Thứ mà ta chưa thảo luận là cách song song hóa quá trình huấn luyện mô hình học sâu. (Ta bỏ qua việc dự đoán trên nhiều GPU vì nó ít khi được sử dụng và là một chủ đề nâng cao nằm ngoài phạm vi của cuốn sách này.) Chúng ta mới chỉ ngầm hiểu rằng bằng cách nào đó dữ liệu có thể được chia ra cho nhiều thiết bị khác nhau. Phần này sẽ bổ sung những chi tiết còn thiếu ấy và mô tả cách huấn luyện song song một mạng học sâu từ đầu. Chi tiết về cách tận dụng các tính năng của Gluon sẽ nằm ở Section 12.6. Vì vậy, chúng tôi xin giả định rằng độc giả đã quen với thuật toán SGD theo minibatch được mô tả ở Section 11.9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.5.1. Chia nhỏ Vấn đề\n",
    "\n",
    "Hãy bắt đầu bằng một bài toán thị giác máy tính đơn giản cùng một kiến trúc mạng lâu đời chứa vài tầng tích chập, tầng gộp và có thể thêm vài tầng dày đặc ở cuối. Như vậy, mạng này sẽ trông khá tương tự như LeNet [LeCun et al., 1998] hoặc AlexNet [Krizhevsky et al., 2012]. Với nhiều GPU (máy chủ để bàn thường có 2, máy chủ g4dn.12xlarge thì có 4, AWS p3.16xlarge có 8, hoặc là 16 trên p2.16xlarge), ta muốn phân chia việc huấn luyện sao cho vừa tăng tốc độ lại vừa tận dụng được các thiết kế đơn giản và tái tạo được. Sau cùng, việc sử dụng nhiều GPU là để tăng cả **bộ nhớ** và năng lực **tính toán**. Nói ngắn gọn, với một minibatch dữ liệu huấn luyện, ta có một vài phương án phân chia khác nhau.\n",
    "\n",
    "![](images/alexnet-original.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "   * Chúng ta có thể phân chia các tầng mạng trên nhiều GPU. Cụ thể, mỗi GPU sẽ nhận một luồng dữ liệu đưa vào từ một tầng xác định, truyền dữ liệu qua một số tầng kế tiếp nhau rồi gửi dữ liệu tới GPU kế tiếp.\n",
    "       * Điều này cho phép ta xử lý dữ liệu với các mạng lớn hơn, điều nằm ngoài khả năng khi chỉ sử dụng một GPU.\n",
    "       * Bộ nhớ bị chiếm dụng trên mỗi GPU có thể được kiểm soát dễ dàng (mỗi GPU sẽ chỉ chiếm một phần tổng dung lượng bộ nhớ cấp phát cho cả mạng).\n",
    "       * Giao tiếp giữa các tầng (cũng như giữa các GPU) đòi hỏi tính đồng bộ chặt chẽ. Điều này có thể sẽ rất khó, đặc biệt nếu khối lượng tính toán không được phân chia hợp lý cho các tầng. Vấn đề sẽ trở nên nghiêm trọng với một số lượng lớn GPU.\n",
    "       * Giao tiếp giữa các tầng yêu cầu một lượng lớn các thao tác truyền dữ liệu (các hàm kích hoạt, các gradient). Điều này có thể vượt quá mức băng thông các bus của GPU.\n",
    "       * Các phép tính tuần tự nhưng nặng về mặt tính toán lại không hề dễ phân chia. [Mirhoseini et al., 2017] là nỗ lực tốt nhất để giải quyết vấn đề này. Nó vẫn còn là một vấn đề khó và chưa rõ ràng liệu có thể đạt được khả năng mở rộng tốt (tăng theo tuyến tính) cho các bài toán không quá đơn giản không. Chúng tôi không khuyến khích cách làm này trừ phi có một framework xuất sắc hay một hệ điều hành hỗ trợ cho việc xâu chuỗi nhiều GPU lại với nhau.\n",
    "   * Chúng ta có thể phân chia công việc của các tầng đơn lẻ. Chẳng hạn, thay vì tính toán 64 kênh trên một GPU, ta có thể chia công việc này cho 4 GPU, mỗi GPU sẽ sinh dữ liệu cho 16 kênh. Tương tự, với một tầng kết nối dày đặc ta có thể chia nhỏ số nơ-ron đầu ra. Fig. 12.5.1 mô tả thiết kế kiểu này. Hình này được trích từ [Krizhevsky et al., 2012], khi chiến lược này được sử dụng để làm việc với nhiều GPU có dung lượng bộ nhớ rất nhỏ (2 GB ở thời điểm đó).\n",
    "       * Điều này cho phép việc điều chỉnh kích thước tính toán tốt, với điều kiện là số kênh (hoặc số nơ-ron) không quá nhỏ.\n",
    "       * Dùng nhiều GPU có thể xử lý các mạng ngày một lớn hơn vì dung lượng bộ nhớ khả dụng cũng tăng tuyến tính.\n",
    "       * Chúng ta cần một lượng rất lớn các phép toán đồng bộ / lớp chặn vì mỗi tầng phụ thuộc vào các kết quả từ tất cả các tầng khác.\n",
    "       * Lượng dữ liệu cần được truyền thậm chí có thể lớn hơn khi phân phối các tầng giữa các GPU. Chúng tôi không khuyến khích cách tiếp cận này do tính phức tạp và chi phí băng thông của nó.\n",
    "   * Cuối cùng, ta có thể phân chia dữ liệu cho nhiều GPU. Cách này cho phép tất cả GPU thực hiện cùng một công việc, chỉ là với các dữ liệu khác nhau. Các gradient được tổng hợp lại trên các GPU sau mỗi minibatch.\n",
    "       * Đây là phương pháp đơn giản nhất và có thể sử dụng cho bất cứ tình huống nào.\n",
    "       * Gắn thêm nhiều GPU không cho phép chúng ta huấn luyện mô hình lớn hơn.\n",
    "       * Chúng ta chỉ cần đồng bộ hóa sau mỗi minibatch. Dù vậy, ta vẫn nên bắt đầu thực hiện trao đổi các gradient đã tính xong kể cả khi việc tính các gradient khác vẫn chưa được hoàn thiện.\n",
    "       * Số lượng GPU lớn dẫn tới kích thước minibatch rất lớn, do đó giảm hiệu quả huấn luyện.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/splitting.svg\" width=700>\n",
    "Nhìn chung việc song song hóa dữ liệu là cách thuận tiện nhất, với điều kiện là ta sở hữu các GPU với bộ nhớ đủ lớn. Xem thêm [Li et al., 2014] để biết chi tiết cách phân chia cho việc huấn luyện phân tán. Bộ nhớ GPU từng là một vấn đề trong những ngày đầu của học sâu. Đến thời điểm này thì hầu hết các vấn đề đã được giải quyết trừ một số trường hợp rất ít gặp. Ở phần kế tiếp, chúng ta sẽ tập trung vào việc song song hóa dữ liệu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.5.2. Song song hóa Dữ liệu\n",
    "\n",
    "Giả sử ta có một máy tính có k\n",
    "\n",
    "GPU. Với một mô hình cần được huấn luyện, mỗi GPU duy trì một tập đầy đủ các tham số mô hình độc lập với nhau. Việc huấn luyện diễn ra như sau (xem Fig. 12.5.3 để rõ hơn về việc huấn luyện song song với hai GPU):\n",
    "\n",
    "   * Ở bất cứ vòng huấn luyện nào, với một tập minibatch ngẫu nhiên cho trước, ta chia đều các mẫu từ batch ban đầu này thành $k$ phần rồi phân bố cho các GPU.\n",
    "* Mỗi GPU sẽ tính mất mát và gradient của các tham số mô hình dựa trên tập mimibatch con mà nó được cấp và các tham số mô hình nó lưu trữ.\n",
    "* Các gradient cục bộ từ $k$ GPU được gom lại để thu được gradient ngẫu nhiên cho minibatch hiện tại.\n",
    "* Gradient tổng hợp này được phân phối trở lại cho các GPU.\n",
    "* Mỗi GPU dùng gradient ngẫu nhiên của minibatch này để cập nhật một tập đầy đủ các tham số mô hình mà nó lưu trữ.\n",
    "\n",
    "![](images/data-parallel.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fig. 12.5.2 so sánh các cách song song hóa khác nhau trên nhiều GPU. Lưu ý rằng trong thực tế ta cần tăng kích thước minibatch lên k lần khi huấn luyện trên k GPU để mỗi GPU có cùng khối lượng công việc cần thực hiện như khi ta huấn luyện trên một GPU đơn lẻ. Trên một server có 16 GPU có thể tăng kích thước minibatch một cách đáng kể và ta cũng có thể sẽ phải tăng tốc độ học một cách tương ứng. Chú ý rằng Section 7.5 cũng cần được điều chỉnh lại (ví dụ, ta có thể sử dụng các hệ số chuẩn hóa theo batch riêng cho mỗi GPU). Trong phần tiếp theo ta sẽ dùng Section 6.6 như một mạng thử nghiệm để minh họa việc huấn luyện đa GPU. Như mọi khi, ta bắt đầu bằng cách nạp các gói thư viện và mô-đun liên quan.\n",
    "\n",
    "![](images/splitting_2.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from d2l import torch as d2l\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.5.3. Ví dụ Đơn giản\n",
    "\n",
    "Ta lập trình từ đầu LeNet trong Section 6.6 để minh họa chi tiết cách trao đổi và đồng bộ tham số."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model parameters\n",
    "scale = 0.01\n",
    "W1 = torch.randn(size=(20, 1, 3, 3)) * scale\n",
    "b1 = torch.zeros(20)\n",
    "W2 = torch.randn(size=(50, 20, 5, 5)) * scale\n",
    "b2 = torch.zeros(50)\n",
    "W3 = torch.randn(size=(800, 128)) * scale\n",
    "b3 = torch.zeros(128)\n",
    "W4 = torch.randn(size=(128, 10)) * scale\n",
    "b4 = torch.zeros(10)\n",
    "params = [W1, b1, W2, b2, W3, b3, W4, b4]\n",
    "\n",
    "# Define the model\n",
    "def lenet(X, params):\n",
    "    h1_conv = F.conv2d(input=X, weight=params[0], bias=params[1])\n",
    "    h1_activation = F.relu(h1_conv)\n",
    "    h1 = F.avg_pool2d(input=h1_activation, kernel_size=(2, 2), stride=(2, 2))\n",
    "    h2_conv = F.conv2d(input=h1, weight=params[2], bias=params[3])\n",
    "    h2_activation = F.relu(h2_conv)\n",
    "    h2 = F.avg_pool2d(input=h2_activation, kernel_size=(2, 2), stride=(2, 2))\n",
    "    h2 = h2.reshape(h2.shape[0], -1)\n",
    "    h3_linear = torch.mm(h2, params[4]) + params[5]\n",
    "    h3 = F.relu(h3_linear)\n",
    "    y_hat = torch.mm(h3, params[6]) + params[7]\n",
    "    return y_hat\n",
    "\n",
    "# Cross-entropy loss function\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.5.4. Đồng bộ Dữ liệu\n",
    "\n",
    "Để huấn luyện hiệu quả trên nhiều GPU, ta cần hai thao tác cơ bản: thứ nhất là phân phối danh sách tham số đến nhiều GPU và gắn gradient, được định nghĩa trong hàm `get_params` dưới đây. Nếu không có các tham số, ta không thể đánh giá mạng trên GPU. Thứ hai, ta cần tính tổng giá trị các tham số trên nhiều thiết bị, khai báo ở hàm `allreduce`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(params, device):\n",
    "    new_params = [p.clone().to(device) for p in params]\n",
    "    for p in new_params:\n",
    "        p.requires_grad_()\n",
    "    return new_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hãy thử sao chép các tham số mô hình của LeNet tới `gpu(0)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_params = get_params(params, d2l.try_gpu(0))\n",
    "print('b1 weight:', new_params[1])\n",
    "print('b1 grad:', new_params[1].grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vì chưa thực hiện tính toán nào, gradient ứng với hệ số điều chỉnh vẫn mang giá trị 0. Bây giờ giả sử ta có các vector được phân phối trên nhiều GPU. Hàm allreduce dưới đây cộng các vector đó và truyền kết quả về tất cả GPU. Chú ý, để hàm này hoạt động, ta cần sao chép dữ liệu đến GPU đang cộng dồn kết quả."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allreduce(data):\n",
    "    for i in range(1, len(data)):\n",
    "        data[0][:] += data[i].to(data[0].device)\n",
    "    for i in range(1, len(data)):\n",
    "        data[i] = data[0].to(data[i].device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hãy kiểm tra bằng cách tạo các vector với giá trị khác nhau trên các thiết bị khác nhau và tổng hợp chúng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [torch.ones((1, 2), device=d2l.try_gpu(i)) * (i + 1) for i in range(2)]\n",
    "print('before allreduce:\\n', data[0], '\\n', data[1])\n",
    "allreduce(data)\n",
    "print('after allreduce:\\n', data[0], '\\n', data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.5.5. Phân phối Dữ liệu\n",
    "\n",
    "Ta cần một hàm hỗ trợ phân phối đều dữ liệu trong minibatch trên nhiều GPU. Ví dụ với 2 GPU, có thể ta sẽ muốn sao chép một nửa dữ liệu tới mỗi GPU. Ta sẽ sử dụng hàm có sẵn trong Gluon để chia và nạp dữ liệu (kiểm thử với ma trận $4×5$\n",
    ")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.arange(20).reshape(4, 5)\n",
    "devices = [torch.device('cuda:0'), torch.device('cuda:1')]\n",
    "split = nn.parallel.scatter(data, devices)\n",
    "print('input :', data)\n",
    "print('load into', devices)\n",
    "print('output:', split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để sử dụng về sau, ta định nghĩa hàm `split_batch` để chia cả dữ liệu và nhãn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def split_batch(X, y, devices):\n",
    "    \"\"\"Split `X` and `y` into multiple devices.\"\"\"\n",
    "    assert X.shape[0] == y.shape[0]\n",
    "    return (nn.parallel.scatter(X, devices),\n",
    "            nn.parallel.scatter(y, devices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.5.6. Huấn luyện\n",
    "\n",
    "Giờ chúng ta có thể lập trình việc huấn luyện với một minibatch trên nhiều GPU. Đoạn mã chủ yếu dựa trên phương pháp song song hóa dữ liệu trong chương này. Ta sẽ dùng các hàm phụ trợ `allreduce` và `split_and_load` ở trên để đồng bộ dữ liệu trên nhiều GPU. Lưu ý rằng ta không cần viết bất cứ đoạn mã cụ thể nào để song song hóa. Vì đồ thị tính toán không có phụ thuộc nào xuyên suốt các thiết bị trong một minibatch, chúng được thực thi song song **một cách tự động**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(X, y, device_params, devices, lr):\n",
    "    X_shards, y_shards = split_batch(X, y, devices)\n",
    "    # Loss is calculated separately on each GPU\n",
    "    losses = [loss(lenet(X_shard, device_W), y_shard).sum()\n",
    "              for X_shard, y_shard, device_W in zip(\n",
    "                  X_shards, y_shards, device_params)]\n",
    "    for l in losses:  # Back Propagation is performed separately on each GPU\n",
    "        l.backward()\n",
    "    # Sum all gradients from each GPU and broadcast them to all GPUs\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(device_params[0])):\n",
    "            allreduce([device_params[c][i].grad for c in range(len(devices))])\n",
    "    # The model parameters are updated separately on each GPU\n",
    "    for param in device_params:\n",
    "        d2l.sgd(param, lr, X.shape[0]) # Here, we use a full-size batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bây giờ ta có thể định nghĩa hàm huấn luyện. Hàm này có một chút khác biệt so với hàm huấn luyện trong các chương trước: ta cần chỉ định GPU và sao chép các tham số mô hình tới tất cả thiết bị. Mỗi batch được xử lý bằng `train_batch` nhằm tận dụng nhiều GPU. Để thuận tiện (và để mã nguồn ngắn gọn), ta tính độ chính xác trên một GPU (cách này **không hiệu quả** vì các GPU khác không được tận dụng)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_gpus, batch_size, lr):\n",
    "    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "    devices = [d2l.try_gpu(i) for i in range(num_gpus)]\n",
    "    # Copy model parameters to num_gpus GPUs\n",
    "    device_params = [get_params(params, d) for d in devices]\n",
    "    # num_epochs, times, acces = 10, [], []\n",
    "    num_epochs = 10\n",
    "    animator = d2l.Animator('epoch', 'test acc', xlim=[1, num_epochs])\n",
    "    timer = d2l.Timer()\n",
    "    for epoch in range(num_epochs):\n",
    "        timer.start()\n",
    "        for X, y in train_iter:\n",
    "            # Perform multi-GPU training for a single minibatch\n",
    "            train_batch(X, y, device_params, devices, lr)\n",
    "            torch.cuda.synchronize()\n",
    "        timer.stop()\n",
    "        # Verify the model on GPU 0\n",
    "        animator.add(epoch + 1, (d2l.evaluate_accuracy_gpu(\n",
    "            lambda x: lenet(x, device_params[0]), test_iter, devices[0]),))\n",
    "    print(f'test acc: {animator.Y[0][-1]:.2f}, {timer.avg():.1f} sec/epoch '\n",
    "          f'on {str(devices)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.5.7. Thí nghiệm\n",
    "\n",
    "Hãy xem hàm trên hoạt động như thế nào trên một GPU. Ta sử dụng kích thước batch 256 và tốc độ học 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(num_gpus=1, batch_size=256, lr=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giữ nguyên kích thước batch và tốc độ học, tăng số GPU lên 2, ta có thể thấy sự cải thiện về độ chính xác trên tập kiểm tra xấp xỉ bằng thí nghiệm trước. Dưới góc nhìn thuật toán tối ưu, hai thí nghiệm là giống hệt nhau. Không may, ta không đạt được sự tăng tốc đáng kể nào: đơn giản vì mô hình quá nhỏ; hơn nữa tập dữ liệu cũng nhỏ, do đó cách huấn luyện không quá tinh vi của chúng ta trên nhiều GPU sẽ chịu chi phí đáng kể do Python. Về sau ta sẽ gặp các mô hình phức tạp hơn và các cách song song hóa tinh vi hơn. Hiện giờ hãy xem thí nghiệm trên Fashion-MNIST cho kết quả như thế nào."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_gpus, batch_size, lr):\n",
    "    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "    devices = [d2l.try_gpu(i) for i in range(num_gpus)]\n",
    "    # Copy model parameters to num_gpus GPUs\n",
    "    device_params = [get_params(params, d) for d in devices]\n",
    "    # num_epochs, times, acces = 10, [], []\n",
    "    num_epochs = 10\n",
    "    animator = d2l.Animator('epoch', 'test acc', xlim=[1, num_epochs])\n",
    "    timer = d2l.Timer()\n",
    "    for epoch in range(num_epochs):\n",
    "        timer.start()\n",
    "        for X, y in train_iter:\n",
    "            # Perform multi-GPU training for a single minibatch\n",
    "            train_batch(X, y, device_params, devices, lr)\n",
    "            torch.cuda.synchronize()\n",
    "        timer.stop()\n",
    "        # Verify the model on GPU 0\n",
    "        animator.add(epoch + 1, (d2l.evaluate_accuracy_gpu(\n",
    "            lambda x: lenet(x, device_params[0]), test_iter, devices[0]),))\n",
    "    print(f'test acc: {animator.Y[0][-1]:.2f}, {timer.avg():.1f} sec/epoch '\n",
    "          f'on {str(devices)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.5.8. Tóm tắt\n",
    "\n",
    "   * Có nhiều cách để chia việc huấn luyện mạng học sâu cho nhiều GPU. Có thể chia các tầng cho một GPU, dùng nhiều GPU cho một tầng, hoặc nhiều GPU cho dữ liệu. Hai cách đầu yêu cầu điều khiển việc truyền dữ liệu chặt chẽ. Song song hóa dữ liệu là cách đơn giản nhất.\n",
    "   * Không khó để huấn luyện bằng song song hóa dữ liệu. Tuy nhiên, cách này cần tăng kích thước hiệu dụng của minibatch để đạt hiệu quả.\n",
    "   * Dữ liệu được chia cho nhiều GPU, mỗi GPU thực thi các lượt truyền xuôi và ngược, sau đó các gradient được tổng hợp lại và kết quả được truyền về các GPU.\n",
    "   * Minibatch lớn có thể yêu cầu tốc độ học cao hơn một chút."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.5.9. Bài tập\n",
    "\n",
    "1. Khi huấn luyện trên nhiều GPU, thử thay đổi kích thước minibatch từ $b$ thành $k⋅b$, tức là nhân thêm số lượng GPU.\n",
    "2. So sánh độ chính xác với các tốc độ học khác nhau. Tốc độ học thay đổi theo số lượng GPU như thế nào?\n",
    "3. Lập trình hàm allreduce hiệu quả hơn để tổng hợp các tham số trên các GPU khác nhau (tại sao cách ban đầu không hiệu quả)?\n",
    "4. Lập trình tính độ chính xác trên tập kiểm tra với nhiều GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.6. Cách lập trình Súc tích đa GPU\n",
    "\n",
    "Lập trình từ đầu việc song song hóa cho từng mô hình mới khá mất công. Hơn nữa, việc tối ưu các công cụ đồng bộ hóa sẽ cho hiệu suất cao. Sau đây chúng tôi sẽ giới thiệu cách thực hiện điều này bằng Gluon. Phần lý thuyết toán và các thuật toán giống trong Section 12.5. Như trước đây, ta bắt đầu bằng cách nhập các mô-đun cần thiết (tất nhiên là ta sẽ cần ít nhất hai GPU để chạy notebook này)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import torch as d2l\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.6.1. Ví dụ Đơn giản\n",
    "\n",
    "Hãy sử dụng một mạng có ý nghĩa hơn một chút so với LeNet ở phần trước mà vẫn có thể huấn luyện dễ dàng và nhanh chóng. Chúng tôi chọn một biến thể của ResNet-18 [He et al., 2016a]. Vì hình ảnh đầu vào rất nhỏ nên ta sửa đổi nó một chút. Cụ thể, điểm khác biệt so với ở Section 7.6 là ở phần đầu, ta sử dụng hạt nhân tích chập có kích thước, sải bước và đệm nhỏ hơn, và cũng loại bỏ đi tầng gộp cực đại."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def resnet18(num_classes, in_channels=1):\n",
    "    \"\"\"A slightly modified ResNet-18 model.\"\"\"\n",
    "    def resnet_block(in_channels, out_channels, num_residuals,\n",
    "                     first_block=False):\n",
    "        blk = []\n",
    "        for i in range(num_residuals):\n",
    "            if i == 0 and not first_block:\n",
    "                blk.append(d2l.Residual(in_channels, out_channels,\n",
    "                                        use_1x1conv=True, strides=2))\n",
    "            else:\n",
    "                blk.append(d2l.Residual(out_channels, out_channels))\n",
    "        return nn.Sequential(*blk)\n",
    "\n",
    "    # This model uses a smaller convolution kernel, stride, and padding and\n",
    "    # removes the maximum pooling layer\n",
    "    net = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, 64, kernel_size=3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU())\n",
    "    net.add_module(\"resnet_block1\", resnet_block(64, 64, 2, first_block=True))\n",
    "    net.add_module(\"resnet_block2\", resnet_block(64, 128, 2))\n",
    "    net.add_module(\"resnet_block3\", resnet_block(128, 256, 2))\n",
    "    net.add_module(\"resnet_block4\", resnet_block(256, 512, 2))\n",
    "    net.add_module(\"global_avg_pool\", nn.AdaptiveAvgPool2d((1,1)))\n",
    "    net.add_module(\"fc\", nn.Sequential(nn.Flatten(),\n",
    "                                       nn.Linear(512, num_classes)))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.6.2. Khởi tạo Tham số và Công việc phụ trợ\n",
    "\n",
    "Phương thức `initialize` cho phép ta thiết lập giá trị mặc định ban đầu cho các tham số trên thiết bị được chọn. Với độc giả mới, có thể tham khảo Section 4.8. Một điều rất thuận tiện là nó cũng cho phép ta khởi tạo mạng trên nhiều thiết bị cùng một lúc. Hãy thử xem cách nó hoạt động trong thực tế."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = resnet18(10)\n",
    "# get a list of GPUs\n",
    "devices = d2l.try_all_gpus()\n",
    "# we'll initialize the network inside the training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.6.3. Huấn luyện\n",
    "\n",
    "Như phần trên, đoạn mã huấn luyện cần thực hiện một số hàm cơ bản để quá trình song song hóa đạt hiệu quả:\n",
    "\n",
    "   * Các tham số của mạng cần được khởi tạo trên tất cả các thiết bị.\n",
    "   * Trong suốt quá trình lặp trên tập dữ liệu, các minibatch được chia nhỏ cho tất cả các thiết bị.\n",
    "   * Ta tính toán song song hàm mất mát và gradient của nó trên tất cả các thiết bị.\n",
    "   * Mất mát được tích luỹ (bởi phương thức huấn luyện trainer) và các tham số được cập nhật tương ứng.\n",
    "\n",
    "Cuối cùng ta tính toán (vẫn song song) độ chính xác và báo cáo giá trị cuối cùng của mạng. Quá trình huấn luyện ở đây khá giống với chương trước, trừ việc ta cần chia nhỏ và tổng hợp lại dữ liệu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, num_gpus, batch_size, lr):\n",
    "    train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "    devices = [d2l.try_gpu(i) for i in range(num_gpus)]\n",
    "    def init_weights(m):\n",
    "        if type(m) in [nn.Linear, nn.Conv2d]:\n",
    "            nn.init.normal_(m.weight, std=0.01)\n",
    "    net.apply(init_weights)\n",
    "    # Set model on multiple gpus\n",
    "    net = nn.DataParallel(net, device_ids=devices)\n",
    "    trainer = torch.optim.SGD(net.parameters(), lr)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    timer, num_epochs = d2l.Timer(), 10\n",
    "    animator = d2l.Animator('epoch', 'test acc', xlim=[1, num_epochs])\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "        timer.start()\n",
    "        for X, y in train_iter:\n",
    "            trainer.zero_grad()\n",
    "            X, y = X.to(devices[0]), y.to(devices[0])\n",
    "            l = loss(net(X), y)\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "        timer.stop()\n",
    "        animator.add(epoch + 1, (d2l.evaluate_accuracy_gpu(net, test_iter),))\n",
    "    print(f'test acc: {animator.Y[0][-1]:.2f}, {timer.avg():.1f} sec/epoch '\n",
    "          f'on {str(devices)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.6.4. Thử nghiệm\n",
    "\n",
    "Hãy cùng xem cách hoạt động trong thực tế. Để khởi động, ta huấn luyện mạng này trên một GPU đơn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(net, num_gpus=1, batch_size=256, lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiếp theo, ta sử dụng 2 GPU để huấn luyện. Mô hình ResNet-18 phức tạp hơn đáng kể so với LeNet. Đây chính là cơ hội để song song hóa chứng tỏ lợi thế của nó, vì thời gian dành cho việc tính toán lớn hơn đáng kể so với thời gian đồng bộ hóa các tham số. Điều này giúp cải thiện khả năng mở rộng do tổng chi phí song song hóa không quá đáng kể."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(num_gpus=2, batch_size=512, lr=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.6.5. Tóm tắt\n",
    "\n",
    "   * Gluon cung cấp các hàm để khởi tạo mô hình trên nhiều thiết bị bằng cách cung cấp một danh sách ngữ cảnh.\n",
    "   * Dữ liệu được tự động đánh giá trên các thiết bị mà nó được lưu trữ.\n",
    "   * Chú ý việc khởi tạo mạng trên mỗi thiết bị trước khi thử truy cập vào các tham số trên thiết bị đó. Nếu không khả năng cao sẽ có lỗi xảy ra.\n",
    "   * Các thuật toán tối ưu tự động tổng hợp kết quả trên nhiều GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.6.6. Bài tập\n",
    "\n",
    "   1. Phần này ta sử dụng ResNet-18. Hãy thử với số epoch, kích thước batch và tốc độ học khác. Thử sử dụng nhiều GPU hơn để tính toán. Chuyện gì sẽ xảy ra nếu ta chạy mô hình này trên máy chủ p2.16xlarge với 16 GPU?\n",
    "   2. Đôi khi mỗi thiết bị khác nhau cung cấp khả năng tính toán khác nhau. Ta có thể sử dụng GPU và CPU cùng lúc. Vậy ta nên phân chia công việc thế nào? Liệu việc phân chia có đáng hay không? Tại sao?\n",
    "   3. Chuyện gì sẽ xảy ra nếu ta bỏ hàm npx.waitall()? Bạn sẽ thay đổi quá trình huấn luyện thế nào để có thể xử lý song song tối đa 2 bước cùng lúc?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.7. Máy chủ Tham số\n",
    "\n",
    "Khi ta chuyển từ các GPU đơn sang đa GPU rồi sang nhiều máy chủ đa GPU, có khả năng các GPU được dàn trải qua nhiều khay chứa và bộ chuyển mạch mạng. Điều này khiến các giải thuật huấn luyện phân tán và song song trở nên phức tạp hơn nhiều. Các chi tiết nhỏ cũng trở nên quan trọng vì các phương thức kết nối khác nhau có băng thông rất khác nhau. Chẳng hạn, NVLink có băng thông lên tới 100GB/s qua 6 đường kết nối với cách thiết lập thích hợp, PCIe 3.0 16x làn có băng thông 16GB/s, trong khi ngay cả Ethernet 100GbE tốc độ cao chỉ đạt 10GB/s. Ngoài ra, khó có thể hy vọng rằng một nhà xây dựng mô hình thống kê cũng là một chuyên gia về kết nối mạng và hệ thống.\n",
    "\n",
    "Ý tưởng cốt lõi của máy chủ tham số được đề xuất từ [Smola & Narayanamurthy, 2010](https://d2l.aivivn.com/chapter_references/zreferences.html#smola-narayanamurthy-2010) trong ngữ cảnh các mô hình biến ẩn phân tán. Kế tiếp, một bản mô tả về ý nghĩa của tác vụ đẩy và kéo (push and pull) được giới thiệu trong [Ahmed et al., 2012](https://d2l.aivivn.com/chapter_references/zreferences.html#ahmed-aly-gonzalez-ea-2012) và một bản mô tả về hệ thống này cùng với thư viện mã nguồn mở được công bố trong [Li et al., 2014](https://d2l.aivivn.com/chapter_references/zreferences.html#li-andersen-park-ea-2014). Trong phần kế tiếp, ta sẽ tìm hiểu các thành phần cần thiết để đạt được hiệu suất cao."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.7.1. Huấn luyện Song song Dữ liệu\n",
    "\n",
    "Hãy cùng xem xét tổng quan phương pháp huấn luyện song song dữ liệu cho việc huấn luyện phân tán. Ta bắt đầu bằng cách này vì việc lập trình sẽ trở nên đơn giản hơn nhiều so với những cách khác. Vì các GPU ngày nay có khá nhiều bộ nhớ, gần như không có một trường hợp đặc biệt nào (ngoại trừ phương pháp học sâu trên đồ thị) mà một phương pháp song song hóa khác lại thích hợp hơn. Fig. 12.7.1 mô tả biến thể của việc song song hóa dữ liệu mà ta đã lập trình ở phần trước. Khía cạnh then chốt ở dạng này là việc tổng hợp gradient diễn ra trên GPU 0 trước khi các tham số cập nhật được phân phát tới tất cả GPU.\n",
    "\n",
    "![](images/ps.svg)\n",
    "\n",
    "<center>Fig. 12.7.1 Trái: việc huấn luyện trên một GPU; Phải: dạng biến thể của việc huấn luyện trên nhiều GPU. Quá trình diễn ra như sau: (1) Ta tính mất mát và gradient, (2) tất cả gradient được tổng hợp trên một GPU, (3) ta cập nhật tham số và các tham số đó được phân phối lại tới tất cả GPU.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nhìn lại, ta không có lý do gì đặc biệt khi quyết định tổng hợp gradient trên GPU 0. Dù sao thì ta cũng có thể tổng hợp gradient trên CPU. Và ta còn có thể tổng hợp một vài tham số trên một GPU và các tham số còn lại trên một GPU khác. Miễn là thuật toán tối ưu hỗ trợ điều này, ta không có lý do gì để không thể thực hiện. Ví dụ, giả sử ta có bốn vector tham số $\\mathbf{v}_1, \\ldots, \\mathbf{v}_4$ với các gradient tương ứng là $\\mathbf{g}_1, \\ldots, \\mathbf{g}_4$, ta có thể tổng hợp gradient của mỗi vector tham số trên một GPU\n",
    "\n",
    "<center>$\\mathbf{g}_{i} = \\sum_{j \\in \\mathrm{GPU}} \\mathbf{g}_{ij}$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cách lý luận này trông có vẻ rất tùy tiện và vô nghĩa. Sau cùng, phần toán xuyên suốt bên dưới vẫn không thay đổi. Nhưng ở đây chúng ta đang làm việc cùng các thiết bị phần cứng vật lý với các bus có băng thông khác nhau như đã thảo luận ở Section 12.4. Xét một máy chủ GPU 4-chiều được mô tả trong Fig. 12.7.2. Nếu nó được kết nối cực kỳ tốt, nó có thể sở hữu một card mạng với tốc độ 100 GbE. Những con số phổ biến hơn thường nằm trong khoảng 1-10 GbE với băng thông hiệu dụng từ 100MB/s đến 1GB/s. Vì các CPU thường có quá ít làn PCIe để kết nối trực tiếp với toàn bộ GPU (ví dụ, CPU thông dụng của Intel có 24 làn) ta cần một mạch đa hợp (multiplexer). Băng thông tới CPU qua cổng PCIe 16 làn thế hệ 3 là 16GB/s. Đây cũng là tốc độ mà mỗi GPU được kết nối với bộ chuyển mạch. Điều này có nghĩa là việc truyền tin trực tiếp giữa các GPU sẽ hiệu quả hơn.\n",
    "\n",
    "![](images/bw-hierarchy.svg)\n",
    "\n",
    "<center>Fig. 12.7.2 Một máy chủ GPU 4-chiều.</center>\n",
    "\n",
    "Để minh họa cho luận điểm trên, giả sử ta cần 160MB để lưu trữ các gradient. Trong trường hợp này, sẽ tốn 30ms để gửi các giá trị gradient này từ 3 thiết bị GPU đến chiếc GPU còn lại (mỗi đợt truyền tin tốn 10ms = 160MB / 16GB/s). Việc truyền lại các vector trọng số mất thêm 30ms nữa, tổng cộng tốn 60ms. Nếu ta gửi toàn bộ dữ liệu đến CPU sẽ phát sinh thêm 40ms vì mỗi GPU cần gửi dữ liệu đến CPU, và tính cả thời gian truyền lại các vector trọng số sẽ tốn 80ms. Cuối cùng, giả định rằng ta có thể chia nhỏ các giá trị gradient thành bốn phần, mỗi phần 40MB. Giờ ta có thể tổng hợp mỗi phần trên một GPU riêng biệt một cách đồng thời vì bộ chuyển mạch PCIe cho phép sử dụng toàn bộ băng thông cho mỗi kết nối. Thay vì 30ms như trước, quá trình này chỉ tốn 7.5ms và 15ms cho toàn bộ quá trình đồng bộ. Nói ngắn gọn, tùy thuộc vào cách các tham số được đồng bộ với nhau, quá trình này có thể chiếm từ 15ms đến 80ms. Fig. 12.7.3 minh họa sự khác biệt giữa các chiến lược trao đổi tham số khác nhau.\n",
    "\n",
    "![](images/ps-distributed.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.7.2. Đồng bộ dạng Vòng\n",
    "\n",
    "Khi nói tới đồng bộ hóa trên các phần cứng học sâu tiên tiến, ta thường gặp những cách kết nối mạng rất riêng. Ví dụ, máy P3.16xlarge trên AWS và NVIDIA DGX-2 cùng sử dụng cấu trúc kết nối trong Fig. 12.7.4. Mỗi GPU kết nối với một CPU chủ thông qua kết nối PCIe có tốc độ tối đa là 16 GB/s. Hơn nữa, mỗi GPU có 6 kết nối NVLink với khả năng truyền đến 300 Gbit/s theo cả hai hướng. Điều này có nghĩa là mỗi kết nối sẽ có tốc độ khoảng 18 GB/s theo mỗi hướng. Nói ngắn gọn, băng thông tổng hợp của NVLink lớn hơn đáng kể so với băng thông của PCIe. Câu hỏi đặt ra là làm sao để tận dụng triệt để điều đó.\n",
    "\n",
    "![](images/nvlink.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fig. 12.7.4 Kết nối NVLink trên các máy chủ 8 GPU V100 (hình ảnh được sự đồng ý từ NVIDIA).\n",
    "\n",
    "Hóa ra theo [Wang et al., 2018], chiến thuật đồng bộ tối ưu là phân tách mạng thành hai kết nối dạng vòng và sử dụng chúng để đồng bộ dữ liệu một cách trực tiếp. Fig. 12.7.5 minh họa việc mạng có thể được phân tách thành một kết nối dạng vòng (1-2-3-4-5-6-7-8-1) với băng thông NVLink gấp đôi và một kết nối dạng vòng khác (1-4-6-3-5-8-2-7-1) với băng thông bình thường. Việc thiết kế một giao thức đồng bộ hóa hiệu quả trong trường hợp này không hề đơn giản.\n",
    "\n",
    "![](images/nvlink-twoloop.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fig. 12.7.5 Phân tách mạng NVLink thành hai kết nối dạng vòng.\n",
    "\n",
    "Xét một thí nghiệm tưởng tượng như sau: cho một kết nối dạng vòng có n\n",
    "đơn vị tính toán (GPU) ta có thể truyền các giá trị gradient từ thiết bị thứ nhất đến thiết bị thứ hai. Ở đó nó sẽ được cộng thêm vào gradient cục bộ và rồi truyền tiếp đến thiết bị thứ ba, và tiếp tục như vậy với các thiết bị sau. Sau $n−1$ bước, gradient tổng hợp sẽ nằm ở thiết bị cuối cùng. Điều này có nghĩa là thời gian tổng hợp gradient sẽ tăng tuyến tính theo số lượng thiết bị trong mạng. Nhưng nếu ta làm vậy, thuật toán sẽ hoạt động kém hiệu quả. Dù sao, tại mọi thời điểm chỉ có một thiết bị thực hiện việc truyền tin. Chuyện gì sẽ xảy ra nếu ta chia các giá trị gradient thành $n$ khúc và bắt đầu đồng bộ khúc thứ $i$ tại thiết bị $i$? Vì mỗi khúc có kích thước $1/n$, tổng thời gian giờ sẽ là $(n−1)/n≈1$. Nói cách khác, thời gian tổng hợp gradient không tăng khi ta tăng số thiết bị trong mạng. Quả là một kết quả đáng kinh ngạc. Fig. 12.7.6 minh họa chuỗi các bước với số thiết bị $n=4$.\n",
    "\n",
    "![](images/ringsync.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fig. 12.7.6 Đồng bộ vòng trên 4 nút. Mỗi nút truyền một phần gradient sang nút liền kề bên trái cho đến khi gradient đầy đủ có thể được tìm thấy tại nút liền kề bên phải nó.\n",
    "\n",
    "Nếu vẫn sử dụng ví dụ đồng bộ 160 MB trên 8 GPU V100, ta có thể đạt xấp xỉ $2⋅160MB/(3⋅18GB/s)≈6ms$\n",
    ". Kết quả này tốt hơn so với việc sử dụng bus PCIe một chút, mặc dù lúc này ta sử dụng đến 8 GPU. Chú ý rằng trong thực tế những con số này sẽ không được tốt như vậy, do các framework học sâu thường gặp khó khăn trong việc tổng hợp thông tin thành cụm lớn hơn để truyền đi. Hơn nữa, việc định thời là cực kì quan trọng. Lưu ý, mọi người thường hiểu nhầm rằng đồng bộ vòng có bản chất khác hẳn so với các thuật toán đồng bộ khác. Thực ra điểm khác biệt duy nhất nằm ở đường đi đồng bộ có phần tinh vi hơn so với phương pháp cây đơn giản."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.7.3. Huấn luyện trên Nhiều Máy tính\n",
    "\n",
    "Việc huấn luyện phân tán trên nhiều máy tính tạo nên một thử thách mới: ta cần phải giao tiếp với các máy chủ chỉ được liên kết với nhau qua loại cáp có băng thông tương đối thấp. Trong một số trường hợp tốc độ thậm chí có thể chậm gấp hơn 10 lần. Đồng bộ nhiều thiết bị là công việc khá phức tạp. Suy cho cùng, mỗi máy tính khác nhau chạy đoạn mã huấn luyện với tốc độ khác nhau đôi chút. Do đó ta cần đồng bộ chúng nếu muốn sử dụng tối ưu phân tán đồng bộ. Fig. 12.7.7 mô tả quá trình huấn luyện phân tán song song.\n",
    "\n",
    "   1. Một batch dữ liệu (khác nhau) được đọc trên mỗi máy tính, chia đều cho các GPU và truyền đến bộ nhớ của GPU. Ở đó các dự đoán và gradient được tính toán riêng biệt theo từng batch trên các GPU khác nhau.\n",
    "   2. Các gradient trên tất cả các GPU cục bộ được tổng hợp trên một GPU (hoặc các phần khác nhau được tổng hợp trên nhiều GPU khác nhau).\n",
    "   3. Các gradient được truyền đến CPU.\n",
    "   4. CPU truyền các gradient đến máy chủ tham số trung tâm để tổng hợp tất cả các gradient.\n",
    "   5. Các gradient tổng sau đó được sử dụng để cập nhật các vector trọng số. Tiếp đó thì các vector trọng số mới được phân phát cho các CPU.\n",
    "   6. Thông tin cập nhật được truyền tới một (hoặc nhiều) GPU.\n",
    "   7. Các vector trọng số đã được cập nhật sau đó được phân bố đều cho tất cả các GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/ps-multimachine.svg)\n",
    "\n",
    "Fig. 12.7.7 Huấn luyện song song phân tán trên nhiều máy tính đa GPU\n",
    "\n",
    "Các thao tác trên nhìn qua thì có vẻ khá dễ hiểu. Quả thật, chúng có thể được thực hiện một cách hiệu quả trong một máy tính. Tuy nhiên khi xét trên nhiều máy tính, ta có thể thấy rằng chính máy chủ tham số trung tâm đã trở thành nút nghẽn cổ chai. Suy cho cùng, băng thông của mỗi máy chủ là có hạn, do đó đối với $m$\n",
    "máy thợ, thời gian để truyền toàn bộ gradient đến máy chủ là $O(m)$. Ta có thể vượt qua rào cản này bằng cách tăng số lượng máy chủ lên n. Khi đó mỗi máy chủ chỉ cần lưu trữ $O(1/n)$ tham số, do đó tổng thời gian cần để cập nhật và tối ưu trở thành $O(m/n)$. Tổng thời gian này sẽ tăng lên theo hằng số bất kể số lượng máy thợ ta sử dụng là bao nhiêu. Trong thực tế, các máy tính sẽ vừa là máy chủ và máy thợ. Fig. 12.7.8 minh hoạ thiết kế này. Độc giả có thể tham khảo [Li et al., 2014] để biết thêm chi tiết. Đặc biệt, việc đảm bảo các máy tính hoạt động với độ trễ không quá lớn không phải là một chuyện dễ dàng. Chúng tôi sẽ bỏ qua chi tiết về các rào cản và chỉ đề cập ngắn gọn tới việc cập nhật đồng bộ và bất đồng bộ dưới đây."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/ps-multips.svg)\n",
    "\n",
    "Fig. 12.7.8 Trên - một máy chủ tham số là một nút nghẽn cổ chai do băng thông của nó có hạn. Dưới - nhiều máy chủ tham số lưu trữ từng phần các tham số với băng thông tổng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.7.4. Lưu trữ (Khóa, Giá trị)\n",
    "\n",
    "Lập trình các bước cần thiết trên cho việc huấn luyện phân tán trên nhiều GPU trong thực tế không hề đơn giản. Cụ thể, có khả năng ta sẽ gặp rất nhiều lựa chọn khác nhau. Do đó, rất đáng để sử dụng một phép trừu tượng hóa khá phổ biến là lưu trữ cặp (khóa, giá trị) với cách cập nhật được định nghĩa lại. Trên nhiều máy chủ và nhiều GPU, việc tính toán gradient có thể được định nghĩa như sau\n",
    "\n",
    "<center>$\\mathbf{g}_{i} = \\sum_{k \\in \\mathrm{máy~thợ}} \\sum_{j \\in \\mathrm{GPU}} \\mathbf{g}_{ijk}.$</center>\n",
    "\n",
    "Đặc điểm chính của thao tác này nằm ở việc nó là một phép rút gọn có tính giao hoán, tức nó gộp nhiều vector thành một vector và thứ tự áp dụng thao tác này không quan trọng. Vì không cần (phải) kiểm soát chi tiết thời điểm gradient được nhận, thao tác này rất phù hợp với mục đích của chúng ta. Lưu ý rằng ta có thể thực hiện phép rút gọn theo từng bước. Thêm nữa, chú ý rằng thao tác này độc lập giữa các khối i\n",
    "\n",
    "gắn liền với các tham số (và các gradient) khác nhau.\n",
    "\n",
    "Điều này cho phép ta định nghĩa hai thao tác sau: đẩy, để cộng dồn gradient; và kéo, để lấy lại gradient được cộng dồn. Vì ta có nhiều tập gradient (do có nhiều tầng), ta cần gán chỉ số cho gradient bằng khóa i\n",
    "\n",
    ". Sự giống nhau giữa phương pháp này và việc lưu trữ (khóa, giá trị) như phương pháp được giới thiệu trong Dynamo [DeCandia et al., 2007] không phải là ngẫu nhiên. Chúng thỏa mãn rất nhiều tính chất, cụ thể là khi phân phối các tham số cho nhiều máy chủ.\n",
    "\n",
    "   * **đẩy (khóa, giá trị)** gửi một gradient cụ thể (giá trị) từ máy thợ đến thiết bị lưu trữ chung. Tại đây các tham số được tổng hợp lại, ví dụ bằng cách lấy tổng.\n",
    "   * **kéo (khóa, giá trị)** lấy lại tham số đã được tổng hợp từ thiết bị lưu trữ chung, sau khi đã kết hợp gradient từ tất cả máy thợ.\n",
    "\n",
    "Bằng cách ẩn đi sự phức tạp của việc đồng bộ sau các thao tác đơn giản là đẩy và kéo, ta có thể tách những mối bận tâm theo hai hướng: của các nhà mô hình thống kê, những người muốn biểu diễn việc tối ưu một cách đơn giản và các kỹ sư hệ thống, những người cần giải quyết sự phức tạp sẵn có trong việc đồng bộ hóa phân tán. Trong phần tiếp theo ta sẽ thử nghiệm việc lưu trữ (khóa, giá trị) trong thực tế."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.7.5. Tóm tắt\n",
    "\n",
    "   * Việc đồng bộ cần có độ thích ứng cao với hạ tầng mạng cụ thể và kết nối trong máy chủ. Điều này có thể tạo ra khác biệt đáng kể trong thời gian đồng bộ.\n",
    "   * Đồng bộ dạng vòng có thể là phương án tối ưu với các máy chủ P3 và DGX-2, còn với các loại máy chủ khác thì không hẳn.\n",
    "   * Chiến lược đồng bộ phân cấp rất tốt khi thêm nhiều máy chủ tham số để tăng băng thông.\n",
    "   * Giao tiếp bất đồng bộ (khi việc tính toán vẫn đang diễn ra) có thể cải thiện hiệu năng.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.7.6. Bài tập\n",
    "\n",
    "   1. Bạn có thể cải thiện đồng bộ dạng vòng hơn nữa không? Gợi ý: bạn có thể gửi thông tin theo cả hai chiều.\n",
    "   2. Đồng bộ bất đối xứng hoàn toàn có độ trễ nào không?\n",
    "   3. Nên để khả năng chịu lỗi (*fault tolerance*) như thế nào? Nếu một máy chủ gặp trục trặc thì sao? Đây có phải vấn đề nghiêm trọng không?\n",
    "   4. Lưu checkpoint như thế nào?\n",
    "   5. Bạn có thể tăng tốc việc tổng hợp dạng cây (*tree aggregation*) không?\n",
    "   6. Tìm hiểu các cách rút gọn khác (như dạng bán vòng giao hoán - *commutative semiring*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
>>>>>>> 9227b05296edb18ff21dd6be103b824b1a2ec9ca
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
