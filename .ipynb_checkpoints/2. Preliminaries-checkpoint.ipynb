{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dive into deep learning\n",
    "# 2. Sơ bộ\n",
    "Để bắt đầu với học sâu, ta sẽ cần nắm bắt một vài kỹ năng cơ bản. Tất cả những vấn đề về học máy đều có liên quan đến việc trích xuất thông tin từ dữ liệu. Vì vậy, chúng tôi sẽ bắt đầu bằng cách học các kỹ năng thực tế để lưu trữ, thao tác và xử lý dữ liệu.\n",
    "\n",
    "Hơn nữa, học máy thường yêu cầu làm việc với các tập dữ liệu lớn, mà chúng ta có thể coi như ở dạng bảng, trong đó các hàng tương ứng với các mẫu (*examples *) và các cột tương ứng với các thuộc tính (*attributes*). Đại số tuyến tính cung cấp cho ta một tập kỹ thuật mạnh mẽ để làm việc với dữ liệu dạng bảng. Chúng ta sẽ không đi quá sâu mà chỉ tập trung cơ bản vào các toán tử ma trận cơ bản và cách thực thi chúng.\n",
    "\n",
    "Bên cạnh đó, học sâu luôn liên quan đến tối ưu hoá (*optimization*). Chúng ta có một mô hình với bộ tham số và muốn tìm ra các tham số khớp với dữ liệu nhất. Việc xác định cách điều chỉnh từng tham số ở mỗi bước trong thuật toán đòi hỏi một chút kiến thức về giải tích, mà sẽ được giới thiệu ngắn gọn dưới đây. May thay, gói **autograd** sẽ tự động tính đạo hàm cho chúng ta, và sẽ được đề cập ngay sau đó.\n",
    "\n",
    "Kế tiếp, học máy liên quan đến việc đưa ra những dự đoán như: Xác định giá trị của một số thuộc tính chưa biết dựa trên thông tin quan sát được? Để suy luận chặt chẽ dưới sự bất định, chúng ta sẽ cần tìm đến ngôn ngữ của xác suất.\n",
    "\n",
    "Cuốn sách này đã cung cấp nội dung toán học ở mức tối thiểu cần có để có được sự hiểu biết đúng đắn về học sâu. Tuy nhiên, điều đó không đồng nghĩa rằng cuốn sách này không cần các kiến thức toán học. Do vậy, chương này sẽ giới thiệu nhanh về các kiến thức toán học cơ bản và thông dụng, cho phép tất cả mọi người tối thiểu là sẽ hiểu được hầu hết nội dung toán trong quyển sách này. Nếu bạn muốn hiểu tất cả nội dung toán học, hãy tham khảo thêm **chap_appendix_math**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Thao tác với Dữ liệu\n",
    "Để hoàn thành công việc, chúng ta cần một số cách để lưu trữ và thao tác dữ liệu. Nói chung, có hai điều quan trọng chúng ta cần làm với dữ liệu: (i) thu thập chúng; và (ii) xử lý chúng khi chúng ở bên trong máy tính. Không có ích gì khi thu thập dữ liệu mà không có cách nào đó để lưu trữ, vì vậy trước tiên chúng ta hãy chơi với dữ liệu tổng hợp. Để bắt đầu, chúng tôi giới thiệu mảng n chiều, còn được gọi là **tensor**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nếu bạn đã làm việc với NumPy, gói tính toán khoa học được sử dụng rộng rãi nhất trong Python, thì bạn sẽ thấy phần này quen thuộc. Lớp tensor (`ndarray` trong MXNet, `tensor` trong cả PyTorch và TensorFlow) tương tự như NumPy’s `ndarray` với một vài tính năng đặt biệt. Đầu tiên, GPU được hỗ trợ tốt để tăng tốc tính toán trong khi NumPy chỉ hỗ trợ tính toán CPU. Thứ hai, lớp tensor hỗ trợ tính vi phân tự động. Những đặc tính này làm cho lớp tensor thích hợp cho việc học sâu. Trong suốt cuốn sách, khi chúng ta nói đến tensor, chúng ta đang đề cập đến các trường hợp của lớp tensor trừ khi được nêu khác."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1. Bắt đầu\n",
    "Trong mục này, mục tiêu của chúng tôi là trang bị cho bạn các kiến thức toán cơ bản và cài đặt các công cụ tính toán mà bạn sẽ xây dựng dựa trên nó xuyên suốt cuốn sách này. Đừng lo nếu bạn gặp khó khăn với các khái niệm toán khó hiểu hoặc các hàm trong thư viện tính toán. Các mục tiếp theo sẽ nhắc lại những khái niệm này trong từng ngữ cảnh kèm theo ví dụ thực tiễn. Mặt khác, nếu bạn đã có kiến thức nền tảng và muốn đi sâu hơn vào các nội dung toán, bạn có thể bỏ qua mục này."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.arange(12)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để bắt đầu, chúng ta có thể sử dụng `arange` để tạo một vector hàng x chứa 12 số nguyên đầu tiên bắt đầu bằng 0, mặc dù chúng được tạo dưới dạng `float` theo mặc định. Mỗi giá trị trong *tensor* được gọi là một phần tử của *tensor*. Ví dụ, có 12 phần tử trong *tensor* `x`. Trừ khi được chỉ định khác, một *tensor* mới sẽ được lưu trữ trong bộ nhớ chính và được chỉ định để tính toán trên CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng ta có thể lấy kích thước (độ dài theo mỗi trục) của tensor bằng thuộc tính `shape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nếu chỉ muốn biết tổng số phần tử của một tensor, nghĩa là tích của tất cả các thành phần trong shape, ta có thể sử dụng thuộc tính size. Vì ta đang làm việc với một vector, cả shape và size của nó đều chứa cùng một phần tử duy nhất."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để thay đổi kích thước của một tensor mà không làm thay đổi số lượng phần tử cũng như giá trị của chúng, ta có thể gọi hàm `reshape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = x.reshape(3, 4)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Việc chỉ định cụ thể mọi chiều khi thay đổi kích thước là không cần thiết. Nếu kích thước mong muốn là một ma trận với kích thước (chiều_cao, chiều_rộng), thì sau khi biết chiều_rộng, chiều_cao có thể được ngầm suy ra. Tại sao ta lại cần phải tự làm phép tính chia? Trong ví dụ trên, để có được một ma trận với  3  hàng, chúng ta phải chỉ định rõ rằng nó có  3  hàng và  4  cột. May mắn thay, ndarray có thể tự động tính một chiều từ các chiều còn lại. Ta có thể dùng chức năng này bằng cách đặt -1 cho chiều mà ta muốn ndarray tự suy ra. Trong trường hợp vừa rồi, thay vì gọi `x.reshape(3, 4)`, ta có thể gọi `x.reshape(-1, 4)` hoặc `x.reshape(3, -1)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thông thường, chúng tôi sẽ muốn các ma trận của mình được khởi tạo bằng số không, số một, một số hằng số khác hoặc số được lấy mẫu ngẫu nhiên từ một phân phối cụ thể. Chúng ta có thể tạo một tensor đại diện cho tensor với tất cả các phần tử được đặt thành 0 và hình dạng của (2, 3, 4) như sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(2, 3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tương tự, chúng ta có thể tạo các tensor với mỗi phần tử được đặt thành 1 như sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(2, 3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đoạn mã dưới đây tạo một ndarray có kích thước ( 3 ,  4 ) với các phần tử được lấy mẫu ngẫu nhiên từ một phân phối **Gauss** (phân phối chuẩn) với trung bình bằng  0  và độ lệch chuẩn  1 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7822, -1.9014, -0.3191,  2.8997],\n",
       "        [-1.4859, -0.4350, -0.4821, -1.0994],\n",
       "        [-0.3959,  0.6754, -0.7233,  1.8441]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta cũng có thể khởi tạo giá trị cụ thể cho mỗi phần tử trong tenor mong muốn bằng cách đưa vào một mảng Python (hoặc mảng của mảng) chứa các giá trị số. Ở đây, mảng ngoài cùng tương ứng với trục  0 , và mảng bên trong tương ứng với trục 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 1, 4, 3],\n",
       "        [1, 2, 3, 4],\n",
       "        [4, 3, 2, 1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. Phép toán\n",
    "Các phép toán tiêu chuẩn (+, -, * *, /, và * ** ) là các phép toán theo từng phần tử trên các tensor đồng kích thước bất kỳ. Ta có thể gọi những phép toán theo từng phần tử lên hai tensor đồng kích thước. Trong ví dụ dưới đây, các dấu phẩy được sử dụng để tạo một tuple  5  phần tử với mỗi phần tử là kết quả của một phép toán theo từng phần tử."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3.,  4.,  6., 10.]),\n",
       " tensor([-1.,  0.,  2.,  6.]),\n",
       " tensor([ 2.,  4.,  8., 16.]),\n",
       " tensor([0.5000, 1.0000, 2.0000, 4.0000]),\n",
       " tensor([ 1.,  4., 16., 64.]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2, 4, 8])\n",
    "y = torch.tensor([2, 2, 2, 2])\n",
    "x + y, x - y, x * y, x / y, x ** y  # The ** operator is exponentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rất nhiều các phép toán khác có thể được áp dụng theo từng phần tử, bao gồm các phép toán đơn ngôi như hàm mũ cơ số  e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta cũng có thể nối (*concatenate*) nhiều tensor với nhau, xếp chồng chúng lên nhau để tạo ra một tensor lớn hơn. Ta chỉ cần cung cấp một danh sách các tensor và khai báo chúng được nối theo trục nào. Ví dụ dưới đây thể hiện cách nối hai ma trận theo hàng (trục  0 , phần tử đầu tiên của kích thước) và theo cột (trục  1 , phần tử thứ hai của kích thước). Ta có thể thấy rằng, cách thứ nhất tạo một tensor với độ dài trục - $0(6)$ bằng tổng các độ dài trục  0  của hai tensor đầu vào ( 3+3 ); trong khi cách thứ hai tạo một tensor với độ dài trục - $1(8)$ bằng tổng các độ dài trục  1  của hai tensor đầu vào ( 4+4 )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [ 2.,  1.,  4.,  3.],\n",
       "         [ 1.,  2.,  3.,  4.],\n",
       "         [ 4.,  3.,  2.,  1.]]),\n",
       " tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
       "         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(12, dtype=torch.float32).reshape((3,4))\n",
    "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đôi khi, ta muốn tạo một tensor nhị phân thông qua các mệnh đề logic. Lấy `x == y` làm ví dụ. Với mỗi vị trí, nếu giá trị củax và y tại vị trí đó bằng nhau thì phần tử tương ứng trong tensor mới lấy giá trị  1 , nghĩa là mệnh đề logic `x == y` là đúng tại vị trí đó; ngược lại vị trí đó lấy giá trị 0 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True, False,  True],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X == Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lấy tổng mọi phần tử trong một tensor tạo ra một tensor với chỉ một phần tử."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(66.)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3. Cơ chế Lan truyền (Broadcasting Mechanism)\n",
    "Trong mục trên, ta đã thấy cách thực hiện các phép toán theo từng phần tử với hai tensor đồng kích thước. Trong những điều kiện nhất định, thậm chí khi kích thước khác nhau, ta vẫn có thể thực hiện các phép toán theo từng phần tử bằng cách sử dụng cơ chế lan truyền (broadcasting mechanism). Cơ chế này hoạt động như sau: Thứ nhất, mở rộng một hoặc cả hai mảng bằng cách lặp lại các phần tử một cách hợp lý sao cho sau phép biến đổi này, hai tensor có cùng kích thước. Thứ hai, thực hiện các phép toán theo từng phần tử với hai mảng mới này.\n",
    "\n",
    "Trong hầu hết các trường hợp, chúng ta lan truyền một mảng theo trục có độ dài ban đầu là  1 , như ví dụ dưới đây:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0],\n",
       "         [1],\n",
       "         [2]]),\n",
       " tensor([[0, 1]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(3).reshape((3, 1))\n",
    "b = torch.arange(2).reshape((1, 2))\n",
    "a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vì a và b là các ma trận có kích thước lần lượt là  3×1  và  1×2 , kích thước của chúng không khớp nếu ta muốn thực hiện phép cộng. Ta lan truyền các phần tử của cả hai ma trận thành các ma trận  3×2  như sau: lặp lại các cột trong ma trận a và các hàng trong ma trận b trước khi cộng chúng theo từng phần tử."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [1, 2],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4. Chỉ số và Cắt chọn mảng\n",
    "Cũng giống như trong bất kỳ mảng Python khác, các phần tử trong một tensor có thể được truy cập theo chỉ số. Tương tự, phần tử đầu tiên có chỉ số  0  và khoảng được cắt chọn bao gồm phần tử đầu tiên nhưng không tính phần tử cuối cùng. Và trong các danh sách Python tiêu chuẩn, chúng ta có thể truy cập các phần tử theo vị trí đếm ngược từ cuối danh sách bằng cách sử dụng các chỉ số âm.\n",
    "\n",
    "Vì vậy, [-1] chọn phần tử cuối cùng và [1:3] chọn phần tử thứ hai và phần tử thứ ba như sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 8.,  9., 10., 11.]),\n",
       " tensor([[ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[-1], X[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ngoài việc đọc, chúng ta cũng có thể viết các phần tử của ma trận bằng cách chỉ định các chỉ số."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  9.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1, 2] = 9\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nếu chúng ta muốn gán cùng một giá trị cho nhiều phần tử, chúng ta chỉ cần trỏ đến tất cả các phần tử đó và gán giá trị cho chúng. Chẳng hạn, [0:2 ,:] truy cập vào hàng thứ nhất và thứ hai, trong đó : lấy tất cả các phần tử dọc theo trục  1  (cột). Ở đây chúng ta đã thảo luận về cách truy cập vào ma trận, nhưng tất nhiên phương thức này cũng áp dụng cho các vector và tensor với nhiều hơn  2  chiều."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12., 12., 12., 12.],\n",
       "        [12., 12., 12., 12.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:2, :] = 12\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.5. Tiết kiệm Bộ nhớ\n",
    "Ở ví dụ trước, mỗi khi chạy một phép tính, chúng ta sẽ cấp phát bộ nhớ mới để lưu trữ kết quả của lượt chạy đó. Cụ thể hơn, nếu viết y = x + y, ta sẽ ngừng tham chiếu đến tensor mà y đã chỉ đến trước đó và thay vào đó gán y vào bộ nhớ được cấp phát mới. Trong ví dụ tiếp theo, chúng ta sẽ minh họa việc này với hàm id() của Python - hàm cung cấp địa chỉ chính xác của một đối tượng được tham chiếu trong bộ nhớ. Sau khi chạy y = y + x, chúng ta nhận ra rằng id(y) chỉ đến một địa chỉ khác. Đó là bởi vì Python trước hết sẽ tính y + x, cấp phát bộ nhớ mới cho kết quả trả về và gán y vào địa chỉ mới này trong bộ nhớ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(Y)\n",
    "Y = Y + X\n",
    "id(Y) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đây có thể là điều không mong muốn vì hai lý do. Thứ nhất, không phải lúc nào chúng ta cũng muốn cấp phát bộ nhớ không cần thiết. Trong học máy, ta có thể có đến hàng trăm megabytes tham số và cập nhật tất cả chúng nhiều lần mỗi giây, và thường thì ta muốn thực thi các cập nhật này tại chỗ. Thứ hai, chúng ta có thể trỏ đến cùng tham số từ nhiều biến khác nhau. Nếu không cập nhật tại chỗ, các bộ nhớ đã bị loại bỏ sẽ không được giải phóng, dẫn đến khả năng một số chỗ trong mã nguồn sẽ vô tình tham chiếu lại các tham số cũ.\n",
    "\n",
    "May mắn thay, việc thực hiện các thao tác tại chỗ rất dễ dàng.  Chúng ta có thể gán kết quả của một phép tính cho một mảng đã được cấp phát trước đó bằng ký hiệu cắt chọn (slice notation), ví dụ, y[:] = <expression>. Để minh họa khái niệm này, đầu tiên chúng ta tạo một ma trận mới z có cùng kích thước với ma trận y, sử dụng zeros_like để gán giá trị khởi tạo bằng  0 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id(Z): 1488182564672\n",
      "id(Z): 1488182564672\n"
     ]
    }
   ],
   "source": [
    "Z = torch.zeros_like(Y)\n",
    "print('id(Z):', id(Z))\n",
    "Z[:] = X + Y\n",
    "print('id(Z):', id(Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(X)\n",
    "X += Y\n",
    "id(X) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.6. Chuyển đổi sang các Đối Tượng Python Khác\n",
    "Dễ dàng chuyển đổi sang tensor NumPy hoặc ngược lại. Kết quả được chuyển đổi không chia sẻ bộ nhớ. Sự bất tiện nhỏ này thực sự khá quan trọng: khi bạn thực hiện các hoạt động trên CPU hoặc trên GPU, bạn không muốn tạm dừng tính toán, chờ xem liệu gói NumPy của Python có thể muốn làm việc gì khác với cùng một phần bộ nhớ hay không."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, torch.Tensor)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = X.numpy()\n",
    "B = torch.tensor(A)\n",
    "type(A), type(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để chuyển đổi một tensor kích thước-1 thành một đại lượng vô hướng Python, chúng ta có thể gọi hàm item hoặc các hàm tích hợp của Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3.5000]), 3.5, 3.5, 3)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([3.5])\n",
    "a, a.item(), float(a), int(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.7. Tổng kết\n",
    "Giao diện chính để lưu trữ và thao tác dữ liệu cho học sâu là tensor (mảng n -dimensional). Nó cung cấp nhiều chức năng bao gồm các phép toán cơ bản, phát sóng, lập chỉ mục, cắt, tiết kiệm bộ nhớ và chuyển đổi sang các đối tượng Python khác."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Chạy đoạn mã nguồn trong mục này. Thay đổi điều kiện mệnh đề x == y sang x < y hoặc x > y, sau đó kiểm tra dạng của tensor nhận được.\n",
    "2. Thay hai tensor trong phép tính theo từng phần tử ở phần cơ chế lan truyền (broadcasting mechanism) với các tensor có kích thước khác, ví dụ như tensor ba chiều. Kết quả có giống như bạn mong đợi hay không?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Tiền xử lý dữ liệu\n",
    "Cho đến giờ chúng tôi đã đề cập tới rất nhiều kỹ thuật thao tác dữ liệu được lưu trong dạng tensor. Nhưng để áp dụng học sâu vào giải quyết các vấn đề thực tế, ta thường phải bắt đầu bằng việc xử lý dữ liệu thô, chứ không có luôn dữ liệu ngăn nắp được chuẩn bị sẵn trong định dạng tensor. Trong số các công cụ phân tích dữ liệu phổ biến của Python, gói *pandas* khá được ưa chuộng. Cũng như nhiều gói khác trong hệ sinh thái rộng lớn của Python, ‘*pandas*’ có thể được sử dụng kết hợp với định dạng tensor. Vì vậy, chúng ta sẽ đi nhanh qua các bước để tiền xử lý dữ liệu thô bằng pandas rồi đổi chúng sang dạng tensor. Nhiều kỹ thuật tiền xử lý dữ liệu khác sẽ được giới thiệu trong các chương sau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1. Đọc tập dữ liệu\n",
    "Để lấy ví dụ, ta bắt đầu bằng việc tạo một tập dữ liệu nhân tạo lưu trong file csv `../data/house_tiny.csv` (csv - comma-separated values - giá trị tách nhau bằng dấu phẩy). Dữ liệu lưu ở các định dạng khác cũng có thể được xử lý tương tự. Hàm `mkdir_if_not_exist` dưới đây để đảm bảo rằng thư mục `../data` tồn tại. Chú thích # Saved in the d2l package for later use (Lưu lại trong gói d2l để dùng sau) là kí hiệu đánh dấu các hàm, lớp hoặc các lệnh `import` được lưu trong gói d2l, để sau này ta có thể trực tiếp gọi hàm `d2l.mkdir_if_not_exist()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def mkdir_if_not_exist(path):  #@save\n",
    "    \"\"\"Make a directory if it does not exist.\"\"\"\n",
    "    if not isinstance(path, str):\n",
    "        path = os.path.join(*path)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau đây ta ghi tệp dữ liệu vào file csv theo từng hàng một."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = '../data/house_tiny.csv'\n",
    "mkdir_if_not_exist('../data')\n",
    "with open(data_file, 'w') as f:\n",
    "    f.write('NumRooms,Alley,Price\\n')  # Column names\n",
    "    f.write('NA,Pave,127500\\n')  # Each row represents a data example\n",
    "    f.write('2,NA,106000\\n')\n",
    "    f.write('4,NA,178100\\n')\n",
    "    f.write('NA,NA,140000\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để gọi tập dữ liệu thô từ tệp csv vừa được tạo ra, ta dùng gói thư viện *pandas* và gọi hàm `read_csv`. Bộ dữ liệu này có  4  hàng và  3  cột, trong đó mỗi hàng biểu thị số phòng (“NumRooms”), kiểu lối đi (“Alley”), và giá (“Price”) của căn nhà."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms Alley   Price\n",
      "0       NaN  Pave  127500\n",
      "1       2.0   NaN  106000\n",
      "2       4.0   NaN  178100\n",
      "3       NaN   NaN  140000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(data_file)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. Xử lý dữ liệu thiếu\n",
    "Để ý rằng giá trị “`NaN`” là các giá trị bị thiếu. Để xử lý dữ liệu thiếu, các cách thường được áp dụng là quy buộc (*imputation*) và xoá bỏ (*deletion*), trong đó quy buộc thay thế giá trị bị thiếu bằng giá trị khác, trong khi xoá bỏ sẽ bỏ qua các giá trị bị thiếu. Dưới đây chúng ta xem xét phương pháp quy buộc.\n",
    "\n",
    "Bằng phương pháp đánh chỉ số theo số nguyên (`iloc`), chúng ta tách data thành `inputs` (tương ứng với hai cột đầu) và `outputs` (tương ứng với cột cuối cùng). Với các giá trị số bị thiếu trong `inputs`, ta thay thế phần tử “`NaN`” bằng giá trị trung bình cộng của cùng cột đó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms Alley\n",
      "0       3.0  Pave\n",
      "1       2.0   NaN\n",
      "2       4.0   NaN\n",
      "3       3.0   NaN\n"
     ]
    }
   ],
   "source": [
    "inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]\n",
    "inputs = inputs.fillna(inputs.mean())\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Với các giá trị dạng hạng mục hoặc số rời rạc trong inputs, ta coi “`NaN`” là một mục riêng. Vì cột “`Alley`” chỉ nhận 2 giá trị riêng lẻ là “`Pave`” (được lát gạch) và “`NaN`”, pandas có thể tự động chuyển cột này thành 2 cột “`Alley_Pave`” và “`Alley_nan`”. Những hàng có kiểu lối đi là “`Pave`” sẽ có giá trị của cột “`Alley_Pave`” và cột “`Alley_nan`” tương ứng là  1  và  0 . Hàng mà không có giá trị cho kiểu lối đi sẽ có giá trị cột “`Alley_Pave`” và cột “`Alley_nan`” lần lượt là  0  và  1 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms  Alley_Pave  Alley_nan\n",
      "0       3.0           1          0\n",
      "1       2.0           0          1\n",
      "2       4.0           0          1\n",
      "3       3.0           0          1\n"
     ]
    }
   ],
   "source": [
    "inputs = pd.get_dummies(inputs, dummy_na=True)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3. Chuyển sang định dạng tensor\n",
    "Giờ thì toàn bộ các giá trị trong inputs và outputs đã ở dạng số, chúng đã có thể được chuyển sang định dạng tensor. Khi đã ở định dạng này, chúng có thể được biến đổi và xử lý với những chức năng của ndarray đã được giới thiệu ở Section 2.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[3., 1., 0.],\n",
       "         [2., 0., 1.],\n",
       "         [4., 0., 1.],\n",
       "         [3., 0., 1.]], dtype=torch.float64),\n",
       " tensor([127500, 106000, 178100, 140000]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "X, y = torch.tensor(inputs.values), torch.tensor(outputs.values)\n",
    "X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4. Tóm tắt\n",
    "1. Cũng như nhiều gói mở rộng trong hệ sinh thái khổng lồ của Python, pandas có thể làm việc được với tensor.\n",
    "2. Phương pháp quy buộc hoặc xoá bỏ có thể dùng để xử lý dữ liệu bị thiếu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.5. Bài tập\n",
    "Tạo một tập dữ liệu với nhiều hàng và cột hơn.\n",
    "\n",
    "1. Xoá cột có nhiều giá trị bị thiếu nhất.\n",
    "2. Chuyển bộ dữ liệu đã được xử lý sang định dạng ndarray."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Đại số tuyến tính\n",
    "### 2.3.1. Số vô hướng (scalars)\n",
    "Chúng ta gọi các giá trị mà chỉ bao gồm một số duy nhất là *số vô hướng* (*scalar*).\n",
    "\n",
    "Một số vô hướng được biểu diễn bằng một tensor chỉ với một phần tử. Trong đoạn mã tiếp theo, chúng tôi khởi tạo hai số vô hướng và thực hiện một số phép toán số học quen thuộc với chúng, cụ thể là cộng, nhân, chia và lũy thừa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([5.]), tensor([6.]), tensor([1.5000]), tensor([9.]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([3.0])\n",
    "y = torch.tensor([2.0])\n",
    "\n",
    "x + y, x * y, x / y, x**y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2. Vector\n",
    "Bạn có thể xem vector đơn thuần như một dãy các số vô hướng. Chúng ta gọi các giá trị đó là phần tử (thành phần) của vector. Khi dùng vector để biễu diễn các mẫu trong tập dữ liệu, giá trị của chúng thường mang ý nghĩa liên quan tới đời thực. Ví dụ, nếu chúng ta huấn luyện một mô hình dự đoán rủi ro vỡ nợ, chúng ta có thể gán cho mỗi ứng viên một vector gồm các thành phần tương ứng với thu nhập, thời gian làm việc, số lần vỡ nợ trước đó của họ và các yếu tố khác.\n",
    "\n",
    "chúng ta làm việc với vector thông qua các tensor 1 -chiều. Thường thì tensor có thể có chiều dài bất kỳ, tùy thuộc vào giới hạn bộ nhớ máy tính."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
