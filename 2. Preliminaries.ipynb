{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dive into deep learning\n",
    "# 2. Sơ bộ\n",
    "Để bắt đầu với học sâu, ta sẽ cần nắm bắt một vài kỹ năng cơ bản. Tất cả những vấn đề về học máy đều có liên quan đến việc trích xuất thông tin từ dữ liệu. Vì vậy, chúng tôi sẽ bắt đầu bằng cách học các kỹ năng thực tế để lưu trữ, thao tác và xử lý dữ liệu.\n",
    "\n",
    "Hơn nữa, học máy thường yêu cầu làm việc với các tập dữ liệu lớn, mà chúng ta có thể coi như ở dạng bảng, trong đó các hàng tương ứng với các mẫu (*examples *) và các cột tương ứng với các thuộc tính (*attributes*). Đại số tuyến tính cung cấp cho ta một tập kỹ thuật mạnh mẽ để làm việc với dữ liệu dạng bảng. Chúng ta sẽ không đi quá sâu mà chỉ tập trung cơ bản vào các toán tử ma trận cơ bản và cách thực thi chúng.\n",
    "\n",
    "Bên cạnh đó, học sâu luôn liên quan đến tối ưu hoá (*optimization*). Chúng ta có một mô hình với bộ tham số và muốn tìm ra các tham số khớp với dữ liệu nhất. Việc xác định cách điều chỉnh từng tham số ở mỗi bước trong thuật toán đòi hỏi một chút kiến thức về giải tích, mà sẽ được giới thiệu ngắn gọn dưới đây. May thay, gói **autograd** sẽ tự động tính đạo hàm cho chúng ta, và sẽ được đề cập ngay sau đó.\n",
    "\n",
    "Kế tiếp, học máy liên quan đến việc đưa ra những dự đoán như: Xác định giá trị của một số thuộc tính chưa biết dựa trên thông tin quan sát được? Để suy luận chặt chẽ dưới sự bất định, chúng ta sẽ cần tìm đến ngôn ngữ của xác suất.\n",
    "\n",
    "Cuốn sách này đã cung cấp nội dung toán học ở mức tối thiểu cần có để có được sự hiểu biết đúng đắn về học sâu. Tuy nhiên, điều đó không đồng nghĩa rằng cuốn sách này không cần các kiến thức toán học. Do vậy, chương này sẽ giới thiệu nhanh về các kiến thức toán học cơ bản và thông dụng, cho phép tất cả mọi người tối thiểu là sẽ hiểu được hầu hết nội dung toán trong quyển sách này. Nếu bạn muốn hiểu tất cả nội dung toán học, hãy tham khảo thêm **chap_appendix_math**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Thao tác với Dữ liệu\n",
    "Để hoàn thành công việc, chúng ta cần một số cách để lưu trữ và thao tác dữ liệu. Nói chung, có hai điều quan trọng chúng ta cần làm với dữ liệu: (i) thu thập chúng; và (ii) xử lý chúng khi chúng ở bên trong máy tính. Không có ích gì khi thu thập dữ liệu mà không có cách nào đó để lưu trữ, vì vậy trước tiên chúng ta hãy chơi với dữ liệu tổng hợp. Để bắt đầu, chúng tôi giới thiệu mảng n chiều, còn được gọi là **tensor**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nếu bạn đã làm việc với NumPy, gói tính toán khoa học được sử dụng rộng rãi nhất trong Python, thì bạn sẽ thấy phần này quen thuộc. Lớp tensor (`ndarray` trong MXNet, `tensor` trong cả PyTorch và TensorFlow) tương tự như NumPy’s `ndarray` với một vài tính năng đặt biệt. Đầu tiên, GPU được hỗ trợ tốt để tăng tốc tính toán trong khi NumPy chỉ hỗ trợ tính toán CPU. Thứ hai, lớp tensor hỗ trợ tính vi phân tự động. Những đặc tính này làm cho lớp tensor thích hợp cho việc học sâu. Trong suốt cuốn sách, khi chúng ta nói đến tensor, chúng ta đang đề cập đến các trường hợp của lớp tensor trừ khi được nêu khác."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1. Bắt đầu\n",
    "Trong mục này, mục tiêu của chúng tôi là trang bị cho bạn các kiến thức toán cơ bản và cài đặt các công cụ tính toán mà bạn sẽ xây dựng dựa trên nó xuyên suốt cuốn sách này. Đừng lo nếu bạn gặp khó khăn với các khái niệm toán khó hiểu hoặc các hàm trong thư viện tính toán. Các mục tiếp theo sẽ nhắc lại những khái niệm này trong từng ngữ cảnh kèm theo ví dụ thực tiễn. Mặt khác, nếu bạn đã có kiến thức nền tảng và muốn đi sâu hơn vào các nội dung toán, bạn có thể bỏ qua mục này."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.arange(12)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để bắt đầu, chúng ta có thể sử dụng `arange` để tạo một vector hàng x chứa 12 số nguyên đầu tiên bắt đầu bằng 0, mặc dù chúng được tạo dưới dạng `float` theo mặc định. Mỗi giá trị trong *tensor* được gọi là một phần tử của *tensor*. Ví dụ, có 12 phần tử trong *tensor* `x`. Trừ khi được chỉ định khác, một *tensor* mới sẽ được lưu trữ trong bộ nhớ chính và được chỉ định để tính toán trên CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng ta có thể lấy kích thước (độ dài theo mỗi trục) của tensor bằng thuộc tính `shape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nếu chỉ muốn biết tổng số phần tử của một tensor, nghĩa là tích của tất cả các thành phần trong shape, ta có thể sử dụng thuộc tính size. Vì ta đang làm việc với một vector, cả shape và size của nó đều chứa cùng một phần tử duy nhất."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để thay đổi kích thước của một tensor mà không làm thay đổi số lượng phần tử cũng như giá trị của chúng, ta có thể gọi hàm `reshape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = x.reshape(3, 4)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Việc chỉ định cụ thể mọi chiều khi thay đổi kích thước là không cần thiết. Nếu kích thước mong muốn là một ma trận với kích thước (chiều_cao, chiều_rộng), thì sau khi biết chiều_rộng, chiều_cao có thể được ngầm suy ra. Tại sao ta lại cần phải tự làm phép tính chia? Trong ví dụ trên, để có được một ma trận với  3  hàng, chúng ta phải chỉ định rõ rằng nó có  3  hàng và  4  cột. May mắn thay, ndarray có thể tự động tính một chiều từ các chiều còn lại. Ta có thể dùng chức năng này bằng cách đặt -1 cho chiều mà ta muốn ndarray tự suy ra. Trong trường hợp vừa rồi, thay vì gọi `x.reshape(3, 4)`, ta có thể gọi `x.reshape(-1, 4)` hoặc `x.reshape(3, -1)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thông thường, chúng tôi sẽ muốn các ma trận của mình được khởi tạo bằng số không, số một, một số hằng số khác hoặc số được lấy mẫu ngẫu nhiên từ một phân phối cụ thể. Chúng ta có thể tạo một tensor đại diện cho tensor với tất cả các phần tử được đặt thành 0 và hình dạng của (2, 3, 4) như sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(2, 3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tương tự, chúng ta có thể tạo các tensor với mỗi phần tử được đặt thành 1 như sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(2, 3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đoạn mã dưới đây tạo một ndarray có kích thước ( 3 ,  4 ) với các phần tử được lấy mẫu ngẫu nhiên từ một phân phối **Gauss** (phân phối chuẩn) với trung bình bằng  0  và độ lệch chuẩn  1 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3669,  0.1273, -0.1305, -2.7554],\n",
       "        [ 0.3201,  0.1790,  0.2746, -0.1316],\n",
       "        [-1.5317, -0.5282,  2.2136,  0.9959]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta cũng có thể khởi tạo giá trị cụ thể cho mỗi phần tử trong tenor mong muốn bằng cách đưa vào một mảng Python (hoặc mảng của mảng) chứa các giá trị số. Ở đây, mảng ngoài cùng tương ứng với trục  0 , và mảng bên trong tương ứng với trục 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 1, 4, 3],\n",
       "        [1, 2, 3, 4],\n",
       "        [4, 3, 2, 1]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. Phép toán\n",
    "Các phép toán tiêu chuẩn (+, -, * *, /, và * ** ) là các phép toán theo từng phần tử trên các tensor đồng kích thước bất kỳ. Ta có thể gọi những phép toán theo từng phần tử lên hai tensor đồng kích thước. Trong ví dụ dưới đây, các dấu phẩy được sử dụng để tạo một tuple  5  phần tử với mỗi phần tử là kết quả của một phép toán theo từng phần tử."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3.,  4.,  6., 10.]),\n",
       " tensor([-1.,  0.,  2.,  6.]),\n",
       " tensor([ 2.,  4.,  8., 16.]),\n",
       " tensor([0.5000, 1.0000, 2.0000, 4.0000]),\n",
       " tensor([ 1.,  4., 16., 64.]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2, 4, 8])\n",
    "y = torch.tensor([2, 2, 2, 2])\n",
    "x + y, x - y, x * y, x / y, x ** y  # The ** operator is exponentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rất nhiều các phép toán khác có thể được áp dụng theo từng phần tử, bao gồm các phép toán đơn ngôi như hàm mũ cơ số  e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta cũng có thể nối (*concatenate*) nhiều tensor với nhau, xếp chồng chúng lên nhau để tạo ra một tensor lớn hơn. Ta chỉ cần cung cấp một danh sách các tensor và khai báo chúng được nối theo trục nào. Ví dụ dưới đây thể hiện cách nối hai ma trận theo hàng (trục  0 , phần tử đầu tiên của kích thước) và theo cột (trục  1 , phần tử thứ hai của kích thước). Ta có thể thấy rằng, cách thứ nhất tạo một tensor với độ dài trục - $0(6)$ bằng tổng các độ dài trục  0  của hai tensor đầu vào ( 3+3 ); trong khi cách thứ hai tạo một tensor với độ dài trục - $1(8)$ bằng tổng các độ dài trục  1  của hai tensor đầu vào ( 4+4 )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [ 2.,  1.,  4.,  3.],\n",
       "         [ 1.,  2.,  3.,  4.],\n",
       "         [ 4.,  3.,  2.,  1.]]),\n",
       " tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
       "         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(12, dtype=torch.float32).reshape((3,4))\n",
    "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đôi khi, ta muốn tạo một tensor nhị phân thông qua các mệnh đề logic. Lấy `x == y` làm ví dụ. Với mỗi vị trí, nếu giá trị củax và y tại vị trí đó bằng nhau thì phần tử tương ứng trong tensor mới lấy giá trị  1 , nghĩa là mệnh đề logic `x == y` là đúng tại vị trí đó; ngược lại vị trí đó lấy giá trị 0 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True, False,  True],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X == Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lấy tổng mọi phần tử trong một tensor tạo ra một tensor với chỉ một phần tử."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(66.)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3. Cơ chế Lan truyền (Broadcasting Mechanism)\n",
    "Trong mục trên, ta đã thấy cách thực hiện các phép toán theo từng phần tử với hai tensor đồng kích thước. Trong những điều kiện nhất định, thậm chí khi kích thước khác nhau, ta vẫn có thể thực hiện các phép toán theo từng phần tử bằng cách sử dụng cơ chế lan truyền (broadcasting mechanism). Cơ chế này hoạt động như sau: Thứ nhất, mở rộng một hoặc cả hai mảng bằng cách lặp lại các phần tử một cách hợp lý sao cho sau phép biến đổi này, hai tensor có cùng kích thước. Thứ hai, thực hiện các phép toán theo từng phần tử với hai mảng mới này.\n",
    "\n",
    "Trong hầu hết các trường hợp, chúng ta lan truyền một mảng theo trục có độ dài ban đầu là  1 , như ví dụ dưới đây:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0],\n",
       "         [1],\n",
       "         [2]]),\n",
       " tensor([[0, 1]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(3).reshape((3, 1))\n",
    "b = torch.arange(2).reshape((1, 2))\n",
    "a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vì a và b là các ma trận có kích thước lần lượt là  3×1  và  1×2 , kích thước của chúng không khớp nếu ta muốn thực hiện phép cộng. Ta lan truyền các phần tử của cả hai ma trận thành các ma trận  3×2  như sau: lặp lại các cột trong ma trận a và các hàng trong ma trận b trước khi cộng chúng theo từng phần tử."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [1, 2],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4. Chỉ số và Cắt chọn mảng\n",
    "Cũng giống như trong bất kỳ mảng Python khác, các phần tử trong một tensor có thể được truy cập theo chỉ số. Tương tự, phần tử đầu tiên có chỉ số  0  và khoảng được cắt chọn bao gồm phần tử đầu tiên nhưng không tính phần tử cuối cùng. Và trong các danh sách Python tiêu chuẩn, chúng ta có thể truy cập các phần tử theo vị trí đếm ngược từ cuối danh sách bằng cách sử dụng các chỉ số âm.\n",
    "\n",
    "Vì vậy, [-1] chọn phần tử cuối cùng và [1:3] chọn phần tử thứ hai và phần tử thứ ba như sau:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 8.,  9., 10., 11.]),\n",
       " tensor([[ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[-1], X[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ngoài việc đọc, chúng ta cũng có thể viết các phần tử của ma trận bằng cách chỉ định các chỉ số."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  9.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1, 2] = 9\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nếu chúng ta muốn gán cùng một giá trị cho nhiều phần tử, chúng ta chỉ cần trỏ đến tất cả các phần tử đó và gán giá trị cho chúng. Chẳng hạn, [0:2 ,:] truy cập vào hàng thứ nhất và thứ hai, trong đó : lấy tất cả các phần tử dọc theo trục  1  (cột). Ở đây chúng ta đã thảo luận về cách truy cập vào ma trận, nhưng tất nhiên phương thức này cũng áp dụng cho các vector và tensor với nhiều hơn  2  chiều."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12., 12., 12., 12.],\n",
       "        [12., 12., 12., 12.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:2, :] = 12\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.5. Tiết kiệm Bộ nhớ\n",
    "Ở ví dụ trước, mỗi khi chạy một phép tính, chúng ta sẽ cấp phát bộ nhớ mới để lưu trữ kết quả của lượt chạy đó. Cụ thể hơn, nếu viết y = x + y, ta sẽ ngừng tham chiếu đến tensor mà y đã chỉ đến trước đó và thay vào đó gán y vào bộ nhớ được cấp phát mới. Trong ví dụ tiếp theo, chúng ta sẽ minh họa việc này với hàm id() của Python - hàm cung cấp địa chỉ chính xác của một đối tượng được tham chiếu trong bộ nhớ. Sau khi chạy y = y + x, chúng ta nhận ra rằng id(y) chỉ đến một địa chỉ khác. Đó là bởi vì Python trước hết sẽ tính y + x, cấp phát bộ nhớ mới cho kết quả trả về và gán y vào địa chỉ mới này trong bộ nhớ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(Y)\n",
    "Y = Y + X\n",
    "id(Y) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đây có thể là điều không mong muốn vì hai lý do. Thứ nhất, không phải lúc nào chúng ta cũng muốn cấp phát bộ nhớ không cần thiết. Trong học máy, ta có thể có đến hàng trăm megabytes tham số và cập nhật tất cả chúng nhiều lần mỗi giây, và thường thì ta muốn thực thi các cập nhật này tại chỗ. Thứ hai, chúng ta có thể trỏ đến cùng tham số từ nhiều biến khác nhau. Nếu không cập nhật tại chỗ, các bộ nhớ đã bị loại bỏ sẽ không được giải phóng, dẫn đến khả năng một số chỗ trong mã nguồn sẽ vô tình tham chiếu lại các tham số cũ.\n",
    "\n",
    "May mắn thay, việc thực hiện các thao tác tại chỗ rất dễ dàng.  Chúng ta có thể gán kết quả của một phép tính cho một mảng đã được cấp phát trước đó bằng ký hiệu cắt chọn (slice notation), ví dụ, y[:] = <expression>. Để minh họa khái niệm này, đầu tiên chúng ta tạo một ma trận mới z có cùng kích thước với ma trận y, sử dụng zeros_like để gán giá trị khởi tạo bằng  0 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id(Z): 2704205491776\n",
      "id(Z): 2704205491776\n"
     ]
    }
   ],
   "source": [
    "Z = torch.zeros_like(Y)\n",
    "print('id(Z):', id(Z))\n",
    "Z[:] = X + Y\n",
    "print('id(Z):', id(Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(X)\n",
    "X += Y\n",
    "id(X) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.6. Chuyển đổi sang các Đối Tượng Python Khác\n",
    "Dễ dàng chuyển đổi sang tensor NumPy hoặc ngược lại. Kết quả được chuyển đổi không chia sẻ bộ nhớ. Sự bất tiện nhỏ này thực sự khá quan trọng: khi bạn thực hiện các hoạt động trên CPU hoặc trên GPU, bạn không muốn tạm dừng tính toán, chờ xem liệu gói NumPy của Python có thể muốn làm việc gì khác với cùng một phần bộ nhớ hay không."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, torch.Tensor)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = X.numpy()\n",
    "B = torch.tensor(A)\n",
    "type(A), type(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để chuyển đổi một tensor kích thước-1 thành một đại lượng vô hướng Python, chúng ta có thể gọi hàm item hoặc các hàm tích hợp của Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3.5000]), 3.5, 3.5, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([3.5])\n",
    "a, a.item(), float(a), int(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.7. Tổng kết\n",
    "Giao diện chính để lưu trữ và thao tác dữ liệu cho học sâu là tensor (mảng n -dimensional). Nó cung cấp nhiều chức năng bao gồm các phép toán cơ bản, phát sóng, lập chỉ mục, cắt, tiết kiệm bộ nhớ và chuyển đổi sang các đối tượng Python khác."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Chạy đoạn mã nguồn trong mục này. Thay đổi điều kiện mệnh đề x == y sang x < y hoặc x > y, sau đó kiểm tra dạng của tensor nhận được.\n",
    "2. Thay hai tensor trong phép tính theo từng phần tử ở phần cơ chế lan truyền (broadcasting mechanism) với các tensor có kích thước khác, ví dụ như tensor ba chiều. Kết quả có giống như bạn mong đợi hay không?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Tiền xử lý dữ liệu\n",
    "Cho đến giờ chúng tôi đã đề cập tới rất nhiều kỹ thuật thao tác dữ liệu được lưu trong dạng tensor. Nhưng để áp dụng học sâu vào giải quyết các vấn đề thực tế, ta thường phải bắt đầu bằng việc xử lý dữ liệu thô, chứ không có luôn dữ liệu ngăn nắp được chuẩn bị sẵn trong định dạng tensor. Trong số các công cụ phân tích dữ liệu phổ biến của Python, gói *pandas* khá được ưa chuộng. Cũng như nhiều gói khác trong hệ sinh thái rộng lớn của Python, ‘*pandas*’ có thể được sử dụng kết hợp với định dạng tensor. Vì vậy, chúng ta sẽ đi nhanh qua các bước để tiền xử lý dữ liệu thô bằng pandas rồi đổi chúng sang dạng tensor. Nhiều kỹ thuật tiền xử lý dữ liệu khác sẽ được giới thiệu trong các chương sau."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1. Đọc tập dữ liệu\n",
    "Để lấy ví dụ, ta bắt đầu bằng việc tạo một tập dữ liệu nhân tạo lưu trong file csv `../data/house_tiny.csv` (csv - comma-separated values - giá trị tách nhau bằng dấu phẩy). Dữ liệu lưu ở các định dạng khác cũng có thể được xử lý tương tự. Hàm `mkdir_if_not_exist` dưới đây để đảm bảo rằng thư mục `../data` tồn tại. Chú thích # Saved in the d2l package for later use (Lưu lại trong gói d2l để dùng sau) là kí hiệu đánh dấu các hàm, lớp hoặc các lệnh `import` được lưu trong gói d2l, để sau này ta có thể trực tiếp gọi hàm `d2l.mkdir_if_not_exist()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def mkdir_if_not_exist(path):  #@save\n",
    "    \"\"\"Make a directory if it does not exist.\"\"\"\n",
    "    if not isinstance(path, str):\n",
    "        path = os.path.join(*path)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau đây ta ghi tệp dữ liệu vào file csv theo từng hàng một."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = '../data/house_tiny.csv'\n",
    "mkdir_if_not_exist('../data')\n",
    "with open(data_file, 'w') as f:\n",
    "    f.write('NumRooms,Alley,Price\\n')  # Column names\n",
    "    f.write('NA,Pave,127500\\n')  # Each row represents a data example\n",
    "    f.write('2,NA,106000\\n')\n",
    "    f.write('4,NA,178100\\n')\n",
    "    f.write('NA,NA,140000\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để gọi tập dữ liệu thô từ tệp csv vừa được tạo ra, ta dùng gói thư viện *pandas* và gọi hàm `read_csv`. Bộ dữ liệu này có  4  hàng và  3  cột, trong đó mỗi hàng biểu thị số phòng (“NumRooms”), kiểu lối đi (“Alley”), và giá (“Price”) của căn nhà."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms Alley   Price\n",
      "0       NaN  Pave  127500\n",
      "1       2.0   NaN  106000\n",
      "2       4.0   NaN  178100\n",
      "3       NaN   NaN  140000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(data_file)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. Xử lý dữ liệu thiếu\n",
    "Để ý rằng giá trị “`NaN`” là các giá trị bị thiếu. Để xử lý dữ liệu thiếu, các cách thường được áp dụng là quy buộc (*imputation*) và xoá bỏ (*deletion*), trong đó quy buộc thay thế giá trị bị thiếu bằng giá trị khác, trong khi xoá bỏ sẽ bỏ qua các giá trị bị thiếu. Dưới đây chúng ta xem xét phương pháp quy buộc.\n",
    "\n",
    "Bằng phương pháp đánh chỉ số theo số nguyên (`iloc`), chúng ta tách data thành `inputs` (tương ứng với hai cột đầu) và `outputs` (tương ứng với cột cuối cùng). Với các giá trị số bị thiếu trong `inputs`, ta thay thế phần tử “`NaN`” bằng giá trị trung bình cộng của cùng cột đó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms Alley\n",
      "0       3.0  Pave\n",
      "1       2.0   NaN\n",
      "2       4.0   NaN\n",
      "3       3.0   NaN\n"
     ]
    }
   ],
   "source": [
    "inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]\n",
    "inputs = inputs.fillna(inputs.mean())\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Với các giá trị dạng hạng mục hoặc số rời rạc trong inputs, ta coi “`NaN`” là một mục riêng. Vì cột “`Alley`” chỉ nhận 2 giá trị riêng lẻ là “`Pave`” (được lát gạch) và “`NaN`”, pandas có thể tự động chuyển cột này thành 2 cột “`Alley_Pave`” và “`Alley_nan`”. Những hàng có kiểu lối đi là “`Pave`” sẽ có giá trị của cột “`Alley_Pave`” và cột “`Alley_nan`” tương ứng là  1  và  0 . Hàng mà không có giá trị cho kiểu lối đi sẽ có giá trị cột “`Alley_Pave`” và cột “`Alley_nan`” lần lượt là  0  và  1 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NumRooms  Alley_Pave  Alley_nan\n",
      "0       3.0           1          0\n",
      "1       2.0           0          1\n",
      "2       4.0           0          1\n",
      "3       3.0           0          1\n"
     ]
    }
   ],
   "source": [
    "inputs = pd.get_dummies(inputs, dummy_na=True)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3. Chuyển sang định dạng tensor\n",
    "Giờ thì toàn bộ các giá trị trong inputs và outputs đã ở dạng số, chúng đã có thể được chuyển sang định dạng tensor. Khi đã ở định dạng này, chúng có thể được biến đổi và xử lý với những chức năng của ndarray đã được giới thiệu ở Section 2.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[3., 1., 0.],\n",
       "         [2., 0., 1.],\n",
       "         [4., 0., 1.],\n",
       "         [3., 0., 1.]], dtype=torch.float64),\n",
       " tensor([127500, 106000, 178100, 140000]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "X, y = torch.tensor(inputs.values), torch.tensor(outputs.values)\n",
    "X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4. Tóm tắt\n",
    "1. Cũng như nhiều gói mở rộng trong hệ sinh thái khổng lồ của Python, pandas có thể làm việc được với tensor.\n",
    "2. Phương pháp quy buộc hoặc xoá bỏ có thể dùng để xử lý dữ liệu bị thiếu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.5. Bài tập\n",
    "Tạo một tập dữ liệu với nhiều hàng và cột hơn.\n",
    "\n",
    "1. Xoá cột có nhiều giá trị bị thiếu nhất.\n",
    "2. Chuyển bộ dữ liệu đã được xử lý sang định dạng ndarray."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Đại số tuyến tính\n",
    "### 2.3.1. Số vô hướng (scalars)\n",
    "Chúng ta gọi các giá trị mà chỉ bao gồm một số duy nhất là *số vô hướng* (*scalar*).\n",
    "\n",
    "Một số vô hướng được biểu diễn bằng một tensor chỉ với một phần tử. Trong đoạn mã tiếp theo, chúng tôi khởi tạo hai số vô hướng và thực hiện một số phép toán số học quen thuộc với chúng, cụ thể là cộng, nhân, chia và lũy thừa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([5.]), tensor([6.]), tensor([1.5000]), tensor([9.]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([3.0])\n",
    "y = torch.tensor([2.0])\n",
    "\n",
    "x + y, x * y, x / y, x**y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2. Vector\n",
    "Bạn có thể xem vector đơn thuần như một dãy các số vô hướng. Chúng ta gọi các giá trị đó là phần tử (thành phần) của vector. Khi dùng vector để biễu diễn các mẫu trong tập dữ liệu, giá trị của chúng thường mang ý nghĩa liên quan tới đời thực. Ví dụ, nếu chúng ta huấn luyện một mô hình dự đoán rủi ro vỡ nợ, chúng ta có thể gán cho mỗi ứng viên một vector gồm các thành phần tương ứng với thu nhập, thời gian làm việc, số lần vỡ nợ trước đó của họ và các yếu tố khác.\n",
    "\n",
    "chúng ta làm việc với vector thông qua các tensor 1 -chiều. Thường thì tensor có thể có chiều dài bất kỳ, tùy thuộc vào giới hạn bộ nhớ máy tính."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2.1. Độ dài, Chiều, và Kích thước\n",
    "Một vector đơn thuần là một dãy các số. Mỗi vector, tương tự như dãy, đều có một độ dài. Trong ký hiệu toán học, nếu ta muốn nói rằng một vector $\\mathbf{x}$ chứa $n$ các số thực vô hướng, ta có thể biểu diễn nó bằng $\\mathbf{x} \\in \\mathbb{R}^n$ . Độ dài của một vector còn được gọi là số **chiều** của vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .3.3. Ma trận\n",
    "Ma trận thường được ký hiệu với ký tự hoa và được in đậm (ví dụ: $\\mathbf{X}$, $\\mathbf{Y}$, và $\\mathbf{Z}$ ); và được biểu diễn bằng các tensor với $2$ trục khi lập trình.\n",
    "\n",
    "Trong ký hiệu toán học, ta dùng $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ để biểu thị một ma trận  A  gồm  m  hàng và  n  cột các giá trị số thực. Về mặt hình ảnh, ta có thể minh họa bất kỳ ma trận $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ như một bảng biểu mà mỗi phần tử $a_{ij}$ nằm ở dòng thứ $i$ và cột thứ $j$ của bảng:\n",
    "\\begin{split}\\mathbf{A}=\\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\\\ \\end{bmatrix}.\\end{split}\n",
    "\n",
    "Trong trường hợp đặc biệt, khi một ma trận có số dòng bằng số cột, dạng của nó là một hình vuông; như vậy, nó được gọi là một *ma trận vuông (square matrix)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11],\n",
       "        [12, 13, 14, 15],\n",
       "        [16, 17, 18, 19]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(20).reshape(5, 4)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong mã nguồn, ta lấy chuyển vị của một ma trận thông qua thuộc tính T."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  4,  8, 12, 16],\n",
       "        [ 1,  5,  9, 13, 17],\n",
       "        [ 2,  6, 10, 14, 18],\n",
       "        [ 3,  7, 11, 15, 19]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Là một biến thể đặc biệt của ma trận vuông, *ma trận đối xứng (symmetric matrix)* $\\mathbf{A}$ có chuyển vị bằng chính nó: $\\mathbf{A} = \\mathbf{A}^\\top$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [2, 0, 4],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B == B.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4. Tensor\n",
    "Giống như vector khái quát hoá số vô hướng và ma trận khái quát hoá vector, ta có thể xây dựng những cấu trúc dữ liệu với thậm chí nhiều trục hơn. Tensor cho chúng ta một phương pháp tổng quát để miêu tả các n-chiều với số trục bất kỳ. Ví dụ, vector là các tensor bậc một còn ma trận là các tensor bậc hai. Tensor được ký hiệu với ký tự viết hoa sử dụng một font chữ đặc biệt (ví dụ: $\\mathsf{X}$, $\\mathsf{Y}$, và $\\mathsf{Z}$) và có cơ chế truy vấn (ví dụ: $x_{ijk}$ and  $[\\mathsf{X}]_{1, 2i-1, 3}$) giống như ma trận.\n",
    "\n",
    "Tensor sẽ trở nên quan trọng hơn khi ta bắt đầu làm việc với hình ảnh, thường được biểu diễn dưới dạng ndarray với 3 trục tương ứng với chiều cao, chiều rộng và một trục kênh (channel) để xếp chồng các kênh màu (đỏ, xanh lá và xanh dương). Tạm thời, ta sẽ bỏ qua các tensor bậc cao hơn và tập trung vào những điểm cơ bản trước."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.5. Các thuộc tính Cơ bản của Phép toán Tensor\n",
    "Số vô hướng, vector, ma trận và tensor với một số trục bất kỳ có một vài thuộc tính rất hữu dụng. Ví dụ, bạn có thể để ý từ định nghĩa của phép toán theo từng phần tử (*elementwise*), bất kỳ phép toán theo từng phần tử một ngôi nào cũng không làm thay đổi kích thước của toán hạng của nó. Tương tự, cho hai tensor bất kỳ có cùng kích thước, kết quả của bất kỳ phép toán theo từng phần tử hai ngôi sẽ là một tensor có cùng kích thước. Ví dụ, cộng hai ma trận có cùng kích thước sẽ thực hiện phép cộng theo từng phần tử giữa hai ma trận này."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [12., 13., 14., 15.],\n",
       "         [16., 17., 18., 19.]]),\n",
       " tensor([[ 0.,  2.,  4.,  6.],\n",
       "         [ 8., 10., 12., 14.],\n",
       "         [16., 18., 20., 22.],\n",
       "         [24., 26., 28., 30.],\n",
       "         [32., 34., 36., 38.]]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "B = A.clone()  # Assign a copy of `A` to `B` by allocating new memory\n",
    "A, A + B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đặc biệt, phép nhân theo phần tử của hai ma trận được gọi là phép nhân Hadamard (Hadamard product – ký hiệu toán học là $\\odot$ ). Xét ma trận $\\mathbf{B} \\in \\mathbb{R}^{m \\times n}$ có phần tử dòng $i$ và cột $j$ là $b_{ij}$. Phép nhân Hadamard giữa ma trận $\\mathbf{A}$ và $\\mathbf{B}$\n",
    "\\begin{split}\\mathbf{A} \\odot \\mathbf{B} =\n",
    "\\begin{bmatrix}\n",
    "    a_{11}  b_{11} & a_{12}  b_{12} & \\dots  & a_{1n}  b_{1n} \\\\\n",
    "    a_{21}  b_{21} & a_{22}  b_{22} & \\dots  & a_{2n}  b_{2n} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    a_{m1}  b_{m1} & a_{m2}  b_{m2} & \\dots  & a_{mn}  b_{mn}\n",
    "\\end{bmatrix}.\\end{split}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,   1.,   4.,   9.],\n",
       "        [ 16.,  25.,  36.,  49.],\n",
       "        [ 64.,  81., 100., 121.],\n",
       "        [144., 169., 196., 225.],\n",
       "        [256., 289., 324., 361.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A*B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nhân hoặc cộng một tensor với một số vô hướng cũng sẽ không thay đổi kích thước của tensor, mỗi phần tử của tensor sẽ được cộng hoặc nhân cho số vô hướng đó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 2,  3,  4,  5],\n",
       "          [ 6,  7,  8,  9],\n",
       "          [10, 11, 12, 13]],\n",
       " \n",
       "         [[14, 15, 16, 17],\n",
       "          [18, 19, 20, 21],\n",
       "          [22, 23, 24, 25]]]),\n",
       " torch.Size([2, 3, 4]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 2\n",
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "a + X, (a * X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.6. Rút gọn\n",
    "Một phép toán hữu ích mà ta có thể thực hiện trên bất kỳ tensor nào là phép tính tổng các phần tử của nó. Ký hiệu toán học của phép tính tổng là $\\sum$. Ta biểu diễn phép tính tổng các phần tử của một vector  $\\mathbf{x}$  với độ dài  $d$  dưới dạng  $\\sum_{i=1}^d x_i$. Trong mã nguồn, ta chỉ cần gọi hàm `sum`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.]), tensor(6.))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(4, dtype=torch.float32)\n",
    "x, x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theo mặc định, hàm sum sẽ rút gọn tensor dọc theo tất cả các trục của nó và trả về kết quả là một số vô hướng. Ta cũng có thể chỉ định các trục được rút gọn bằng phép tổng. Lấy ma trận làm ví dụ, để rút gọn theo chiều hàng (trục  0 ) bằng việc tính tổng tất cả các hàng, ta đặt `axis=0` khi gọi hàm `sum`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([40., 45., 50., 55.]), torch.Size([4]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sum_axis0 = A.sum(axis=0)\n",
    "A_sum_axis0, A_sum_axis0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.6.1. Tổng không rút gọn\n",
    "Tuy nhiên, việc giữ lại số các trục đôi khi là cần thiết khi gọi hàm `sum` hoặc `mean`, bằng cách đặt `keepdims=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.],\n",
       "        [22.],\n",
       "        [38.],\n",
       "        [54.],\n",
       "        [70.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_A = A.sum(axis=1, keepdims=True)\n",
    "sum_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ví dụ, vì `sum_A` vẫn giữ lại  2  trục sau khi tính tổng của mỗi hàng, chúng ta có thể chia `A` cho `sum_A` thông qua cơ chế lan truyền."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nếu chúng ta muốn tính tổng tích lũy các phần tử của `A` dọc theo các trục, giả sử `axis=0` (từng hàng một), ta có thể gọi hàm `cumsum`. Hàm này không rút gọn chiều của tensor đầu vào theo bất cứ trục nào."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  6.,  8., 10.],\n",
       "        [12., 15., 18., 21.],\n",
       "        [24., 28., 32., 36.],\n",
       "        [40., 45., 50., 55.]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.cumsum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.7. Tích vô hướng\n",
    "Cho đến giờ, chúng ta mới chỉ thực hiện những phép tính từng phần tử tương ứng, như tổng và trung bình. Nếu đây là tất những gì chúng ta có thể làm, đại số tuyến tính có lẽ không xứng đáng để có nguyên một mục. Tuy nhiên, một trong nhưng phép tính căn bản nhất của đại số tuyến tính là tích vô hướng. Với hai vector $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^d$ cho trước, tích vô hướng (dot product) $\\mathbf{x}^\\top \\mathbf{y}$ (hoặc $\\langle \\mathbf{x}, \\mathbf{y} \\rangle$ ) là tổng các tích của những phần tử có cùng vị trí: $\\mathbf{x}^\\top \\mathbf{y} = \\sum_{i=1}^{d} x_i y_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.ones(4, dtype = torch.float32)\n",
    "x, y, torch.dot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lưu ý rằng chúng ta có thể thể hiện tích vô hướng của hai vector một cách tương tự bằng việc thực hiện tích từng phần tử tương ứng rồi lấy tổng:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(x * y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tích vô hướng thể hiện phép tính trung bình trọng số (*weighted average*). Sau khi được chuẩn hoá thành hai vector đơn vị, tích vô hướng của hai vector đó là giá trị cos của góc giữa hai vector đó. Chúng tôi sẽ giới thiệu khái niệm về *độ dài* ở các phần sau trong mục này."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.8. Tích giữa Ma trận và Vector\n",
    "Chúng ta có thể nghĩ đến việc nhân một ma trận $\\mathbf{A}\\in \\mathbb{R}^{m \\times n}$ với một vector $\\mathbf{x} \\in \\mathbb{R}^n$ như một phép biến hình, chiếu vector từ không gian $\\mathbb{R}^{n}$ thành $\\mathbb{R}^{m}$. Những phép biến hình này hóa ra lại trở nên rất hữu dụng. Ví dụ, chúng ta có thể biểu diễn phép xoay là tích với một ma trận vuông. Bạn sẽ thấy ở những chương tiếp theo, chúng ta cũng có thể sử dụng tích giữa ma trận và vector để thực hiện hầu hết những tính toán cần thiết khi tính các tầng trong một mạng nơ-ron dựa theo kết quả của tầng trước đó.\n",
    "\n",
    "Khi lập trình, để thực hiện nhân ma trận với vector, chúng ta cũng sử dụng hàm `dot` giống như tích vô hướng. Việc gọi np.dot(A, x) với ma trận A và một vector x sẽ thực hiện phép nhân vô hướng giữa ma trận và vector. Lưu ý rằng chiều của cột A (chiều dài theo trục  1 ) phải bằng với chiều của vector x (chiều dài của nó)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 4]), torch.Size([4]), tensor([ 14.,  38.,  62.,  86., 110.]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape, x.shape, torch.mv(A, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.9. Phép nhân Ma trận\n",
    "Nếu bạn đã quen với tích vô hướng và tích ma trận-vector, tích ma trận-ma trận cũng tương tự như thế.\n",
    "\n",
    "$\\begin{split}\\mathbf{C} = \\mathbf{AB} = \\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\\\\n",
    "\\mathbf{a}^\\top_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{a}^\\top_n \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    " \\mathbf{b}_{1} & \\mathbf{b}_{2} & \\cdots & \\mathbf{b}_{m} \\\\\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}\n",
    "\\mathbf{a}^\\top_{1} \\mathbf{b}_1 & \\mathbf{a}^\\top_{1}\\mathbf{b}_2& \\cdots & \\mathbf{a}^\\top_{1} \\mathbf{b}_m \\\\\n",
    " \\mathbf{a}^\\top_{2}\\mathbf{b}_1 & \\mathbf{a}^\\top_{2} \\mathbf{b}_2 & \\cdots & \\mathbf{a}^\\top_{2} \\mathbf{b}_m \\\\\n",
    " \\vdots & \\vdots & \\ddots &\\vdots\\\\\n",
    "\\mathbf{a}^\\top_{n} \\mathbf{b}_1 & \\mathbf{a}^\\top_{n}\\mathbf{b}_2& \\cdots& \\mathbf{a}^\\top_{n} \\mathbf{b}_m\n",
    "\\end{bmatrix}.\\end{split}$\n",
    "\n",
    "Ta có thể coi tích hai ma trận $\\mathbf{AB}$ như việc tính $m$ phép nhân ma trận và vector, sau đó ghép các kết quả với nhau để tạo ra một ma trận $n×m$. Giống như tích vô hướng và phép nhân ma trận-vector, ta có thể tính phép nhân hai ma trận bằng cách sử dụng hàm dot. Trong đoạn mã dưới đây, chúng ta tính phép nhân giữa A và B. Ở đây, A là một ma trận với  $5$  hàng  $4$  cột và B là một ma trận với $4$ hàng $3$ cột. Sau phép nhân này, ta thu được một ma trận với  $5$  hàng  $3$  cột."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6.,  6.,  6.],\n",
       "        [22., 22., 22.],\n",
       "        [38., 38., 38.],\n",
       "        [54., 54., 54.],\n",
       "        [70., 70., 70.]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.ones(4, 3)\n",
    "torch.mm(A, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.10. Chuẩn\n",
    "Một trong những toán tử hữu dụng nhất của đại số tuyến tính là *chuẩn (norm)*. Nói dân dã thì, các chuẩn của một vector cho ta biết một vector *lớn* tầm nào. Thuật ngữ *kích thước* đang xét ở đây không nói tới số chiều không gian mà đúng hơn là về độ lớn của các thành phần.\n",
    "\n",
    "Trong đại số tuyến tính, chuẩn của một vector là hàm số $f$  ánh xạ một vector đến một số vô hướng, thỏa mãn các tính chất sau. Cho vector $\\mathbf{x}$  bất kỳ, tính chất đầu tiên phát biểu rằng nếu chúng ta co giãn toàn bộ các phần tử của một vector bằng một hằng số $\\alpha$ , chuẩn của vector đó cũng co giãn theo giá trị tuyệt đối của hằng số đó:\n",
    "\n",
    "$f(\\alpha \\mathbf{x}) = |\\alpha| f(\\mathbf{x})$\n",
    "\n",
    "Tính chất thứ hai cũng giống như bất đẳng thức tam giác:\n",
    "\n",
    "$f(\\mathbf{x} + \\mathbf{y}) \\leq f(\\mathbf{x}) + f(\\mathbf{y})$.\n",
    "\n",
    "Tính chất thứ ba phát biểu rằng chuẩn phải không âm:\n",
    "\n",
    "$f(\\mathbf{x}) \\geq 0$\n",
    "\n",
    "Bạn chắc sẽ để ý là các chuẩn có vẻ giống như một phép đo khoảng cách. Và nếu còn nhớ khái niệm khoảng cách Euclid (định lý Pythagoras) được học ở phổ thông, thì khái niệm không âm và bất đẳng thức tam giác có thể gợi nhắc lại một chút. Thực tế là, khoảng cách Euclid cũng là một chuẩn: cụ thể là  $\\ell_2$ . Giả sử rằng các thành phần trong vector  $n$  chiều  $\\mathbf{x}$  là  $x1,…,xn$ . Chuẩn $ \\ell_2$  của  $\\mathbf{x}$  là căn bậc hai của tổng các bình phương của các thành phần trong vector:\n",
    "\n",
    "$\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum_{i=1}^n x_i^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = torch.tensor([3.0, -4.0])\n",
    "torch.norm(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trong học sâu, chúng ta thường gặp chuẩn $\\ell_2$  bình phương hơn. Bạn cũng sẽ thường xuyên gặp chuẩn $\\ell_1$ , chuẩn được biểu diễn bằng tổng các giá trị tuyệt đối của các thành phần trong vector:\n",
    "\n",
    "$\\|\\mathbf{x}\\|_1 = \\sum_{i=1}^n \\left|x_i \\right|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(u).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cả hai chuẩn $ \\ell_2$  và $ \\ell_1$  đều là trường hợp riêng của một chuẩn tổng quát hơn, chuẩn $ \\ell_p$ :\n",
    "\n",
    "$\\|\\mathbf{x}\\|_p = \\left(\\sum_{i=1}^n \\left|x_i \\right|^p \\right)^{1/p}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chuẩn *Frobenius* thỏa mãn tất cả các tính chất của một chuẩn vector. Nó giống chuẩn  ℓ2  của một vector nhưng ở dạng của ma trận. Ta dùng hàm `norm` để tính toán chuẩn Frobenius của ma trận."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(torch.ones((4, 9)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.10.1. Chuẩn và Mục tiêu\n",
    "Tuy không muốn đi quá nhanh nhưng chúng ta có thể xây dựng phần nào trực giác để hiểu tại sao những khái niệm này lại hữu dụng. Trong học sâu, ta thường cố giải các bài toán tối ưu: *cực đại hóa xác suất* xảy ra của dữ liệu quan sát được; *cực tiểu hóa* khoảng cách giữa dự đoán và nhãn gốc. Gán các biểu diễn vector cho các đối tượng (như từ, sản phẩm hay các bài báo) để cực tiểu hóa khoảng cách giữa các đối tượng tương tự nhau và cực đại hóa khoảng cách giữa các đối tượng khác nhau. Mục tiêu, thành phần quan trọng nhất của một thuật toán học sâu (bên cạnh dữ liệu), thường được biễu diễn diễn theo *chuẩn (norm)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.11. Bàn thêm về Đại số Tuyến tính\n",
    "Vẫn còn rất nhiều kiến thức đại số tuyến tính, phần lớn đều hữu dụng cho học máy. Một ví dụ là phép phân tích ma trận ra các thành phần, các phép phân tích này có thể tạo ra các cấu trúc thấp chiều trong các tập dữ liệu thực tế. Có cả một nhánh của học máy tập trung vào sử dụng các phép phân tích ma trận và tổng quát chúng lên cho các tensor bậc cao để khám phá cấu trúc trong các tập dữ liệu và giải quyết các bài toán dự đoán. Tuy nhiên, cuốn sách này chỉ tập trung vào học sâu. Và chúng tôi tin rằng bạn sẽ muốn học thêm nhiều về toán một khi đã có thể triển khai được các mô hình học máy hữu dụng cho các tập dữ liệu thực tế. Bởi vậy, trong khi vẫn còn nhiều kiến thức toán cần bàn thêm ở phần sau, chúng tôi sẽ kết thúc mục này ở đây.\n",
    "\n",
    "Nếu bạn muốn học thêm về đại số tuyến tính, bạn có thể tham khảo [Geometry and Linear Algebraic Operations](https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/geometry-linear-algebraic-ops.html) hoặc các nguồn tài liệu xuất sắc tại [Strang, 1993](https://d2l.ai/chapter_references/zreferences.html#strang-1993), [Petersen et al., 2008](https://d2l.ai/chapter_references/zreferences.html#petersen-pedersen-ea-2008)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.12. Tóm tắt\n",
    "* Số vô hướng, vector, ma trận, và tensor là các đối tượng toán học cơ bản trong đại số tuyến tính.\n",
    "* Vector là dạng tổng quát của số vô hướng và ma trận là dạng tổng quát của vector.\n",
    "* Trong cách biểu diễn các số vô hướng, vector, ma trận và tensor lần lượt có 0, 1, 2 và một số lượng tùy ý các trục.\n",
    "* Một tensor có thể thu gọn theo một số trục bằng `sum` và `mean`.\n",
    "* Phép nhân theo từng phần tử của hai ma trận được gọi là tích Hadamard của chúng. Phép toán này khác với phép nhân ma trận.\n",
    "* Trong học sâu, chúng ta thường làm việc với các chuẩn như chuẩn $\\ell_1$, chuẩn  $\\ell_1$  và chuẩn Frobenius.\n",
    "* Chúng ta có thể thực hiện một số lượng lớn các toán tử trên số vô hướng, vector, ma trận và tensor với các hàm của tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.13. Bài tập\n",
    "1. Chứng minh rằng chuyển vị của một ma trận chuyển vị là chính nó: $(\\mathbf{A}^\\top)^\\top = \\mathbf{A}$.\n",
    "2. Cho hai ma trận $\\mathbf{A}$ và $\\mathbf{B}$, chứng minh rằng tổng của chuyển vị bằng chuyển vị của tổng: $\\mathbf{A}^\\top + \\mathbf{B}^\\top = (\\mathbf{A} + \\mathbf{B})^\\top$ .\n",
    "3. Cho một ma trận vuông $\\mathbf{A}$, liệu rằng $\\mathbf{A} + \\mathbf{A}^\\top$ có luôn đối xứng? Tại sao?\n",
    "4. Chúng ta đã định nghĩa tensor X với kích thước ( 2 ,  3 ,  4 ) trong mục này. Kết quả của `len(X)` là gì?\n",
    "5. Cho một tensor X với kích thước bất kỳ, liệu len(X) có luôn tương ứng với độ dài của một trục nhất định của X hay không? Đó là trục nào?\n",
    "6. Chạy `A / A.sum(axis=1)` và xem điều gì xảy ra. Bạn có phân tích được nguyên nhân không?\n",
    "7. Khi di chuyển giữa hai điểm ở Manhattan (đường phố hình bàn cờ), khoảng cách tính bằng tọa độ (tức độ dài các đại lộ và phố) mà bạn cần di chuyển là bao nhiêu? Bạn có thể đi theo đường chéo không? (Xem thêm bản đồ Manhattan, New York để trả lời câu hỏi này)\n",
    "8. Xét một tensor với kích thước ( 2 ,  3 ,  4 ). Kích thước của kết quả sau khi tính tổng theo trục  0 ,  1  và  2  sẽ như thế nào?\n",
    "9. Đưa một tensor với 3 trục hoặc hơn vào hàm `linalg.norm` và quan sát kết quả. Hàm này thực hiện việc gì cho các ndarray với kích thước bất kỳ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Giải tích (Calculus)\n",
    "Tìm diện tích của một đa giác vẫn là một bí ẩn cho tới ít nhất $2.500$ năm trước, khi người Hy Lạp cổ đại chia đa giác thành các tam giác và cộng diện tích của chúng lại. Để tìm diện tích của các hình cong, như hình tròn, người Hy Lạp cổ đại đặt các đa giác nội tiếp bên trong các hình cong đó. Một đa giác nội tiếp với càng nhiều cạnh bằng nhau thì càng xấp xỉ đúng hình tròn. Quy trình này còn được biết đến như *phương pháp vét kiệt (method of exhaustion)*.\n",
    "\n",
    "![polygon_circle](images/polygon_circle.svg)\n",
    "\n",
    "Phương pháp vét kiệt chính là khởi nguồn của *giải tích tích phân (integral calculus)*. Hơn  2.000  năm sau, nhánh còn lại của giải tích, *giải tích vi phân (differential calculus)*, ra đời. Trong những ứng dụng quan trọng nhất của giải tích vi phân, các bài toán tối ưu hoá sẽ tìm cách tốt nhất để thực hiện một công việc nào đó. Các bài toán như vậy vô cùng phổ biến trong học sâu.\n",
    "\n",
    "Trong học sâu, chúng ta *huấn luyện* các mô hình, cập nhật chúng liên tục để chúng ngày càng tốt hơn khi học với nhiều dữ liệu hơn. Thông thường, trở nên tốt hơn tương đương với cực tiểu hoá một *hàm mất mát*, một điểm số sẽ trả lời câu hỏi “mô hình của ta đang tệ tới mức nào?” Câu hỏi này lắt léo hơn ta tưởng nhiều. Mục đích cuối cùng mà ta muốn là mô hình sẽ hoạt động tốt trên dữ liệu mà nó chưa từng nhìn thấy. Nhưng chúng ta chỉ có thể khớp mô hình trên dữ liệu mà ta đang có thể thấy. Do đó ta có thể chia việc huấn luyện mô hình thành hai vấn đề chính: i) *tối ưu hoá*: quy trình huấn luyện mô hình trên dữ liệu đã thấy. ii) *tổng quát hoá*: dựa trên các nguyên tắc toán học và sự uyên thâm của người huấn luyện để tạo ra các mô hình mà tính hiệu quả của nó vượt ra khỏi tập dữ liệu huấn luyện."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
